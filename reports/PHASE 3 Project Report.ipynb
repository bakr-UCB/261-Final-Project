{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5451befa-0ade-48d3-b632-66b1c422480e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Report Rubric\n",
    "\n",
    "## In-class presentation (10 pts)\n",
    "In-Class Presentation should have a logical and business flow to it. In more detail, your In-Class Presentation should have a logical and scientific flow to it with main sections for each of the following:\n",
    "\n",
    "- a title slide (with the project name, Group Number, the team member names, and photos).\n",
    "- an abstract slide\n",
    "- Make sure it has an outline slide with good descriptive section headings\n",
    "- Team names, photos\n",
    "- Project description\n",
    "- Some summary visual EDA\n",
    "- Feature engineering and Top features\n",
    "- Overview of Modeling Pipelines explored\n",
    "- Results and discussion of results (Accuracy, ROC/AUC, etc.. from this phase and previous phases)\n",
    "- Conclusions (best performing model, number of features, top 10 best features, hyper-parameters) and next steps\n",
    "\n",
    "## Team and project meta information (10 pts)\n",
    "Please provide the following:\n",
    "* Team ID\n",
    "* The complete list of team members and project meta information (e.g., **email**).\n",
    "\n",
    "* Credit assignment plan updates (who does/did what and when, amount of effort in terms of person-hours, start and end dates, estimated improvement in terms of key metrics) in Table format\n",
    "No Credit assignment plan means ZERO points\n",
    "A credit assignment plan not in Table format means ZERO points\n",
    "No start and end dates and (budgeted) hours of effort mean an incomplete plan. This may result in zero points.\n",
    "\n",
    "## Project Abstract (10 pts)\n",
    "- Final Abstract: The final form of the abstract! It should have everything covered in previous phases, plus the new experiments and the final model selected, as well as your final results (report the number!)\n",
    "- Make sure to describe what your focused on and accomplished in this project (include this phase and previous phases). Have a look at the expectations with regard to a good abstract.\n",
    "\n",
    "## Data and feature engineering (10 pts)\n",
    "- Summarize the data lineage and key data transformations (joins)\n",
    "- List of feature families explored and explanation of each\n",
    "- List of features within each family and description of each, along with THEIR EDA\n",
    "- Please refer to experiments showing the value of each feature/family\n",
    "\n",
    "## Neural Network (MLP) (10 pts)\n",
    "You are expected to train a Neural Network\n",
    "- Implement Neural Network (NN) model\n",
    "- Experiment with at least 2 different Network architectures and report results.\n",
    "- Must show training and performance scores, **including training curves by epoch**\n",
    "\n",
    "## Leakage (10 pts)\n",
    "- Define what is leakage and provide a a hypothetical example of leakage\n",
    "- Go through your Pipeline and check if there is any leakage.\n",
    "- Are you violating any cardinal sins of ML?\n",
    "- Describe how your pipeline does not suffer from any leakage problem and does not violate any cardinal sins of ML\n",
    "\n",
    "## Modeling Pipelines (10 pts)\n",
    "Expectations here are to provide the following in sections and subsections:\n",
    "\n",
    "- A visualization of the modeling pipeline (s) and subpipelines if necessary\n",
    "- Families of input features and count per family\n",
    "- Number of input features\n",
    "- Hyperparameters and settings considered\n",
    "- Loss function used (data loss and regularization parts) in latex\n",
    "- Number of experiments conducted\n",
    "- Experiment table with the following details per experiment:\n",
    "    - Baseline experiment\n",
    "    - Any additional experiments\n",
    "    - Final model tuned\n",
    "    - best results (1 to three) for all experiments you conducted with the following details\n",
    "    - Computational configuration used\n",
    "    - Wall time for each experiment\n",
    "\n",
    "## Results and discussion of results (20 pts)\n",
    "Expectations here are to provide the following: The goal of Discussion’ section is present an interpretation of key results , which means explain, analyse, and compare them (results from all the phases). Often, this part is the most important, simply because it lets the researcher take a step back and give a broader look at all experiments conducted. Do not discuss any outcomes not presented in the results part.\n",
    "\n",
    "Make sure to provide the following in sections and subsections:\n",
    "- Your experiments are properly enumerated/tabulated and discussed (accurate descriptions, performance metrics)\n",
    "- Discuss results not substantiated in your experimental section above in the modeling pipelines\n",
    "- Provide gap analysis\n",
    "\n",
    "## Conclusion (10 pts)\n",
    "Expectations here are to address the following following in your conclusion in a main section by itself (150 words or less):\n",
    "\n",
    "- Restate your project focus and explain why it’s important. Make sure that this part of the conclusion is concise and clear.\n",
    "- Restate your hypothesis (e.g., ML pipelines with custom features can accurately predict .......)\n",
    "- Summarize main points of your project: Remind your readers your key points. (e.g, best features, best model, hyper-parameters and so on)\n",
    "- Discuss the significance of your results\n",
    "- Discuss the future of your project.\n",
    "\n",
    "## Extra credit\n",
    "- Deep learning (5 points)\n",
    "- Recent data (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ecd627d-9ebc-4fd0-ae5b-20c814e9d29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Phase III Project Report\n",
    "__`Team 4-1`__\n",
    "\n",
    "`April 19, 2025`\n",
    "\n",
    "`Phase III led by Erica Landreth`\n",
    "\n",
    "## Authored By:\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/authors.png?raw=true\" alt=\"ML Pipeline\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe783fdb-2d27-4045-b4c7-935de8895f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Phase Leader Plan\n",
    "| Phase |  Phase Leader | Phase Leader Email|\n",
    "|:---:|:---:|:---:|\n",
    "| **Phase 0, HW5**: Finalize Teams, and submitting HW5 | Danielle Yoseloff | dyoseloff@berkeley.edu |\n",
    "| **Phase 1**: Project Plan, describe datasets, joins, tasks, and metrics  | Mohamed Bakr | m.baker@berkeley.edu |\n",
    "|**Phase 2**: EDA, baseline pipeline, Scalability, Efficiency, Distributed/parallel Training, and Scoring Pipeline| Shruti Gupta | sguptaray@berkeley.edu |\n",
    "|**Phase 3**: Select the optimal algorithm, fine-tune and submit a final report| Erica Landreth | erica.landreth@berkeley.edu |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59c46d31-94ed-4a47-ad6a-e38db513c7d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Credit assignment plan \n",
    "\n",
    "| Phase | team Meamber | Tasks | Hrs|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|**PHASE 0**| Danielle Yoseloff | Forming Team, Create Slack Channel, and team introduction |  2 |\n",
    "|**PHASE 1**| Danielle Yoseloff | Machine algorithms and metrics | 8 |\n",
    "|| | Pipeline Graph | 1 |\n",
    "||Erica Landreth | Abstract and Report Editing | 3 |\n",
    "||| EDA | 2.5 |\n",
    "||| Data Description | 8 |\n",
    "|| Shruti Gupta | EDA | 4 |\n",
    "|| |Missing & Null Value Exploration | 4 |\n",
    "|| Mohamed Bakr | Phase Leader Table, Credit Assigment plan, and GANTT chart |  8 |\n",
    "||| Digesting the Data and Checkpointing Strategy | 4 |\n",
    "|||Report editing and review| 2 |\n",
    "|**PHASE 2**| Danielle Yoseloff | Feature Engineering | 15|\n",
    "|| | Slides and Report| 8|\n",
    "||Erica Landreth | EDA and Cleaning | 11.5 |\n",
    "||| Pipeline and Cross Validation Development | 6.5 |\n",
    "||| Feature Engineering | 9.5 |\n",
    "||| Slides and Report | 12 |\n",
    "|| Shruti Gupta | EDA and Cleaning | 15 |\n",
    "|| | Feature Engineering | 10 |\n",
    "|| | Hyperparameter Tuning and Analysis | 6 |\n",
    "|| | Report | 6 |\n",
    "|| Mohamed Bakr | Setting Up Work Environment and GitHub| 1 |\n",
    "|| | Join and OTPW EDA | 12|\n",
    "|| | Join Pipeline | 16|\n",
    "|| | Slides and Report | 8 |\n",
    "|**PHASE 3**| Danielle Yoseloff | Report and Presentation | 10 |\n",
    "|| | Model Development | 24 |\n",
    "||Erica Landreth | Feature Engineering | 7 |\n",
    "||  | Modeling and Hyperparameter Tuning | 9 |\n",
    "||  | Project Management, Report, Slides, Figure Generation | 25 |\n",
    "|| Shruti Gupta | Feature Engineering | 10 |\n",
    "||  | Modeling and Hyperparameter Tuning | 20 |\n",
    "||  | Analysis of results | 6 |\n",
    "||  | Report | 6 |\n",
    "|| Mohamed Bakr | Join full data | 8|\n",
    "||  | Hyperparameter Tuning | 8 |\n",
    "||  | Exploring leakage and exploring solutions | 8 |\n",
    "||  | Report and Presentation| 8 |\n",
    "\n",
    "**Detailed Plan and GANTT Chart:** https://docs.google.com/spreadsheets/d/1E4A3SaTAEjh9owH4SBUMv987bktwrW4Q6TXCZ5LJ6Xg/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee4d207-94a3-411e-8def-a3a5bbf2753d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "\n",
    "According to a 2019 FAA study, national airline delay-related costs exceeded $8 billion due to increased operating expenses.[2] Equipping airports with predictive systems for flight disruptions enables proactive mitigation strategies to absorb operational shocks and prevent cascading delays throughout the system. Therefore, our team attempted to design a machine learning classification model to predict whether an upcoming flight's departure would be disrupted or not, using information available two hours or more prior to the scheduled departure time, where \"disrupted\" flights consist of either delays over 15 minutes or cancellations.\n",
    "\n",
    "\n",
    "Data was sourced from historic Department of Transportation (DoT) flight data and associated National Oceanic and Atmospheric Association (NOAA) weather station reports from the years 2015 to 2021. All results discussed in this report are with respect to a 5-year subset of the data (2015-2019, inclusive). \n",
    "\n",
    "F2 score was used to evaluate model performance, reflecting airports' priority to penalize false negatives (i.e., incorrectly predicting disrupted flights as on schedule). For the train set, F2 scores were computed within time-series cross-validation folds and averaged to summarize overall training scores. Logistic regression was chosen as the baseline model because of its suitability for a binary outcome and interpretability, achieving an F2 of .475 on train and .478 on test. Three advanced architectures were also considered and tuned to achieve their best performances: random forest (train: .475; test: .478), multilayer perceptron (train: .507; test: .525), XGBoost (train: .520; test: .535). Additionally, an ensemble was taken over their predictions and yielded an F2 score of .554 on the test set. \n",
    "\n",
    "The team focused on engineering recency- and network- related features that would inform the model of delay propagation patterns. **not sure what/how much to put here: describe what your focused on and accomplished in this project**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ede1c0d-31a9-4bc0-84c2-41676b907e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Research Objective\n",
    "\n",
    "Our primary customer is the airport management and administration; therefore, our aim is to use machine learning models to make a binary prediction of flight disruptions 2 hours prior to the scheduld flight departure time using the models described above. We define a disruption as a delay (according to the FAA definition, a flight that departs 15 minutes or more after its scheduled departure), or a cancellation. We consider flight cancellations as functionally analogous to long-term delays, similar to those reported as exceeding 24 hours. This approach is based on the idea that cancellations, like long delays, can disrupt resource allocation and operational flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb49b27-c36c-4b45-8d6d-c06f513e6dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Description\n",
    "\n",
    "For this phase of our analysis, we focused on flight and weather data spanning the years 2015 through 2021. This section describes the data sources we used, and defines the fields relevent to our analysis.\n",
    "\n",
    "### Data size and source\n",
    "\n",
    "We used the following data sources for our modeling an analysis:\n",
    "\n",
    "| Dataset Name     | Dataset Size    | Dataset Description      |Dataset Source   |\n",
    "| :-------------: | ------------- | ------------- |  ------------- |\n",
    "| Flights 2015-2021 |   74,177,433 rows by 109 columns | DoT historical flight data from the years 2015-2021 | [4] |\n",
    "| Weather 2015-2021 | 898,983,399 rows by 128 columns | NOAA weather conditions for the corresponding time period | [1], [3] |\n",
    "| Stations | 5,004,169 rows by 12 columns | The weather station data defines the distances from various weather stations to various airports. |  |\n",
    "| Airports | 57,421 rows by 12 columns | The airport dataset provides airport metadata and identifiers necessary for joins. |  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bf00810-82c6-4f4f-986b-2c66aca149c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data dictionary\n",
    "\n",
    "This section defines the variables from each source that we used for our modeling and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc02a1ca-2f6a-48fe-9e59-3f91b386c3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Flights data\n",
    "\n",
    "The flights data provide metadata for a given flight, and will also help us to study time-series trends and aggregate delay statistics by characteristics such as airport and airline. The below definitions were informed by DoT documentation [4].\n",
    "\n",
    "| Column |  Raw Data Type | Meaning | Intended Use |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| QUARTER | Integer | Quarter | Categorical variable to capture seasonal-periodic trends |\n",
    "| MONTH | Integer | Month | Categorical variable to capture month-periodic trends |\n",
    "| DAY_OF_WEEK | Integer | 1-7: Monday-Sunday | Categorical variable to capture week-periodic trends |\n",
    "| FL_DATE | String | Flight date | Used in flight timestamp UTC conversion |\n",
    "| OP_UNIQUE_CARRIER | String | Unique flight carrier ID | Airline categorical variable |\n",
    "| TAIL_NUM | String | Aircraft tail number (registration code) | Create time-based tracking features |\n",
    "| ORIGIN | String | Origin airport IATA code | Join to airports data; create route tracking features; match to seasonal components  |\n",
    "| DEST | String | Destination airport IATA code | Join to airports data; create time-based tracking feature |\n",
    "| CRS_DEP_TIME | Integer | Scheduled departure time (local, HHMM format) | Create time-based tracking features |\n",
    "| DEP_TIME | Integer | Actual departure time (local, HHMM format) | Create time-based tracking features |\n",
    "| DEP_DELAY | Double | Departure delay (min) | Define Boolean departure disruption status; create time-based tracking features |\n",
    "| TAXI_OUT | Double | Time taxiing out (min) | Create time-based tracking features |\n",
    "| TAXI_IN | Double | Time taxiing in (min) | Create time-based traffic flow |\n",
    "| CRS_ARR_TIME | Integer | scheduled arrival time (local, HHMM format) | Create time-based tracking features |\n",
    "| ARR_TIME | Integer | Actual arrival time (local, HHMM format) | Create time-based tracking features |\n",
    "| ARR_DELAY | Double | Arrival delay (min) | Create time-based tracking features |\n",
    "| CANCELLED | Double | 1.0/0.0: Cancelled/not cancelled | Define Boolean departure disruption status |\n",
    "| CRS_ELAPSED_TIME | Double | Scheduled flight duration (min) | Represent anticipated flight length; create time-basd tracking features |\n",
    "| ACTUAL_ELAPSED_TIME | Double | Actual flight duration (min) | Create time-based tracking features |\n",
    "| AIRTIME | Double | Time between take-off and landing (min) | Represent flight length |\n",
    "| DISTANCE | Double | Distance between origin and destination airports | Represent flight length |\n",
    "| YEAR | Integer | Year | Time series feature engineering |\n",
    "| DAY_OF_MONTH | Integer | Day of the month | Categorical variable to capture month-periodic trends |\n",
    "| ORIGIN_CITY_NAME | String | Origin Airport, City Name | Geographic sanity checks |\n",
    "| DEST_CITY_NAME | String | Destination Airport, City Name\t| Geographic sanity checks |\n",
    "\n",
    "We chose to drop some variables from the full flights table based on redundancy, the proportion of missing values, and relevance to our analysis. These include alternate representations of airport and airline ID's and diversion information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f88ec66-4ad2-427a-80f3-888eb647f53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Weather data\n",
    "\n",
    "The weather data allows us to define weather conditions relevant to an individual flight, as well as characterize longer-term regional weather trends. The below definitions were informed by NOAA documentation [1] and [3].\n",
    "\n",
    "| Column |  Raw Data Type | Meaning | Intended Use |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| STATION | String | Weather station ID | Key for joining to stations data |\n",
    "| DATE | String | Date the time (UTC) of weather report | Filter weather reports in time |\n",
    "| YEAR | Int | Year | Time series feature engineering |\n",
    "| LATITUDE | String | Station latitude (degrees North) | Characterize station location |\n",
    "| LONGITUDE | String | Station longitude (degrees East) | Characterize station location |\n",
    "| REPORT_TYPE | String | Weather report type | Filter to relevant report types |\n",
    "| HourlyDewPointTemperature | String | Dew point temperature (degrees F) | Define weather conditions |\n",
    "| HourlyDryBulbTemperature | String | Air temperature (degrees F) | Define weather conditions |\n",
    "| HourlyPrecipitation | String | Precipitation amount (in) | Define weather conditions |\n",
    "| HourlyPresentWeatherType | String | String code defining present weather *e.g.* rain or hail | Parse report to fill in missing information |\n",
    "| HourlyPressureChange | String | Change in pressure (in Hg) | Define weather conditions |\n",
    "| HourlyRelativeHumidity | String | Relative humidity (percentage) | Define weather conditions |\n",
    "| HourlyVisibility | String | Horizontal visibility (mi) | Define weather conditions |\n",
    "| HourlyWetBulbTemperature | String | Wet bulb temperature (degrees F) | Define weather conditions |\n",
    "| HourlyWindGustSpeed | String | Wind gust speed (mph) | Define weather conditions |\n",
    "| HourlyWindSpeed | String | Wind speed (mph) | Define weather conditions |\n",
    "| NAME | String | Weather Station Name | Used to Identify Weather Stations |\n",
    "| REM | String | Remarks Data Section | Used for imputing some of the missing values |\n",
    "\n",
    "We chose to drop some variables from the full weather table based on redundancy, the proportion of missing values, and relevance to our analysis. These include alternate station identifiers, daily and monthly averages, and station backup/maintenance information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e841ecd-c8f2-4dfe-bb9c-64a7dd2cde8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Weather station data\n",
    "\n",
    "The weather station data defines the distances from various weather stations to various airports.\n",
    "\n",
    "| Column |  Raw Data Type | Meaning | Intended Use |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| station_id | String | Weather station ID | Key for joining to weather data |\n",
    "| lat | Double | Station latitude (degrees North) | Characterize station location |\n",
    "| lon | Double | Station longitude (degrees East) | Characterize station location |\n",
    "| neighbor_name | String | Airport name | Sanity check for joins |\n",
    "| neighbor_state | String | Airport state | Sanity check for joins |\n",
    "| neighbor_call | String | Airport ICAO code | Key for joining to airport data |\n",
    "| neighbor_lat | Double | Airport latitude (degrees North) | Characterize airport location |\n",
    "| neighbor_lon | Double | Airport longitude (degrees East) | Characterize airport location |\n",
    "| distance_to_neighbor | Double | Haversine Distance (mi) from station to airport | Find weather stations near a given airport |\n",
    "\n",
    "\n",
    "We chose to drop some variables from the full stations table based on redundancy and relevance to our analysis. These include alternate station and airport identifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "743d4782-4027-4ab5-8a96-db0ec8163f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Airport data\n",
    "\n",
    "The airport dataset provides airport metadata and identifiers necessary for joins.\n",
    "\n",
    "| Column |  Raw Data Type | Meaning | Intended Use |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| icao_code | String | Airport ICAO code | Join to stations data |\n",
    "| type | String | Airport type | Characterize airport operations |\n",
    "| iso_region | String | ISO code of airport region | Filtering and sanity check for joins |\n",
    "| iata_code | String | Airport IATA code | Join to flights data |\n",
    "| coordinates | String | Airport latitude and longitude | Characterize airport location |\n",
    "\n",
    "We chose to drop some variables from the full airports table based on redundancy and relevance to our analysis. These include alternative identifiers and local, categorical location codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7627133-babe-4019-a5b4-ca52c7acc562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Lineage and Key Transformations\n",
    "\n",
    "For our modeling, we chose to create a joined dataset from the raw weather and flights data. The following subsections outline the cleaning we did to prepare the raw data for joining, and the join pipeline itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "446e0a07-8fdb-4d09-8616-4b67e86e6c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key Transformations\n",
    "\n",
    "The data had to be processed and cleaned prior to implementing the join pipeline. The main processing steps were handling missing values and deduplicating records. The following subsections outline how we prepared both the raw weather and raw flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edc9333d-e6cd-4249-bfc8-6c6465cdc5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Weather\n",
    "\n",
    "The most critical missing values in the weather data were location-based; without latitude or longitude information we could not match the observation to the nearest airport. To identify stations, we extracted the USAF and WBAN codes from the first and second halves of the given weather station ID and parsed the ICAO code from the text report column (\"REM\"). We then matched whichever attribute was available to the stations dataset to fill in identifying information to the weather data and filtered out stations not in the United States or its territories. Missing feature observations in the weather dataset could be derived from sensor malfunctions and were often compounded to result in several hours or days in a row of missing data, even despite prolific duplicates. Duplicates are defined as multiple reports emitted from the same station at the same time. Therefore, our deduplication rule was simply to keep the record with the least null values in the hourly-level columns (our columns of interest). The de-duplicated dataset with location identifiers was then used as the weather base for the join. \n",
    "\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/RawWeatherNulls.png?raw=true\" alt=\"Null Counts: Raw Weather Data\" style=\"width: 200px\">\n",
    "\n",
    "\n",
    "_Above: Example of Nulls Distribution in Selected Raw Weather Columns on 1y Training Data_\n",
    "\n",
    "To address missing values in the weather data used for modeling, we first parsed the remarks column which contains METAR reports to extract relevant values. In cases where the METAR reports contained insufficient information or were also missing, we prioritized spatially-based imputation. This decision was based on the fact that the weather data matched to each flight was already two hours stale, limiting the usefulness of interpolation over time. Airports were geohashed using the python-geohash package at a precision level of 2, which clusters airports into coarse regional buckets to enable spatially coherent imputation. A more granular precision level resulted in not enough airports per bucket, whereas the less granular level was too broad and would not adequately capture region-specific weather conditions. \n",
    "\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/geohash.jpg?raw=true\" alt=\"Null Counts: Raw Weather Data\" style=\"width: 100px; display: inline-block;\"> \n",
    "\n",
    "_Above: Example of Geohashed Regions on 1y Training Data_\n",
    "\n",
    "For each missing weather observation, we attempted to impute values by pulling the most recent non-null weather reports timestamped between 2–6 hours prior to the flight's scheduled departure from other airports within the same geohash bucket in an attempt to capture immediate recent weather status and events.\n",
    "\n",
    "In cases where multiple stations within the geohash region had valid reports in that time window, we selected the most recent single record rather than computing an average, to reduce computational complexity. In cases where all stations in a region were down—due to widespread outages or technical issues—we implemented a fallback strategy by computing an exponential moving average (EMA) over the last 8 non-null records prior to the missing timestamp. This parameter was tuned to sufficiently capture remaining nulls without being unnecessarily wide. We chose the EMA approach to balance responsiveness to recent trends with the need to smooth over noise. Importantly, this method does not introduce label leakage: because all weather data were sourced from a 2–4 hour window prior to each flight's scheduled departure, no future data relative to the prediction target was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8650bac6-9327-4251-ae57-f83f695950ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Flights [UPDATE STATS OR NOTE THAT THEY ARE ON 1Y DATA]\n",
    "\n",
    "The flights dataset had true duplicates for each record, which was expected due to its information being recorded at origin and destination airports. The columns attributing delay minutes to causes (carrier, NAS, weather, security, late aircraft) were missing over 50% of their values, so we elected not to use them in the analysis. Time-related columns like arrival time, actual elapsed time, or departure time contained missing values only in the case of cancelled flights, or, in rare cases, diverted flights. Diverted flights made up just 0.27% of the training dataset and diversion-related columns were extremely sparse, so we elected to drop these columns for modeling and analysis. \n",
    "\n",
    "The TAIL_NUM column is essential for relating multiple flights by the same aircraft, and contained only 0.29% missing values in the training set, so nulls were treated as a missing value indicator which was inherited by dependent features. We also encountered cases where the same aircraft appeared scheduled to depart to different destinations at the same UTC departure timestamp. These apparent duplicates occurred exclusively when one of the records experienced a severe delay or cancellation, so we concluded that they were not true duplicates but reflected inconsistencies from when system snapshots were recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d5e00f-dffc-475d-a2b3-e9c74a7c51aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Joining strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c6489a-8a1d-4064-bc25-56c57b79a7c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The team chose to construct a joined dataset using airports, flights, weather, and stations data to minimize leakage. In contrast to our modeling objective of making predictions exclusively with data obtained two hours _before_ scheduled flight departure, initial EDA on the provided OTPW datset revealed weather station records obtained _at_ the scheduled departure time.\n",
    "\n",
    "To address this, we developed a multi-step pipeline to join the **7 years** of flights and weather data from 2015 to 2021 with several checkpoints, as outlined below:\n",
    "\n",
    "1. We created a UDF to extract time zones based on each airport’s and station's latitude and longitude (parsed from the `coordinates` column in the airport codes stations table). The result was a 2 helper table:\n",
    "    \n",
    "    -  Airport time zones containing `icao_code`, `latitude`, `longitude`, and `timezone`, covering 2,237 unique airports.\n",
    "    -  Stations time zones table containing `STATION`, `LATITUDE`, `LONGITUDE`, and `timezone` , coveering 2,939 unique weather stations. \n",
    "\n",
    "2. We cleaned and deduplicated the flights data, retaining only necessary features. The dataset was reduced from a dimension of **74.18M x 109** to **42.43M x 29**.\n",
    "\n",
    "3. We joined the flights data with the airport codes table twice—once on the `ORIGIN` IATA code and once on the `DEST` IATA code—to obtain the corresponding `icao_code`, `type`, and `iso_region`.\n",
    "\n",
    "4. We recalculated the distance between all weather stations and airports using the Haversine formula to get accurate proximity values in kilometers.\n",
    "\n",
    "5. Before joining flights with stations, we identified seven missing `icao_code`s that were not present in the stations dataset. We augmented the stations data by computing distances for those airports using their coordinates from the airport codes table and saved the updated result for reuse.\n",
    "\n",
    "6. To improve join efficiency, we filtered the stations dataset down to only the closest station per airport. This reduced the station-airport combination dataset from ~5 million rows to just 2,236 rows, significantly reducing shuffle during the join.\n",
    "\n",
    "7. We joined the flights dataset with the filtered stations data to retrieve the `station_id`, `station_lat`, `station_lon`, `airport_lat`, `airport_lon`, and `station_distance` for both origin and destination.\n",
    "\n",
    "8. After ensuring there were no missing `icao_code`s in the time zone helper table, we enriched the flights data by joining it with time zone info using `icao_code`. This enabled us to convert the scheduled departure time into UTC (`sched_depart_utc`) and compute `two_hours_prior_depart_UTC` and `four_hours_prior_depart_UTC` using UDFs.\n",
    "\n",
    "9. The weather dataset was preprocessed by removig duplicates, filtering only USA locations, selecting relevant features, converting date and time to UTC, and filtering to only include station-date combinations that matched those in the flights dataset. This reduced the weather data from a dimension of **898.98M x 124** to **29.27M x 18**.\n",
    "\n",
    "10. Finally, we joined the flights data with weather data and matched on station ID and filtered for weather records where the UTC timestamp was between two and four hours before scheduled departure. (See the data description section for selected weather features.)\n",
    "\n",
    "The full join pipeline took approximately **4 and half hours** using **5–10 workers**, producing a final DataFrame of **42.43M x 78** and a Parquet file of ~6.24 GB for the 7 years dataset.\n",
    "\n",
    "All location-specific columns were prefixed with `origin_`/ to clearly indicate their reference point.\n",
    "\n",
    "To validate the pipeline, we tested it first on a 3-month sample and one year datasets before scaling to 7 years. We ensured data quality and maintained full lineage tracking throughout. All joins had a **100% match rate**, except for the weather join, which had a **99.86% match rate** for origin as expected due to slight gaps in available weather records.\n",
    "\n",
    "This pipeline provides a robust and well-validated dataset that serves as the foundation for downstream feature engineering and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c847651-f479-4904-8c83-5a71d1401999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/Join%20Pipeline.jpeg?raw=true\" alt=\"Join Pipeline\" style=\"width: 200px\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/Join%20pipeline%20ERD.png?raw=true\" alt=\"Join ERD\" style=\"width: 200px\">\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ef060c-9ea3-43df-8fb9-9a0ca4cf6242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86dde4eb-b47a-4d78-8c25-a0128f9a256b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "After cleaning and joining the data and before modeling, we performed exploratory data analysis in order to understand the characteristics of the outcome variable, and to investigate features or trends that might be useful for feature engineering. Unless otherwise specified, all EDA is presented on the training dataset, which covers the years 2015-2018 inclusive.\n",
    "\n",
    "### Flights\n",
    "\n",
    "Our outcome variable is a Boolean flag describing flight departure disruption status. The variable is true when a flight is either delayed at least 15 minutes or cancelled, and false otherwise. By this definition, in the 2015-2019 dataset, 21% of the records were disrupted; of these, 10% were caused by cancellations and 90% by delays.\n",
    "\n",
    "The below figure shows disruption statistics over time for the full 2015-2021 dataset. The top subplot depicts the number of flights taking place each day, the middle plot the daily average proportion of flight disrupted, and the bottom the daily average flight delay in minutes. We see that delay characteristics vary throughout the year, as does the volume of flights: we see increases in flight volume in the summer and around the holidays, which roughly correspond to increases in disruption. On a longer timescale, the disruption metrics are fairly stable for the years 2015-2019, despite the notable increase in flight volume in 2018 and 2019. There is, of course, a notable change in all three plots with the onset of the pandemic in early 2020. For a brief period of time, when flight volume is especially low in 2020, the average delay is negative (indicating that flights, on average, departed early)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "416e169b-0d20-42ad-b2b3-12f5454ff778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/time_series_EDA.png?raw=true\" alt=\"Time Series Plot\" style=\"width: 100px; display: inline-block;\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffcba621-edfa-4852-a31f-075d93f9d70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The raw flights data included fields attributing a given flight's delay time to one or more of the DoT's flight delay categories [6]:\n",
    "- Late aircraft delay: Flight disrupted because its aircraft arrived late from its prior flight\n",
    "- Carrier delay: Flight disrupted by carrier related circumstances (_e.g._ crew staffing, baggage handling, etc.)\n",
    "- NAS delay: Flight disrupted by National Aviation System conditions (often related to widespread events)\n",
    "- Weather delay: Flight disrupted due to extreme weather\n",
    "- Security delay: Flight disrupted due to security breach or exceptionally long security screening times\n",
    "\n",
    "The below plot depicts the proportion of total delay minutes attributed to each category for flights during the years 2015-2019. Note that the effect of weather is somewhat underrepresented in this plot. The DoT \"weather delay\" category refers only to extreme weather, and non-extreme weather disruptions may show up in late aircraft, carrier, or NAS delays. Regardless, we see that weather information alone is not enough to explain delay characteristics. Therefore, in our feature engineering work, we aimed to design features that would capture circumstances related to these other DoT delay types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93035718-f76c-4cbf-869f-23ee149d4de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/delay_causes_EDA.png?raw=true\" alt=\"Delay Minutes Plot\" style=\"width: 100px; display: inline-block;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "708d5957-43a0-4ddb-a91b-0d5ca24c0168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "678a6143-1f9b-4ef1-ab80-7c66ceb19889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We also explored airport-specific associations with delay. In this figure, the marker size represents the relative proportion of flights departing from the origin airport, from 2015-2018, with a minimum size for visibility, and color represents continuous delay amount in minutes. Less busy airports appear to have more severe delays. This motivated our inclusion of the categorical origin airport type (small, medium, or large size) during the modeling phase. Delays do not appear to be concentrated in regional patterns, and locations outside the continental US did not exhibit significantly different behavior. The visual is limited because it does not display cancellations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d07def6d-7d2b-4bfc-a746-67eea039804b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "%md\n",
    "We also explored airport-specific associations with delay. In this figure, the marker size represents the relative proportion of flights departing from the airport during the first three-quarters of the year, with a minimum size for visibility, and color represents continuous delay amount in minutes. Less busy airports appear to have more severe delays. This motivated our inclusion of the categorical origin airport type (small, medium, or large size) during the modeling phase. Delays do not appear to be concentrated in regional patterns, and locations outside the continental US did not exhibit significantly different behavior. The visual is limited because it does not display cancellations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px; padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/airportdelays.jpg?raw=true\" alt=\"Join Pipeline\" style=\"width: 50%; height: auto; display: inline-block;\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5d5c825-8b20-4889-bd46-a5a52ad343ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Weather\n",
    "\n",
    "Correlation analysis was conducted on the training dataset, which consists of the first four years of the dataset (2015-2018 inclusive). Initial analysis focused on the relationship between weather features and a constructed departure delay indicator, which identifies flights that were delayed by more than 15 minutes or canceled. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40f7b7ad-6b5e-4c76-b532-84bf5e526273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px; padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/weatherspearman.png?raw=true\" alt=\"Spearman Correlation\" style=\"width: 50%; height: auto; display: inline-block;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adf45353-056d-46b2-b714-697490cf9e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Spearman correlation results indicated relatively low correlations overall. The strongest correlations with flight delays were observed for precipitation amount (measured in hundredths of an inch) and wind gust speeds, both of which were positively associated with flight disruptions. Additionally, several weather features were found to be highly correlated with one another, suggesting that they may not contribute additional variance or new information to the model. The low correlation matches previous results whis showed that weather delay minutes represent a very small proportion of overall delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5deaaf6f-f4b9-49cb-ac10-a1a14b382bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "To augment the features available natively in the flights and weather datasets, we engineered features related to prior flights, lagged delay statistics, seasonality, and graph features. Unless otherwise specified, all exploratory analysis of engineered features is conducted on the training dataset, i.e. spanning the four years from 2015-2018 inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78344194-30fc-452b-8ed2-4767fe72f251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prior Flight / Recency Features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f29705a2-8979-47d8-8def-7bec4fcda023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65cd1164-5dbd-4604-80e7-1119578aff83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "One of our primary engineering focuses was recency features, based on the hypothesis that operational status indicators from the preceding flight leg of an aircraft—such as whether the aircraft was delayed, cancelled, departed, or arrived—would be highly predictive of disruption outcomes for the aircraft at the current origin. This decision was further supported by initial exploratory data analysis (discussed further below): Spearman correlation coefficients between raw features and the target variable revealed limited signal in most static flight attributes, and distributional comparisons showed little meaningful variation. This aligns with domain intuition: disruptions in the aircraft’s prior leg are likely to propagate and impact on-time performance at the next origin, which is supported by the sucess in model performance using these engineered features.\n",
    "\n",
    "Among the recency-based features we created, we selected the following estimates (i.e., calculated only from information known at or before 2 hours before scheduled departure) for modeling:\n",
    "\n",
    "1. Binary indicators capturing prior flight’s status:\n",
    "   - Whether it departed from its previous origin\n",
    "   - Whether it was delayed at its previous origin\n",
    "   - Whether it was cancelled at its previous origin\n",
    "   - Whether it arrived at the current origin\n",
    "\n",
    "2. Continuous timing features (in minutes):\n",
    "   - Departure delay at the previous origin\n",
    "   - Air time of the prior flight\n",
    "   - Turnaround time between the prior arrival and scheduled departure of the current flight\n",
    "\n",
    "\n",
    "When incorporating aircraft tracking data, we focused on addressing two major concerns: data quality issues and leakage.\n",
    "\n",
    "_Data Quality_\n",
    "\n",
    "We defined a prior flight by three conditions:\n",
    "\n",
    "1. Consistent aircraft identified by tail number\n",
    "2. The aircraft's immediate previous destination matches the current origin\n",
    "3. The aircraft left its immediate previous origin less than 24 hours before the current flight's T-2 scheduled departure time\n",
    "\n",
    "Our first condition assumes that a flight's actual tail number, i.e. assigned aircraft, is known at the time of evaluation. The second condition is motivated by observed inconsistencies in aircraft flight routes. For example, in one day an aircraft arrives at airport A, yet the next record of the same aircraft shows it departing from airport B, with no flight record of its flight from A to B. This condition intends to enforce data integrity by ensuring a prior flight really is the flight that aircraft completed to arrive at the current origin. The third condition is also motivated by the possibility of missing flight records and upholding the integrity of the meaning of a prior flight. There exist records where a plane's prior flight to its current origin may be several days or even months in the past. We believe a \"prior flight\" that happened too far in the past does not affect current flight delay in the way we are are hoping to capture via these recency features. Furthermore, because we don't understand the context for these gaps we consider the possibility that true prior flight activity records are not present. \n",
    "\n",
    "These filters helped reduce the risk of incorporating misleading features derived from incomplete route chains and uphold the expected meaning of our engineered features.\n",
    "\n",
    "_Leakage_\n",
    "\n",
    "We only wanted to incorporate information that would be known at the threshold T-2 hours before the scheduled departure time. This influenced the variables considered in our calculations, based on whether the estimated or actual timestamp data would be available, and how much of the continuous time duration data would be available.\n",
    "\n",
    "Two core assumptions were made: Firstly, that all prior flights are scheduled more than 2 hours before a record's scheduled departure time. Secondly, that an airport would know at the time threshold whether the immediate prior flight of an aircraft was cancelled. This is because we do not know at what point a flight is declared cancelled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f887174-0dda-4dc6-bd9b-50e302121437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Methods\n",
    "\n",
    "We began by calculating a threshold timestamp for 2 hours prior to each flight’s scheduled departure. Using this, we generated lagged features over the aircraft tail number (i.e., unique aircraft identifier), including origin and destination airports, scheduled and actual departure times, delays, and arrival times.\n",
    "\n",
    "Contingent on meeting the prior flight information criteria, we created the following features\n",
    "\n",
    "_Cancellation_: \n",
    "- Indicator (boolean): A binary flag indicating whether the previous flight was cancelled. No restriction on timing was applied, as cancellations are often logged early and knowing about them before the prediction threshold (2 hours prior to departure) aligns with our use case.\n",
    "\n",
    "_Delays:_\n",
    "- Continuous variable (minutes): Estimated delay of the prior flight, computed based on available data:\n",
    "\n",
    "   - If the prior flight scheduled departure and recorded departure were both before the threshold, we  simply used the true recorded delay value.\n",
    "   - If the prior flight was scheduled to depart before the threshold, but did not have a recorded true departure time yet, we did not attempt to estimate what the further delay might be. Instead, we essentially made the assumption that it departed at the 2 hours prior UTC time by recording the delay as the difference between the threshold and the prior flight scheduled departure time. In the future, this could be fine tuned by setting a default parameter relative to the estimated prior flight time or estimated based on some other indicator, but it only accounted for a small proportion of cases and we did not want to introduce additional computational overhead.\n",
    "   - If the prior flight was scheduled to depart after the threshold, and the route information met the standard, we assumed there would be no delay, as we do not have cause to believe there might be. This could also be tuned by calculating average delay for that route, but as this only represented a small portion of cases, we similarly hesitated to introduce computationally intensive operations.\n",
    "\n",
    "   - If the prior flight was cancelled or the route information was missing, leaving us without data on the prior flight, we filled in the delay calculation with the most recent non-null delay data from the same route’s previous leg (i.e., the same origin-destination pair). Since we don't have the specific prior leg information, we instead look for the most recent available instance for the same route (same origin and destination) and use the delay from that prior flight as a proxy for the current flight’s delay. \n",
    "    \n",
    "       - This decision is based on the understanding that operational disruptions, including delays, are often correlated within the same route. Delays from one leg of a flight route are likely to impact subsequent flights on the same route. The rationale was further validated by EDA on the initial engineered features, which showed that when the prior flight's destination did not match the current flight's origin—an issue that occurred in 3% of the training dataset—58% of those cases led to disrupted outcomes (delays or cancellations). \n",
    "\n",
    "\n",
    "\n",
    "- Indicator (boolean): If the prior flight was estimated to have been delayed, or known to have been cancelled, the delay indicator was set to True.\n",
    "\n",
    "\n",
    "_Departures_: \n",
    "\n",
    "- Indicator (boolean): If and only if the known prior flight departure time met the data quality standard and was before the threshold the boolean prior flight departure indicator was set to true. \n",
    "\n",
    "- Estimator (timestamp): The prior departure time was estimated by adding the estimated delay calculation to the scheduled departure time.\n",
    "\n",
    "_Arrivals_:\n",
    "\n",
    "- Indicator (boolean): If and only if the prior flight known arrival time met the data quality standard and was before the threshold, the indicator was set to True.\n",
    "\n",
    "- Estimator (timestamp):\n",
    "   - If the prior flight arrived before the 2-hour window, we filled this in with the true arrival time.\n",
    "   - If the prior flight was known to have departed before the threshold, we filled this in by adding the estimated elapsed time to the known departure time.\n",
    "   - Otherwise, we simply added the estimated elapsed time to the estimated departure time.\n",
    "\n",
    "\n",
    "_Turnaround time_: \n",
    "\n",
    "- To calculate the estimated amount of time the aircraft had between arriving and departing, we took the difference between the estimated arrival time of the previous flight and the estimated departure time of the current flight. If the previous flight was not confirmed, we again estimated this from the calculated turnaround time from the last record of this route being flown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c329a2e6-cd09-41b0-92c4-46efd366150c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px; padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/turnaroundhist.png?raw=true\" alt=\"Turnaround Time Histogram\" style=\"width: 50%; height: auto; display: inline-block;\">\n",
    "\n",
    "\n",
    "The bar chart displays the frequencies of estimated turnaround times, broken out by disrupted vs non disrupted flights. We can see that both disrupted and non-disrupted flights have similar turnaround time distribution, with most flights from both classes experiencing relatively low turnaround times. This indicates that estimated turnaround time by itself may not be a helpful feature for the models to distinguish between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb6e30a-744b-4b2f-b276-b25d0ba82fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px; padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/priorflightspearman.png?raw=true\" alt=\"Prior Flight Correlation Matrix\" style=\"width: 50%; height: auto; display: inline-block;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "115d5ff5-75d7-44fd-b444-e1f3ebd94eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Spearman correlation heatmap relating prior flight features to disrupted flights shows a surprisingly minimal correlation between the prior flight estimated delay time and disruption of the current flight. This could indicate that the estimation is too naive, i.e. not a good estimator of the actual prior flight delay time. However, we also note that there is a very small correlation with whether the prior flight is estimated to be delayed, a higher-level estimator that has less room for inaccuracy, so instead the no-correlation could indicate that even if the prior flight is delayed, it is able to make up the delay time in the air so as not to cause a disruption for the current flight.\n",
    "\n",
    "As expected, we see a slight negative correlation between flight disruption and whether the prior flight is known to have departed and arrived, and a stronger, positive correlation with whether the prior flight was cancelled. \n",
    "\n",
    "Overall, compared to the Spearman weather correlations, features like estimated turnaround time and an indicator for whether the prior flight is known to have departed show stronger correlations with flight disruptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3d4c982-1861-43b9-b039-db1b482474e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delay Statistics Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71dcd0ad-ab16-491d-a2d6-1e13256e7e34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Overview\n",
    "\n",
    "Many flight delays can be attributed to widespread events happening regionally or at a specific airport, *e.g.* a grounding due to lightning or a computer system glitch. In this way, having information about other flights that have recently been delayed at a given airport can provide valuable insight into whether an upcoming flight will be delayed.\n",
    "\n",
    "With this in mind, we created engineered features to describe the average delay amount and average proportion of flights disrupted at the origin airport for a given record, considering flights departing between two and four hours prior to the record of interest. Note that this time window was chosen based on gut feeling; in future work we would like to explore the effect of expanding or shrinking this window.\n",
    "\n",
    "##### Methods\n",
    "\n",
    "The delay statistics features were calculated as lagged moving window averages using Spark SQL's Window functionality. First, a Window was constructed to capture all records between 0 and 4 hours prior to a given flight record at the flight's origin airport, based on scheduled departure time. The windowed data was processed using a user-defined function operating on the scheduled, actual, and two hours prior to scheduled departure times, and the delay amount and disruption status within the window:\n",
    "\n",
    "- The maximum \"two hours prior to scheduled\" time within the window (corresponding to the \"0 hours prior\" record _i.e._ the \"two hours prior\" value for the record of interest) was used to define the maximum time cutoff for leakage\n",
    "- For all records within the window having actual departure time prior to the leakage cutoff:\n",
    "  - Take the average of the departure delay\n",
    "  - Take the average of the disruption status\n",
    "\n",
    "This processing resulted in the following features:\n",
    "\n",
    "| Feature |  Data Type | Description |\n",
    "|:---:|:---:|:---:|\n",
    "| mean_dep_delay | Float | Mean departure delay of flights at origin airport departing 2-4 hours prior to the flight of interest |\n",
    "| prop_delayed | Float | Proportion of flights delayed at origin airport, of those departing 2-4 hours prior to the flight of interest |\n",
    "\n",
    "As shown in the Spearman heatmap correlation below, for the 2015-2019 data, we see moderate correlation between these engineered features and our outcome variable. Therefore, they show promise in being able to help our models better predict flight disruption status.\n",
    "\n",
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/delay_stats_correlation.png?raw=true\" alt=\"Delay Stats Heatmap\" style=\"width: 100px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "150b7f0c-f0fd-444b-9de5-b9df1b8d8a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Seasonality Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be253ec1-3aa5-4604-a3e8-057e8aba421a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Overview\n",
    "\n",
    "We expect fluctuation in flight delays on a number of timescales, as travel demand and airports' ability to meet that demand vary over time. For example, delays vary throughout the day with the volume of traffic at an airport, and as delays from earlier in the day impact later flights. Delays also vary throughout the year as travel habits change--*e.g.* consider spring break, winter holidays, summer travel, or ski trips. We have engineered seasonality features to capture these effects quantitatively, in order to provide the model input about what seasonal effects may be at play for a given record.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8714de08-a8c5-4fb9-8e56-fa3f39d07b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Methods\n",
    "To produce these features, we trained seasonality models using the Prophet Python library [5]. For a given training dataset (for each cross-validation fold and overall), a Prophet model was fit for each airport using the UTC departure time as the time field and departure delay in minutes as the outcome variable. Each model assumed linear growth, an uncertainty interval width of 90%, and included weekly, daily, yearly, and holiday (based on Prophet's US holiday lookup functionality) seasonality components. Each model was used to forecast predictions one week into the future, with an hourly frequency (*i.e.* to get the daily and weekly components for each hour throughout the week), and one year into the future with a daily frequency (*i.e.* to get the yearly and holiday components for each day of the year). These components, along with the airport identifier, were stored in lookup tables. The example below shows the seasonality components for Boston Logan International Airport (BOS), trained on the 2015-2018 training set.\n",
    "\n",
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/BOS_seasonality_full_data.png?raw=true\" alt=\"BOS Seasonality\" style=\"transform: scale(0.2);\">\n",
    "<div>\n",
    "\n",
    "To apply these seasonality components to the modeling data, the modeling data was joined to this lookup table. The daily and weekly components were joined on airport, day of week, and hour of day; and the yearly and holiday components were joined on airport, month, and day of month. The following table summarizes the resulting features.\n",
    "\n",
    "| Feature |  Data Type | Description |\n",
    "|:---:|:---:|:---:|\n",
    "| daily | Float | Daily seasonality component (offset from trend) in minutes |\n",
    "| weekly | Float | Weekly seasonality component (offset from trend) in minutes |\n",
    "| yearly | Float | Yearly seasonality component (offset from trend) in minutes |\n",
    "| holidays | Float | Holiday-related seasonality component (offset from trend) in minutes |\n",
    "\n",
    "Because these seasonality components are *trained* from data, we had to be mindful of leakage when creating these features. To ensure that our cross-validation and overall test sets were not contaminated with test data information, we trained a seaparate seasonality model for each cross-validation fold and the overall dataset, utilizing the relevant training dataset in each case (*e.g* the seasonality trained on CV fold 1 training data was applied to the CV fold 1 training and test sets). This does cause some leakage when the seasonality is applied to the same training set that it was trained on, but avoids leakage during evaluation on the test set. In future work, we'd like to train seasonality models on a rolling basis for smaller chunks of data (_e.g._ quarterly) and apply seasonality models only to data occurring after all of the seasonality training data. This would ensure that both our training and test sets would be leakage-free.\n",
    "\n",
    "The below figure summarizes the Spearman correlation of the seasonality features with the outcome variable for the full 5 year (2015-2019) dataset. We see that daily seasonality is moderately correlated with the outcome, and yearly seasonality to a lesser extent, but the weekly and holiday features are only very weekly correlated with the outcome.\n",
    "\n",
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/seasonality_correlation_full.png?raw=true\" alt=\"Seasonality Heatmap\" style=\"width: 200px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b4941a-13c0-43c7-becb-c0e52a70be5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Graph Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e767992c-0267-4e8c-918a-f7024b53a933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The PageRank of the origin airport was computed within each cross-validation fold. The idea is that a higher PageRank indicates an airport that is well-connected to other busy airports, which could serve as a proxy for delay likelihood (e.g., busy hubs receiving traffic from other busy hubs). The PageRank scores were calculated per cross-validation fold, then combined over the train set to form a set of scores over the complete four years. The scores produced in the final fold were used for the test dataset. \n",
    "\n",
    "\n",
    "For the training folds, using the pagerank score is analogous to assuming access to scheduled flights over a one-year horizon, since specific flight characteristics like cancellations were not accounted for. If this assumption is not met, there is leakage within the training set. Alternative methods for dynamically generating centrality measures were explored, such as a message-passing network between airport nodes. The network was developed and implemented on a subset of data, but the approach was not further pursued as computational complexity outweighed the expected performance gains.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7a1c8b-c978-456b-856c-7d55dbebbe65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 6; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/pagerankeda.png?raw=true\" alt=\"BOS Seasonality\" style=\"transform: scale(0.2);\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ea0b40-1cc7-456c-8a52-00f1580b599f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Looking at disruptions from 2015-2018 by pagerank score, we see that the distributions are almost identical. Disrupted flights have a slightly wider IQR, but otherwise, Pagerank score alone does not appear to distinguish disrupted versus non disrupted flights. The density plot shows that Pagerank scores are clustered near zero for most airports, with a long right tail representing a small number of highly connected hubs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d8e5f44-299a-4e83-9dcd-3698df481113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Leakage\n",
    "\n",
    "The following section provides an analysis of possible leakage in our data preparation pipeline.\n",
    "\n",
    "### What is Leakage?\n",
    "\n",
    "Leakage in machine learning happens when information from outside the training dataset—or from the future—accidentally sneaks into the model in a way that wouldn’t be available at prediction time. This can lead to overly optimistic performance during training and evaluation, but poor performance in real-world settings because the model is learning from data it shouldn’t have had access to.\n",
    "\n",
    "### Example of Leakage\n",
    "\n",
    "In our context, imagine you’re trying to predict if a flight will be disrupted two hours before departure. If you include a weather observation taken at the scheduled departure time or if you use the status of an incoming aircraft that hasn’t arrived yet—you’re giving the model future information it wouldn’t have access to in real-time. That’s leakage.\n",
    "\n",
    "### Pipeline Review – Are We Leaking?\n",
    "\n",
    "We were very intentional about avoiding leakage when building our pipeline. For example:\n",
    "- We only used the scheduled departure time `CRS_DEP_TIME` instead of the actual departure time because we won't have access to the actual departure time in real time. \n",
    "- We aligned weather data to two hours before scheduled departure, never using data closer to the schduled departure or after takeoff.\n",
    "- We carefully joined based on location and time zones to ensure realistic data alignment.\n",
    "- We used the 2 hours before the scheduled departure time as a threshold to bring any other features.\n",
    "\n",
    "However, we did identify three key points of potential leakage:\n",
    "\n",
    "1. **Prior Flight Status Assumption**\n",
    "\n",
    "    We made the decision to assume the status of the previous flight (e.g., cancellation or delay) was known two hours before the current flight.\n",
    "\n",
    "      **Why it’s a problem:** In real-time, this may not always be true. The prior flight might still be en route or awaiting departure.\n",
    "\n",
    "      **Impact:** The model might be trained with more accurate context than would exist at prediction time.\n",
    "\n",
    "2. **PageRank on the Full data**\n",
    "\n",
    "    We computed graph-based PageRank features using the entire flight network.\n",
    "    \n",
    "      **Why it’s a problem:** This approach allows “future” routes to influence the importance of nodes (airports) in the graph.\n",
    "\n",
    "      **Impact:** It gives the model indirect access to information from the future, which wouldn’t be available in real time.\n",
    "\n",
    "3. **Prophet Models Applied to Data they were Trained On**\n",
    "\n",
    "    Our seasonality features were generated using Prophet models that were trained on each fold's training split.\n",
    "\n",
    "      **Why it’s a problem:** For the training splits, seasonality features include information about future seasonal trends (e.g., holiday surges) that wouldn’t be known in advance.\n",
    "\n",
    "      **Impact:** This violates the assumption that seasonal patterns must be learned only from past data. \n",
    "\n",
    "### Steps to Prevent Leakage Going Forward\n",
    "\n",
    "We will be working to ensure the final models don't suffer from these issues:\n",
    "\n",
    "- For **prior flight features**, we’re re-evaluating how we simulate real-time knowledge and will likely restrict this feature to more conservative assumptions or delay signals (e.g., scheduled vs. actual gaps).\n",
    "- For **PageRank**, we plan to compute it incrementally over time windows—e.g., quarterly, monthly or weekly—so the model only sees past network patterns.\n",
    "\n",
    "- For **seasonality**, we’ll retrain Prophet using only data prior to the prediction period using rolling windows-e.g., annually, quarterly,or monthly-to simulate realistic forecasting scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c3b897b-9765-4dce-91ab-f58b3589d227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modeling Pipeline\n",
    "\n",
    "The following steps and diagram outline our end-to-end modeling workflow. The remainder of this section provides additional details for each step.\n",
    "\n",
    "1. **Ingestion:** Load raw data into Spark DataFrames\n",
    "2. **Feature selection:** Drop unnecessary columns\n",
    "3. **Join:** Combine data sources into joined DataFrame\n",
    "4. **Feature engineering and imputation:** Add engineered features and fill missing values using time series methods\n",
    "5. **Split:** Divide data into training, validation, and test splits\n",
    "6. **Sample:** Undersample training data\n",
    "7. **Define machine learning pipelines:** Create Spark Pipeline objects for feature transformations and modeling\n",
    "8. **Hyperparameter tuning:** Use cross-validation to train a model that balances performance and generalizability\n",
    "9. **Model training:** Train final model(s) using chosen hyperparameters\n",
    "10. **Model evaluation:** Assess trained models on test data\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/ml_pipeline.png?raw=true\" alt=\"ML Pipeline\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59c8041c-16b3-43fb-8d6d-17aada9839ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestion, Feature Selection, Join, and Feature Engineering\n",
    "\n",
    "Raw data were loaded from the provided parquet files and unnecessary columns were dropped on the basis of relevance (as discussed in **Data Dictionary**) or missing value status (as discussed in **Missing Value Analysis**). The weather and flights data were joined (as discussed in **Joining Strategy**) and saved out to an intermediate parquet file. The engineered featuers were calculated and added (as discussed in **Feature Engineering**) and the results saved to another intermediate parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dc0ef56-3ecd-45ad-81e1-f40892c24dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train, Test, and Cross Validation Splits\n",
    "\n",
    "For the five year dataset, we trained our machine learning models on the first four years (2015-2018) and tested on a held out set consisting of the last year (2019). To validate our models and tune hyperparameters, the training set was further split into 5 cross-validation folds with 20% overlap. The folds and overlap were defined in terms of number of days (*i.e.* the folds were split so that each included the same number of days' worth of data), with the assumption that this would produce splits with comparable numbers of record. See the table below for the date limits (each date cutoff corresponds to midnight UTC) of data included in each split.\n",
    "\n",
    "Each fold had approximately 4.5 million records in train and test each (before downsampling the training data), and the full train and test sets containted 24,313,177 (before downsampling) and 7,409,309 records, respectively. See the table below for date limits of data included in each split, for each fold. Note that the maximum times are exclusive.\n",
    "\n",
    "| Modeling Case |  Train Time Period | # Train Records(Downsampled) | Test Time Period | # Test Records |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| CV Fold 1 | 12/31/14 - 10/09/2015 | 1,840,924 | 10/09/2015 - 07/17/2016 | 4,337,763 |\n",
    "| CV Fold 2 | 08/14/2015 - 05/21/2016 | 1,426,778 | 05/21/2016 - 02/27/2017 | 4,323,858 |\n",
    "| CV Fold 3 | 03/27/2016 - 01/01/2017 | 1,591,596 | 01/01/2017 - 10/10/2017 | 4,418,889 |\n",
    "| CV Fold 4 | 11/08/2016 - 08/14/2017 | 1,805,438 | 08/14/2017 - 05/23/2018 | 4,889,432 |\n",
    "| CV Fold 5 | 06/22/2017 - 03/27/2018 | 1,742,243 | 03/27/2018 - 01/01/2019 | 5,587,812 |\n",
    "| Overall | 12/31/14 - 01/01/2019 | 9,353,252 | 01/01/2019 - 01/01/2020 | 7,409,309 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "880498d6-bf49-4f4a-ad14-1791fb0c5cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sampling Strategy\n",
    "\n",
    "\n",
    "The computations of datapoint distances used in oversampling/SMOTE is not favorable considering the size of our data, so we took advantage of the robustness of our observations and used undersampling to address the class imbalance in our outcome variable. We randomly sampled the majority class without replacement to create a balanced dataset where the number of samples in each class was roughly equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b045d7e-a214-468d-93c0-607784a68ba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Machine Learning Pipelines\n",
    "\n",
    "We used the Pyspark Pipelines API to transform our chosen features and train machine learning models. This subsection explains our outcome variable, which features we used for modeling, and how we defined our Pipeline to transform those features and perform modeling.\n",
    "\n",
    "#### Outcome variable\n",
    "\n",
    "We have chosen to pursue a binary classification problem characterizing a flight's departure disruption status. Our outcome variable is disruption status: a binary flag indicating whether or not a flight was either delayed (using the FAA definition of 15 or more minutes late) or cancelled. We choose to include cancellations in our \"disrupted\" case since they have similar consequences for our stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96424748-8f77-4a2d-be62-9c8abe5ef90f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Feature Families\n",
    "\n",
    "We explored 7 different feature families in our modeling experiements. The below table summarizes the features in each family.  We have provided aliases of the variable names for the purpose of discussion:\n",
    "\n",
    "| Feature Family (# Features) | Raw Feature Name | Type | Raw or Engineered | Description |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| **Numeric Weather Features (9)** | origin_HourlyDewPointTemperature | Float | Raw | Hourly dew point temp. at origin airport |Weather-DewTemp|\n",
    "|  | origin_HourlyDryBulbTemperature | Float | Raw | Hourly dry temp. at origin airport |Weather-DryBulbTemp|\n",
    "|  | origin_HourlyPrecipitation | Float | Raw | Hourly precipitation at origin airport |Weather-Precipitation|\n",
    "|  | origin_HourlyPressureChange | Float | Raw | Hourly pressure change at origin airport |Weather-Pressure|\n",
    "|  | origin_HourlyRelativeHumidity | Float | Raw | Hourly relative humidity at origin airport |Weather-Humidity|\n",
    "|  | origin_HourlyVisibility | Float | Raw | Hourly visibility at origin airport |Weather-Visibility|\n",
    "|  | origin_HourlyWetBulbTemperature | Float | Raw | Hourly wet bulb temp. at origin airport |Weather-WetBulbTemp|\n",
    "|  | origin_HourlyWindGustSpeed | Float | Raw | Hourly wind gust speed at origin airport |Weather-WindGust|\n",
    "|  | origin_HourlyWindSpeed | Float | Raw | Hourly wind speed at origin airport |Weather-Wind|\n",
    "| **Flight Categorical Metadata (5)** | OP_UNIQUE_CARRIER | Categorical | Raw | Carrier (airline) |Flight-Carrier|\n",
    "|  | ORIGIN_ICAO | Categorical | Raw | Origin airport |Flight-Origin|\n",
    "|  | DEST_ICAO | Categorical | Raw | Destination airport |Flight-Dest|\n",
    "|  | origin_type | Categorical | Raw |  Origin airport type |Flight-OType|\n",
    "|  | dest_type | Categorical | Raw |  Destination airport type |Flight-DType|\n",
    "| **Date Information (5)** | YEAR | Categorical | Raw | Year of flight date |Date-Year|\n",
    "|  | QUARTER | Categorical | Raw | Quarter of flight date |Date-Quarter|\n",
    "|  | MONTH | Categorical | Raw | Month of flight date |Date-Month|\n",
    "|  | DAY_OF_MONTH | Categorical | Raw | Day of month of flight |Date-DoM|\n",
    "|  | DAY_OF_WEEK | Categorical | Raw | Day of week of flight |Date-DoW|\n",
    "| **Seasonality Components (4)** | daily | Float | Engineered | Daily seasonlity component from Prophet model |Seasonality-D|\n",
    "|  | weekly | Float | Engineered | Weekly seasonality component from Prophet model |Seasonality-W|\n",
    "|  | yearly | Float | Engineered | Yearly seasonality component from Prophet model |Seasonality-Y|\n",
    "|  | holidays | Float | Engineered | Holiday-based seasonality component from Prophet model |Seasonality-H|\n",
    "| **Prior and Current Flight Details (8)** | priorflight_elapsed_time_calc_raw | Float | Engineered | Estimated prior flight duration |PF-Duration|\n",
    "|  | turnaround_time_calc | Float | Engineered | Calculated time between prior flight arrival and present flight departure |PF-TurnaroundT|\n",
    "|  | priorflight_depdelay_calc | Float | Engineered | Estimated prior flight delay |PF-DelayT|\n",
    "|  | priorflight_isdeparted | Boolean | Engineered | Indicates whether prior flight has departed |PF-IsDepart|\n",
    "|  | priorflight_isarrived_calc | Boolean | Engineered | Indicates whether prior flight has arrived |PF-IsArrive|\n",
    "|  | priorflight_isdelayed_calc | Boolean | Engineered | Indicates whether prior flight was delayed |PF-IsDelay|\n",
    "|  | DISTANCE | Float | Raw |  Distance between origin and destination airports |Distance|\n",
    "|  | CRS_ELAPSED_TIME | Float | Raw |  Scheduled flight duration |CF-SchedDuration|\n",
    "| **Lagged Delay Stat Features (2)** | mean_dep_delay | Float | Engineered | Mean departure delay at origin airport 2-4 hours prior |Lag-MeanDelay|\n",
    "|  | prop_delayed | Float | Engineered | Proportion of flights delayed at origin airport 2-4 hours prior |Lag-PropDelay|\n",
    "| **Graph Features (1)** | pagerank | Float | Engineered | Origin airport PageRank value |GF-Pagerank|\n",
    "\n",
    "Note that we have chosen not to employ feature selection techniques in our modeling, since the total number of features in use is relatively small.\n",
    "\n",
    "The importance of the features families was explored via two methods: Random Forest modeling experiments and XGBoost importance scoring.\n",
    "\n",
    "For the modeling experiments, we trained one Random Forest model per feature family that excluded that feature family from the input features, to asses the impact to the modeling results. For more details, see the **Modeling Experiments** section. From this analysis, we found that removing either the seasonality, weather, or lagged delay statistics families negatively impacted model performance, while removing the other feature families actually improved performance, all else equal.\n",
    "\n",
    "We also explored feature importance by extracting the importance scores from our trained XGBoost model. The below figure summarizes the sum of weight, average cover, and average gain by family.\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/feat_imp_fam_v3.png?raw=true\" alt=\"Feature Importance by Family\" style=\"width: 500px\">\n",
    "<div>\n",
    "\n",
    "Note that in the figure above, \"time\" refers to the Lagged Delay Stat feature family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a49d882-3ece-41e3-8510-e75f39f299fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Examining the feature family importance metrics, we see that the highest weights are represented by weather features, even though weather features have a lower average gain. This could be be because the weather family has the most continuous features, so it could be used in more splits based on different ranges. In other words, the weather features are used for splits often, but don't usually result in large improvements in model performance. Metadata features, like the origin airport and flight carrier, are not used often for splits often, as shown by their lower weight, but tend to affect many samples, as shown by their higehr average cover. \n",
    "\n",
    "According to the average gain graph, time features, i.e. mean departure delay and delay proportion, contribute the most to model improvement, even though they are not the most frequently used. This implies they have the most value per use. Prior flight features account for the second highest increase in average gain, are used often for splits and affect many samples. Even though all prior flight features are calculated with information that is at at least 2 hours stale at the time of scheduled departure, they are impactful in improving model performance. This suggests that further development should go into prior flight features. \n",
    "\n",
    "Somewhat surprisingly, the categorical date features account for a larger share of the average gain increase than the seasonality features. This could suggest redundancy or interference between the two, where the presence of one makes the other less informative to the model.\n",
    "\n",
    "Overall, we note that engineered feature families account for the largest proportions of increases in average gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11cf0560-fbca-4665-973e-c00562e6a7fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Feature Transformations\n",
    "\n",
    "Our final feature set consisted of both numeric and categorical features, which were handled separately within the pipeline. Numeric features were scaled using Spark's MinMaxScaler. Categorical features were one-hot encoded using Spark's StringIndexer (maps text categories to integers) and OneHotEncoder (one-hot encodes integer categories). The transformed numeric, categorical, and interaction features were assembed using Spark's VectorAssembler, to be used as input to the classification object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53657389-51f4-4cf0-8395-9c6a57c93dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Spark Pipeline Diagram\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/data_transformation_pipeline.png?raw=true\" alt=\"Data Transformation Pipeline\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a72b57fb-4cdd-4ec1-adcc-e3d8cc1240cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Modeling\n",
    "\n",
    "We explored four machine learning modeling architectures throughout our experiments. In all cases, time series cross fold validation was used to obtain training and validation scores. For the multilayer perceptron and XGBoost models, all five training models developed for cross-validation were used to generate predictions on the test set, then exponentially weighted, such that more recent folds had a higher weight, and averaged to produce a final prediction. After some experimentation, the exponential weighting parameter used was 0.5. For the random forest and logistic regression models, one single pipeline was fit to the entire training set after cross-validation was completed, and used to predict on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c69f3f-f7a8-4fe1-8c2a-7163965bad5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "##### Logistic Regression\n",
    "\n",
    "We chose logistic regression for our baseline model because of its suitability for a binary outcome, interpretability, and strong performance on linearly separable data. With a simple set up and hyperparameter tuning, logistic regression was a straightforward but strong method with our engineered fearures.\n",
    "\n",
    "##### Random Forest\n",
    "Random forest was chosen because bagged methods perform well at scale. Additionally, we hoped that using an ensemble would mitigate overfitting while also providing interpretable feature importance artifacts. Another benefit of tree methods is their ability to handle null values.\n",
    "\n",
    "##### XGBoost\n",
    "Similarly to random forest, XGBoost was chosen for its interpretability and its capacity to optimally leverage our engineered features. We anticipated a slower training time compared to random forest due to the sequential boosted nature but felt that the gains in performance were valuable.\n",
    "\n",
    "##### Multi-layer Perceptron\n",
    "A multilayer perceptron model was also chosen for its flexibility and ability to handle complex relationships through deep learning techniques. The tradeoffs of this model include its low interpretability and higher training time, and its reliance on finding the optimal hyperparameter combination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9853bf-7f07-4ff3-b335-6376fe1556c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modeling Experiments, Hyperparameter Tuning, and Training\n",
    "\n",
    "We performed modeling experiments to assess and compare the performance of our four chosen architectures, the effect of adjusting hyperparameters, and the importance of feature families. For hyperparameter tuning, we used the Python hyperopt library, which uses Bayesian optimization to efficiency explore the parameter space. See more details in the **Modeling Experiments** section.\n",
    "\n",
    "For each experiment, a model was first trained on each fold of the cross-validation data and output predictions to evaluate cross-validation performance. They were then trained on the full training dataset and output predictions for the held out test set. For all training, binary crossentropy loss (equation below) was used as the loss function.\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(\\hat{p}^{(i)})+(1-y^{(i)})\\log(1-\\hat{p}^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecbdf5ae-e432-45e9-bd3d-84c57922c1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Evaluation\n",
    "\n",
    "In choosing an appropriate metric to evaluate our classification performance, we weighed the cost of each classification error:\n",
    "\n",
    "**Type I error** (*false positive:*) The model predicts a delay, but the flight departs on time) this may cause confusion and unnecessary changes from air traffic control. however, it's more acceptable if we are prioritizing caution and want to minimize unexpected delays.\n",
    "\n",
    "**Type II error** (*false negative:*) The model predicts the flight will be on time, but it ends up delayed) Passengers and crew aren’t prepared for the delay, potentially leading to missed connections, poor customer satisfaction, and operational disruptions. This type is more costly if unexpected delays cause major disruptions, and you aim to avoid them at all costs.\n",
    "\n",
    "We estimate that Type II errors are roughly twice as costly as Type I errors to our stakeholder, therefore we choose the F-beta metric with a beta value of 2, to reflect the emphasis on recall over precision. The metric is defined below, where FP and FN refer to the number of false positives and false negatives the model predicts, respectively.\n",
    "\n",
    "$$F\\beta-Score = \\frac{\\beta^2 + 1}{\\frac{\\beta^2(TP + FN)}{TP} + \\frac{TP+FP}{TP}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3488f07-d7aa-40c9-b71a-fd5d6ddacfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modeling Experiments\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "We used a Bayesian optimization approach to perform hyperparameter tuning for our random forest and multi-layer perceptron architectures. We chose not to perform tuning for the logistic regression model because our Phase II work found minimal impact from incorporating regularization or incorporating interaction features. We chose not to tune the XGBoost models because adjusting most of the available hyperparameters would have made our model more conservative, and we did not see overfitting present in the cross-validation of our initial XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d4d69f-55ac-4280-b9dd-b8fc65eb427e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Multi-layer Perceptron\n",
    "\n",
    "| Description | CV Fold F2 Scores | Average Score | Test Score | Scores Std. Dev |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| Basline | [0.46, 0.51, 0.51, 0.47, 0.50] | 0.4912 | 0.4725 | 0.0235 |\n",
    "| No weather | [0.46, 0.51, 0.51, 0.47, 0.50] | 0.4912 | 0.4933 | 0.0235 |\n",
    "| Tuned | [0.45, 0.47, 0.49, 0.42, 0.46] | 0.4575* | 0.5254 | 0.0259 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f098899d-e6e3-4cbf-974a-7d9fcf364c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Three main MLP variants were considered to support feature selection before implementing hyperparameter tuning experiments. The base (default) set of features present in each variant were: \n",
    "\n",
    "- Seasonality features: daily, weekly, yearly, holiday\n",
    "- Delay statistic features: Mean departure delay, proportion delayed\n",
    "- Graph features: pagerank\n",
    "\n",
    "A model was run with limited flight features to help evaluate the predictive power of weather features.\n",
    "\n",
    "- Flight features: flight distance, scheduled elapsed time\n",
    "- Prior flight features: turnaround time calculation, estimated prior flight delay \n",
    "- Weather features: Hourly dew point temperature, dry and wet bulb temperatures, precipitation, pressure change, visibility, relative humidity, and wind and wind gust speeds\n",
    "\n",
    "Next, a model was run without any weather features with an emphasis on airport and aircraft characteristic features:\n",
    "- Flight features: Flight distance, scheduled elapsed time, airline carrier, quarter, month, day of month, day of week, origin type, origin region\n",
    "- Prior flight features: turnaround time calculation, estimated prior flight delay, prior flight departed indicator, prior flight arrival indicator, prior flight disruption indicator (estimated to be delayed > 15 minutes or known to be cancelled), prior origin region, prior flight carrier.\n",
    "\n",
    "Without the weather features, we noted this model actually performed better on the training and test sets without overfitting. This implies the weather features may have been adding unnecessary noise. Before proceeding to hyperparameter tuning, we therefore limited the set of weather features used; the final set of features, along with the default seasonality, delay statistic, and graph features was: \n",
    "\n",
    "- Weather features: Hourly dew point temperature, precipitation, wind gust speed, visibility, and pressure change \n",
    "- Flight features: Flight distance, scheduled elapsed time, airline carrier, quarter, month, day of month, day of week, origin type, origin region\n",
    "- Prior flight features: turnaround time calculation, estimated prior flight delay, prior flight departed indicator, prior flight arrival indicator, prior flight disruption indicator (estimated to be delayed > 15 minutes or known to be cancelled), prior origin region, prior flight carrier.\n",
    "\n",
    "The hyperparameters tuned were the hidden layers, step size, max iterations, and block size. The hidden layers determine the architecture and capacity of the neural network. The step size (learning rate) controls how much to adjust the model in response to the estimated error each time the model weights are updated. The max iterations set the number of times the model will work through the entire training dataset, and the block size specifies the number of training examples utilized in one iteration.\n",
    "\n",
    "| Parameter       | Values                              |\n",
    "|-----------------|-------------------------------------|\n",
    "| hidden_layers   | [[64, 32], [32, 8, 4], [128, 32]]   |\n",
    "| stepSize        | 0.01 to 0.3                         |\n",
    "| maxIter         | [100, 200]                          |\n",
    "| blockSize       | [64, 128]                           |\n",
    "\n",
    "\n",
    "The best model had 2 hidden layers of sizes [128, 32], max iterations of 200, and a step size of .0524, achieving an average F2 score across training folds of .51. As mentioned previously, each fold model was used to produce test set predictions. The class 1 probability predictions were combined in an exponentially weighted average such that the most recent fold (fold 5) had the most weight, then thresholded to produce the final prediction. This approach yielded a final F2 test score of 0.5254. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a41a7f9-329b-444b-bfbf-c59c8046735c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Random Forest\n",
    "\n",
    "To tune the random forest models, we ran 10 iterations of hyperopt tuning for the parameters numTrees (choice between 20, 40, or 60), maxDepth (between 5 and 12), and featureSubsetStrategy (choice between \"auto,\" \"sqrt,\" and \"log2\"). In all cases, we included all feature families in the input feature set. We got the following results:\n",
    "\n",
    "| Run | numTrees | maxDepth | featureSubsetStrategy | CV Fold F2 Scores | Average Score | Std. Dev. Score |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 1 | 60 | 7 | auto | [0.42, 0.46, 0.48, 0.40, 0.46] | 0.4455 | 0.0318 |\n",
    "| 2 | 60 | 5 | auto | [0.41, 0.45, 0.47, 0.39, 0.44] | 0.4298 | 0.0317 |\n",
    "| **3** | **20** | **10** | **auto** | **[0.44, 0.49, 0.50, 0.44, 0.47]** | **0.4676** | **0.0274** |\n",
    "| 4 | 60 | 10 | auto | [0.48, 0.50, 0.43, 0.47, 0.46] | 0.4645 | 0.0301 |\n",
    "| 5 | 20 | 7 | sqrt | [0.41, 0.47, 0.48, 0.44, 0.45] | 0.4493 | 0.0278 |\n",
    "| 6 | 20 | 9 | sqrt | [0.42, 0.47, 0.49, 0.42, 0.46] | 0.4531 | 0.0325 |\n",
    "| 7 | 40 | 11 | sqrt | [0.44, 0.48, 0.50, 0.43, 0.47] | 0.4654 | 0.0263 |\n",
    "| 8 | 40 | 11 | sqrt | [0.44, 0.48, 0.50, 0.43, 0.47] | 0.4652 | 0.0270 |\n",
    "| 9 | 60 | 9 | auto | [0.43, 0.48, 0.49, 0.41, 0.47] | 0.4573 | 0.0325 |\n",
    "| 10 | 60 | 8 | auto | [0.43, 0.47, 0.49, 0.42, 0.46] | 0.4531 | 0.0306 |\n",
    "\n",
    "From the above, we choose the model with the highest average CV F2 score (bolded above), with parameters of: numTrees of 20, maxDepth of 10, featureSubsetStrategy of \"auto.\" Training this model on the overall training data and evaluating on the test set, we get a test F2 score of 0.4564. To run the above set of hyperparameter tuning experiments on 5-10 workers took 1 hour, 28 minutes. Training the final model and evaluating on the test set took an additional 8 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "811ddeca-4375-451a-8071-59b867b9fd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Feature Importance Experiments\n",
    "\n",
    "As one method of assessing the importance of the feature families included in our models, we trained a model for each feature family that excluded those features from the model input. In all cases, we used the hyperparameters chosen by the hyperparameter tuning, discussed above. We got the following results:\n",
    "\n",
    "| Case | CV Fold F2 Scores | Average Score | Std. Dev. Score | Wall Time |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| All features | [0.44, 0.49, 0.50, 0.44, 0.47] | 0.4676 | 0.0274 | 96 min* |\n",
    "| No weather | [0.45, 0.47, 0.49, 0.42, 0.46] | 0.4575 | 0.0245 | 21 min |\n",
    "| No seasonality | [0.42, 0.48, 0.49, 0.40, 0.46] | 0.4502 | 0.0372 | 14 min |\n",
    "| No lagged delay stats | [0.45, 0.47, 0.48, 0.43, 0.45] | 0.4565 | 0.0187 | 18 min |\n",
    "| No prior flight | [0.45, 0.49, 0.49, 0.44, 0.50] | 0.4749 | 0.0267 | 18 min |\n",
    "| No graph | [0.42, 0.48, 0.50, 0.42, 0.48] | 0.4613 | 0.0379 | 24 min |\n",
    "| No date info | [0.44, 0.47, 0.51, 0.44, 0.48] | 0.4679 | 0.0286 | 13 min |\n",
    "| No flight metadata | [0.45, 0.49, 0.50, 0.44, 0.48] | 0.4713 | 0.0281 | 10 min |\n",
    "\n",
    "*Note that the wall times above correspond to training and evaluating on all cross validation splits and the over train/test sets using 5-10 workers. The wall time for \"All features\" includes hyperparameter tuning, as discussed above. For the remaining experiments, we used the tuned hyperparameters from the \"All features\" hypertuning experiments, so the wall times for those experiments **do not** include hyperparameter tuning.\n",
    "\n",
    "Since the training and test F2 scores drop when we exclude the weather, seasonality, or lagged delay stats feature families (relative to the \"All features\" model), we can conclude that these features are effective predictors for our modeling objective. Since the training and test F2 scores increase when we exclude the graph, date info, and flight metadata families, these features do not seem to be helping the performance of the model. Therefore in future work, we recommend either excluding these feature families or revising the features' development, in the case of the engineered feature families."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87751713-2eec-437b-9459-2015fcbf9a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a26ea84-9420-4788-932a-630e4d12aead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As a result of our hyperparameter tuning and feature selection, we get the following final results for the selected model for each of our four model architectures. As a fifth architecture, we created an ensemble of the results of the four models. Below, we also present the confusion matrices for each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "717aaec2-d0e2-4058-8d0d-b4a2be2b1b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### F2 Scores by Model\n",
    "\n",
    "| Model | CV Fold F2 Scores |  CV Score Avg  | CV Score SD | Test Set F2 | # Workers | Wall Time |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| Logistic Regression | [0.47, 0.48, 0.52, 0.45, 0.46] | 0.4752 | 0.0270 | 0.4778 | 5-10 | 63 min (no tuning) |\n",
    "| Random Forest | [0.44, 0.49, 0.50, 0.44, 0.47] | 0.4676 | 0.0274 | 0.4676 | 5-10 | 106 min (with tuning) |\n",
    "| XGBoost | [0.49, 0.53, 0.55, 0.50, 0.54] | 0.5220 | 0.0259 | 0.5350 | 5-10 | 20 min (no tuning) |\n",
    "| Multilayer Perceptron | [0.486, 0.521, 0.531, 0.474, 0.521] | 0.5066 | 0.0250 | 0.5254 | 5-10 | 120 min (with tuning) |\n",
    "| Ensemble | N/A | N/A | N/A | 0.5539 | 5-10 | 5 min (no tuning) |\n",
    "\n",
    "Note: The ensemble model was created by combining the predictions of the final trained models from each architecture. Therefore we do not have cross-validation information for the ensemble.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbc9332b-84c7-49b2-a8c1-82e57af5a9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Logistic Regression Confusion Matrix\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/lr_cf.png?raw=true\" alt=\"LR Confusion Matrix\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "052ba3c9-8087-4c51-86de-2c9ec8d8be97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Logistic regression was used as the baseline. Previous exploration on a one-year subset of data (2015) revealed limited improvement in F2 score after incorporating interaction features, so they were not used. \n",
    "\n",
    "\n",
    "| Feature Family | Features Used |\n",
    "|:---:|:---:|\n",
    "| Flight | CRS_ELAPSED_TIME, YEAR, MONTH, DAY_OF_MONTH, DAY_OF_WEEK, OP_UNIQUE_CARRIER, ORIGIN_ICAO, DEST_ICAO,  DISTANCE |\n",
    "| Prior Flight | turnaround_time_calc, priorflight_depdelay_calc, priorflight_isdeparted, priorflight_isarrived, priorflight_isdelayed | 0.4676 | 0.0274 |  | 5-10? | 98 min (with tuning) + FINAL EVAL??? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a3ba20-0c7e-4ce6-9642-382aa7c77471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The logistic regression confusion matrix shows that most of the incorrect predictions are due to false negatives rather than false positives. This is not preferable considering our business objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2002950-f269-4829-938a-4718b77a54c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c812d00e-3173-47d4-b6db-2de893ed9906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Random Forest Confusion Matrix\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/rf_cf.png?raw=true\" alt=\"RF Confusion Matrix\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97151d10-72f4-4324-af7d-390ab4ce2f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The random forest achieves roughly equal proportions of false positives and negatives, and a somewhat improved performance predicting true positives as compared to the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e44df886-81c5-4407-8146-4f584cbc6fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### XGBoost Confusion Matrix\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/xgb_cf.png?raw=true\" alt=XGB Confusion Matrix\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5582b988-e2d2-4620-b52c-92dc1e9ca79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Even though the XGBoost model is the best at identifying true negatives, it struggles with identifying true positive on: it predicts less true positives compared to the other models, and more false negatives. Overall it exhibits similar behavior to the logistic regression, except for its improvements in predicting true negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e19384dc-111e-44dc-ba5a-f0a048d62682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Multilayer Perceptron Confusion Matrix\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/mlp_cf.png?raw=true\" alt=\"MLP Confusion Matrix\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c1295ad-1efb-4a55-9408-a4d75b6ce06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The MLP model performs similarly to the random forest with a slight improvement in predicting both true negatives and true positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "629523f6-72db-4161-b16b-c9c394221753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/objectivehistory.png?raw=true\" alt=\"MLP Confusion Matrix\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7038166-febd-4f53-b641-012c30baa024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The model doesn't appear to be overfitting, as there is a steady decline in the objective function even as iterations continue. This justifies the choice of 200 iterations as the hyperparameter. Increasing iterations further could potentially lead to overfitting. Additionally, some fold models, such as for fold 3, show a more promising decline, while others might start overfitting with more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c733a59-c70c-4d0b-aea8-ee7cc0a147ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ensemble Model Confusion Matrix\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/avg_cf.png?raw=true\" alt=\"Ensemble Confusion Matrix\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe20ed9-1e53-49ba-8382-4665b2150d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Overall, the ensemble model is not as good as the other models in predicting true negatives but performs the best in predicting true positives. It also has the lowest false negative score, and is the only model to have a higher false positive prediction rate. Given this performance, it is the most suitable model for our priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ebba6f1-e3ae-49e8-8e2b-4834edd3756f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comparison to Phase II\n",
    "\n",
    "In Phase II of our project, we achieved an F2 score of 0.5895 on train and 0.5732 on test with our most performant logistic regression model. This model was trained and evaluated on only one year of data, and did not include graph, lagged delay statistic, or yearly seasonality engineered features.\n",
    "\n",
    "Given that our Phase III logistic regression models achieved lower performance despite being trained on more data and richer features in Phase III, we attritube the decrease in Phase III performance versus Phase II performance to the challenges of modeling on long periods of time series data. Despite having more total data to train on, data drift makes it difficult to capture trends in the data with high fidelity over long periods of time. In future work, we would like to explore more sophisticated time series methods in order to try to improve our performance on data of this time scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "626ef8e0-40d9-43e3-be0a-0ea1b834c845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gap Analysis\n",
    "\n",
    "Our most performant model, the ensemble model, achieves a test F2 score of 0.55, indicating that there are areas for improvement. In this section, we will explore what this model is currently missing, and therefore what future modeling efforts should focus on to improve performance.\n",
    "\n",
    "Note that all figures below have been generated using the data from the full 5 year (2015-2019) dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93afe45e-ec56-412a-ab80-03eb4c4bcbf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/positives.png?raw=true\" alt=\"Gap Analysis by Day\" style=\"width: 500px\">\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df0d66f-a47b-4704-b833-0cc554b6fd2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "The ensemble captures the seasonal structure reasonably well but exhibits systematic underprediction of troughs (i.e., it overestimates the number of low-delay days) and fails to fully capture peaks (i.e., it underestimates the number of high-delay days). This indicates a conservative response to seasonal extremes.\n",
    "\n",
    "The time series of false negatives display characteristics resembling white noise, yet with some discernible seasonal patterns. For instance, in June, where there is a known seasonal peak in delays, the model appropriately predicts fewer non-delays (i.e., it shows a slight trough in false negative prediction), aligning with real-world conditions. Conversely, in the fall, during periods with fewer delays, the model predicts more non-delays (i.e., a slight peak in false negatives).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ab35adf-20d9-4df4-bb63-53935d4b46bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/misclass_by_hour.png?raw=true\" alt=\"Gap Analysis by Hour\" style=\"width: 500px\">\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd730397-85a2-401e-8ee8-68700ae6d17f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "False negatives are mostly evenly distributed, only following the minor peak for the morning influx of flights, whereas false positive counts increase as the total count increases and share the 5 PM peak.\n",
    "\n",
    "Both false negative and false positive counts decline as total flight volume declines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76a7987d-52e2-4c3c-82c3-52adab8e4dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, we analyze prediction type by disruption rate, where disruption rate is calculated as an average over all 5 years. In the below scatter plots, each point represents the false negative or false positive rate versus the disruption rate for an individual airport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4211b00a-34cb-4510-9fd8-b014dbfc8724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/fasle_neg_by_disruption_rate.png?raw=true\" alt=\"False Neg by Delay Rate\" style=\"width: 200px\">\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16f5de2c-66a3-48bb-84a0-e1359a6a55ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "As the disruption rate increases from 0.05 to 0.2, the false negative rate also rises, indicating that the model increasingly underpredicts disruptions even at airports with higher disruption rates. On the other hand, as expected, the false negative count is lower for higher-disruption-rate-airports past the 0.2 threshold. This implies that model is figuring out what airports have higher disruption rates, and correctly predicting less non-disruptions. The false negative rate varies widely around the 0.2 threshold, which is expected given that many data points cluster at this delay rate—i.e., many airports exhibit this level of disruption, making it harder for the model to extract a consistent predictive signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76d378bf-bb08-4f09-9f8b-410a89b1eee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/false_pos_by_disruption_rate.png?raw=true\" alt=\"False Pos by Delay Rate\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "499c6a79-1277-4cf1-b9ef-424cb9438d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "False positives increase as disruption rates rise, again suggesting that the model recognizes certain airports as having frequent disruptions, but overcorrects—predicting disruptions at those airports even when none occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c8fa861-de49-43a7-b32a-a6d4880f2efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In addition to exploring our results with respect to flight features, we must also consider the weather. The below figure depicts the Spearman correlation between our numeric weather features and false positive and false negative status. In general, we see only weak correlation betwen false negative status and weather features. This is encouraging--it does not seem we are systematically missing flight delays during particular weather events. There are some moderate correlations between false positive status and temperatures and wind speeds. However, this may indicate events where extreme weather caused widespread disruption, and the model predicted false positives because of a high volume of disruption overall.\n",
    "\n",
    "<div style=\"text-align: center; line-height: 10; padding-top: 30px;  padding-bottom: 30px;\">\n",
    "<img src=\"https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/weather_misclass_corr.png?raw=true\" alt=\"Gap Analysis Weather Corr\" style=\"width: 500px\">\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79438263-ec5d-4256-8271-f14b4b4c6140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Considering the true and false positive and negative rates by airport type, we see similar behavior for medium and large airports, but worse misclassification behavior for small airports. In particular, small airports have a very high false positive rate. Though we are typically more concerned about false negatives, the extent of overprediction of delays in this case is worthy of further analysis in future modeling endeavors.\n",
    "\n",
    "\n",
    "| Origin Airport Type | False Negative Rate | False Positive Rate | True Negative Rate | True Positive Rate |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| Large Airport | 0.067 | 0.299 | 0.488 | 0.146 |\n",
    "| Medium Airport | 0.078 | 0.290 | 0.514 | 0.119 |\n",
    "| Small Airport | 0.022 | 0.629 | 0.094 | 0.255 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53873aba-3052-45c8-9c61-87c4960d2f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Accurate prediction of flight delays enables airports to prepare for operational shocks and prevent delay propagation. Our aim was to design a machine learning classification model to predict flight disruption status, using flight and weather data from 2015-2019. Throughout our project, we introduced engineered features for prior flight status, lagged delay statistics, graph characteristics, and seasonality. Training machine learning models on these features, plus numeric weather and flight metadata features from the raw data, we produced models that achieved F2 scores ranging from 0.46 to 0.55 on a held-out set, with the best performance from an ensembled model of logistic regression, random forest, XGBoost, and multilayer perceptron outputs. With the ensemble model correctly predicting 67% of (currently unanticipated) disrupted flights, we believe our model to be a very promising option to mitigate inefficiencies related delay propagation at airports, and hope that our stakeholders choose to move forward with deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff431c8-d14d-4f8f-8624-97d658a47d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Code Notebooks\n",
    "\n",
    "- Directory and raw data prep: [0.01-mas-dir-and-raw-data-prep](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/4262628234468304?o=4021782157704243#command/7738973093567460)\n",
    "- Weather data cleaning: [0.03-sg-weather-clean](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/905158478261251?o=4021782157704243#command/8643339954781431), [0.03-mas-weather-cleanup](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1017866335725443?o=4021782157704243#command/1017866335725444)\n",
    "- Join pipeline: [0.08-mas-data-join-pipeline](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/905158478261859?o=4021782157704243#command/7738973093566465)\n",
    "- Prior flight feature engineering: [1.10-dy-joined-prior-feat-eng](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1556213624083044?o=4021782157704243#command/7738973093573204)\n",
    "- Joined data cleaning: [0.12-sg-joined-cleaning](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1556213624085050?o=4021782157704243#command/7738973093573832)\n",
    "- Joined data cleaning and feature engineering: [1.14-sg-5y-joined-graph-feateng-clean](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2940542981705990?o=4021782157704243#command/5849022068804427)\n",
    "- MLP Modeling: [3.16-sg-5y-modeling-MLP](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3201478130150963?o=4021782157704243)\n",
    "- XGBoost Modeling: [3.17-sg-5y-XGBOOST](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/4472038552166082?o=4021782157704243#command/4472038552166084)\n",
    "- Hyperparameter Tuning: [3.15-mas-modeling-pipeline-with-tuning](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3336123436792754?o=4021782157704243#command/5381617944351471)\n",
    "- Models Ensemble and analysis: [3.18-sg-ensemble](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3198055233260107?o=4021782157704243#command/3198055233260108)\n",
    "- Modeling postprocessing: [3.13-modeling-analysis](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3581936500213637?o=4021782157704243#command/6823299216258268)\n",
    "- Time-based feature engineering: [1.13-eil-joined-time-based-feat-eng](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3336123436788973?o=4021782157704243#command/3336123436788995)\n",
    "- Initial 5 year modeling exploration and creation of 5 year seasonality data: [3.14-eil-5y-initial-modeling](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3336123436791044?o=4021782157704243#command/6352054888934311)\n",
    "- Figure generation for report and slides: [2.15-eil-figures](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3336123436791855?o=4021782157704243#command/5381617944350124)\n",
    "- Random Forest hyperparameter tuning [3.19-eil-modeling-pipeline-with-tuning](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/4472038552165641?o=4021782157704243#command/6352054888932701)\n",
    "- Feature importance experiments [3.20-eil-feature-importance-experiments](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3198055233260353?o=4021782157704243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05787567-9a0d-4294-adb6-3f5d6abb3ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Presentation\n",
    "\n",
    "- [Slide Deck](https://github.com/bakr-UCB/261-Final-Project/blob/main/reports/figures/Phase_III_Final_Presentation.pdf) presented on April 16, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a22fae-bac1-4d34-b639-e59d8a30bb90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bibliography\n",
    "\n",
    "<ol>\n",
    "    <li>\"Federal Climate Complex Data Documentation for Integrated Surface Data (ISD).\" NOAA NCEI, 12 Jan. 2018, https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf. Accessed 16 Mar. 2025.</li>\n",
    "    <li>Lee, Kangoh. “Airline operational disruptions and loss-reduction investment.” Transportation Research Part B: Methodological, vol. 177, Nov. 2023, p. 102817, https://doi.org/10.1016/j.trb.2023.102817. </li>\n",
    "    <li>“Local Climatological Data (LCD) Dataset Documentation.” Local Climatological Data (LCD) Data, NOAA NCEI, www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf. Accessed 16 Mar. 2025.</li>\n",
    "    <li>\"Reporting Carrier On-Time Performance (1987-present).\" Bureau of Transportation Statistics, https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ. Accessed 16 Mar. 2025.</li>\n",
    "    <li>Taylor, Sean and Letham Benjamin. \"Forecasting at scale.\" PeerJ Preprints 5:e3190v2, 2017. https://doi.org/10.7287/peerj.preprints.3190v2</li>\n",
    "    <li>“Understanding the Reporting of Causes of Flight Delays and Cancellations.” Bureau of Transportation Statistics, US Department of Transportation, 15 Apr. 2024, www.bts.gov/topics/airlines-and-airports/understanding-reporting-causes-flight-delays-and-cancellations. </li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PHASE 3 Project Report",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
