{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa988ec4-230e-49bc-868f-55235d230db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modeling Pipeline (Experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165fe1c6-20c0-4156-8fcd-78eff30c68e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "134d7e1c-a32a-4883-bdfb-ca3a7927701a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5e9bdd-2536-4ffb-b641-9c87ff506349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any, Union,Callable\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    RandomForestClassifier,\n",
    "    MultilayerPerceptronClassifier\n",
    ")\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "import mlflow\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40542d5e-46eb-4213-9a16-989eb8ee96a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6df1ab-7364-416b-8a35-70efe4d364ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variables and directories\n",
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/datasets_final_project_2022\"\n",
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/checkpoints\")\n",
    "period = \"_1y\" # one of the following values (\"\", \"_3m\", \"_6m\", \"_1y\")\n",
    "k = 5 # cv folds\n",
    "overlap = 0.2 # cv overlap\n",
    "\n",
    "# Datasets\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined{period}_cleaned_engineered_timefeat.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320be5ab-c3e4-4b6e-b7ed-fc1c2c34f4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory Inspection\n",
    "display(dbutils.fs.ls(f\"{team_BASE_DIR}/interim/join_checkpoints/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bd689be-1e15-4a47-aafd-210212a9ef0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP : Features Selection and Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3ea8b7-6ebb-41e8-974d-c9b09d34d8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"ORIGIN\",\n",
    "    \"DEST\",\n",
    "    \"QUARTER\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\"\n",
    "]\n",
    "\n",
    "label = \"outcome\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62304ab2-aed9-4e91-9287-4d5e2920e612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP : Time-series CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00561fab-518a-493a-a18e-bf2ee77644db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_timeseries(df, time_col: str, test_fraction: float = 0.2, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Splits a PySpark DataFrame into a train/test set based on a timestamp column.\n",
    "    The most recent `test_fraction` of the data (by time) is used as test set.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        time_col (str): Timestamp column name (must be sortable).\n",
    "        test_fraction (float): Fraction of time span to allocate to the test set.\n",
    "        verbose (bool): Print boundaries and sizes.\n",
    "\n",
    "    Returns:\n",
    "        (train_df, test_df): Tuple of train and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Get min and max time\n",
    "    min_time, max_time = df.selectExpr(f\"min({time_col})\", f\"max({time_col})\").first()\n",
    "    total_days = (max_time - min_time).days\n",
    "    test_days = int(total_days * test_fraction)\n",
    "\n",
    "    test_start = max_time - timedelta(days=test_days)\n",
    "\n",
    "    train_df = df.filter(F.col(time_col) < test_start)\n",
    "    test_df = df.filter(F.col(time_col) >= test_start)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"ðŸ“… Total date range: {min_time.date()} â†’ {max_time.date()} ({total_days} days)\")\n",
    "        print(f\"âœ… Train: {min_time.date()} â†’ {test_start.date()} ({train_df.count():,} rows)\")\n",
    "        print(f\"ðŸ§ª Test: {test_start.date()} â†’ {max_time.date()} ({test_df.count():,} rows)\")\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f023b00f-7de8-4f31-afd2-7ccf8a70098a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_series_cv_folds(\n",
    "    df,\n",
    "    time_col: str,\n",
    "    k: int=3,\n",
    "    blocking: bool=False,\n",
    "    overlap: float=0.0,\n",
    "    verbose: bool=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Split a time-series PySpark DataFrame into k train/test folds with optional overlap and blocking.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): PySpark DataFrame with a timestamp column.\n",
    "        dep_utc_time_colvarname (str): Name of the timestamp column.\n",
    "        k (int): Number of folds.\n",
    "        blocking (bool): Whether to block the training set to avoid cumulative data.\n",
    "        overlap (float): Fraction of overlap between validation windows (e.g. 0.2 = 20% overlap).\n",
    "        verbose (bool): Whether to print the time splits.\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_df, val_df) tuples.\n",
    "    \"\"\"\n",
    "    # Get time boundaries\n",
    "    min_date = df.select(F.min(time_col)).first()[0]\n",
    "    max_date = df.select(F.max(time_col)).first()[0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "\n",
    "    # Adjust chunk sizing\n",
    "    total_width = k + 1 - overlap * (k - 1)\n",
    "    chunk_size = int(np.ceil(n_days / total_width))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Splitting data into {k} folds with {overlap*100:.0f}% overlap\")\n",
    "        print(f\"Min date: {min_date}, Max date: {max_date}\")\n",
    "        print(f\"{chunk_size:,} days per fold\")\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        # Offset calculation with overlap\n",
    "        train_start_offset = 0 if not blocking else int(i * (1 - overlap) * chunk_size)\n",
    "        train_end_offset = int((i + 1) * chunk_size)\n",
    "        val_start_offset = train_end_offset\n",
    "        val_end_offset = int(val_start_offset + chunk_size)\n",
    "\n",
    "        # Compute actual timestamps\n",
    "        train_start = min_date + timedelta(days=train_start_offset)\n",
    "        train_end = min_date + timedelta(days=train_end_offset)\n",
    "        val_start = min_date + timedelta(days=val_start_offset)\n",
    "        val_end = min_date + timedelta(days=val_end_offset)\n",
    "\n",
    "        if val_start >= max_date:\n",
    "            break\n",
    "        if val_end > max_date:\n",
    "            val_end = max_date + timedelta(days=1)\n",
    "\n",
    "        # Apply filters\n",
    "        train_df = df.filter((F.col(time_col) >= train_start) & (F.col(time_col) < train_end))\n",
    "        val_df = df.filter((F.col(time_col) >= val_start) & (F.col(time_col) < val_end))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold {i + 1}:\")\n",
    "            print(f\"  TRAIN: {train_start.date()} â†’ {train_end.date()} ({train_df.count():,} rows)\")\n",
    "            print(f\"  VAL:   {val_start.date()} â†’ {val_end.date()} ({val_df.count():,} rows)\")\n",
    "            print(\"------------------------------------------------------------\")\n",
    "\n",
    "        folds.append((train_df, val_df))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d832d46-24b1-4dbd-9f1a-8292d06f3647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_class_weights(df, label_col: str):\n",
    "    label_counts = df.groupBy(label_col).count().toPandas()\n",
    "    neg, pos = label_counts.sort_values(label_col)[\"count\"].tolist()\n",
    "    pos_weight = float(neg) / pos\n",
    "\n",
    "    df_weighted = df.withColumn(\"weight\", F.when(F.col(label_col) == 1, pos_weight).otherwise(1.0))\n",
    "    return df_weighted, pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd84a9f-b5bf-4675-afbe-815c28481c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cv_eval(predictions: DataFrame, label_col=\"outcome\", prediction_col=\"prediction\", metric:str=\"F2\"):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = predictions.select(['prediction', label_col]).rdd\n",
    "  rdd_preds_b = predictions.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  if metric == \"F2\":\n",
    "    score = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  elif metric == \"pr\":\n",
    "    score = metrics_b.areaUnderPR\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41d7990-598b-4ea5-aff1-09b4c4677a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_tuner(\n",
    "    model_name: str,\n",
    "    model_params: Dict[str, Any],\n",
    "    stages,\n",
    "    folds: List[Tuple[DataFrame, DataFrame]],\n",
    "    mlflow_run_name: str = \"/Users/m.bakr@berkeley.edu/flight_delay_tuning\",\n",
    "    metric: str = \"F2\",\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Union[float, str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Universal tuning function for PySpark classification models using time-series cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of ['logreg', 'rf', 'mlp', 'xgb']\n",
    "        model_params (Dict[str, Any]): Parameters to apply to the model\n",
    "        folds (List of (train_df, val_df)): Time-aware CV folds\n",
    "        mlflow_run_name (str): Optional MLflow parent run name\n",
    "        verbose (bool): Whether to log outputs during tuning\n",
    "\n",
    "    Returns:\n",
    "        Dict with best average F2 or pr score, model name, and parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Model factory\n",
    "    model_factory = {\n",
    "        \"logreg\": LogisticRegression,\n",
    "        \"rf\": RandomForestClassifier,\n",
    "        \"mlp\": MultilayerPerceptronClassifier,\n",
    "        \"xgb\": SparkXGBClassifier\n",
    "    }\n",
    "\n",
    "    assert model_name in model_factory, f\"Unsupported model: {model_name}\"\n",
    "\n",
    "    ModelClass = model_factory[model_name]\n",
    "\n",
    "    # Apply required fields\n",
    "    model = ModelClass(\n",
    "        featuresCol=features,\n",
    "        labelCol=label,\n",
    "        weightCol=\"weight\",  # Handles imbalance\n",
    "        **model_params\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[model] + stages) \n",
    "\n",
    "    scores = []\n",
    "\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "        for i, (train_df, val_df) in enumerate(folds):\n",
    "            fitted_model = pipeline.fit(train_df)\n",
    "            preds = fitted_model.transform(val_df)\n",
    "            score = cv_eval(preds, metric)\n",
    "            scores.append(score)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"[Fold {i+1}] {metric} Score: {score:.4f}\")\n",
    "\n",
    "            mlflow.log_metric(f\"{metric}_fold_{i+1}\", score)\n",
    "\n",
    "        avg_score = float(np.mean(scores))\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_params(model_params)\n",
    "        mlflow.log_metric(\"avg_{metric}_score\", avg_score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"âœ… Average {metric} Score: {avg_score:.4f} | Model: {model_name}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"params\": model_params,\n",
    "        \"avg_f2_score\": avg_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85f2801c-bf0c-44a0-9bed-e2ca4eb7940b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_hyperopt_objective(\n",
    "    model_name: str,\n",
    "    folds: List[Tuple[DataFrame, DataFrame]],\n",
    "    stages: List,\n",
    "    param_space_converter: Callable[[Dict[str, Any]], Dict[str, Any]],\n",
    "    mlflow_experiment_name: str = \"Hyperopt_Universal_Tuning\",\n",
    "    verbose: bool = True\n",
    ") -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Creates a Hyperopt-compatible objective function for any PySpark classifier.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of 'logreg', 'rf', 'mlp', 'xgb'.\n",
    "        folds (List of (train_df, val_df)): Time-series CV folds.\n",
    "        param_space_converter (Callable): Converts Hyperopt sample into model params.\n",
    "        mlflow_experiment_name (str): MLflow experiment name.\n",
    "        verbose (bool): Logging toggle.\n",
    "\n",
    "    Returns:\n",
    "        Callable that can be passed as fn to hyperopt.fmin()\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(sampled_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Convert sampled param space to Spark-friendly params\n",
    "        model_params = param_space_converter(sampled_params)\n",
    "\n",
    "        result = model_tuner(\n",
    "            model_name=model_name,\n",
    "            model_params=model_params,\n",
    "            stages=stages,\n",
    "            folds=folds,\n",
    "            mlflow_run_name=f\"hyperopt_{model_name}\",\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": -result[\"avg_f2_score\"],  # Minimize negative F2\n",
    "            \"status\": STATUS_OK,\n",
    "            \"params\": result[\"params\"]\n",
    "        }\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42387b9d-111d-43a4-b6da-de46968fc2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c8eb8bc-9cf8-44fc-8e20-8abd5d2400e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Testing Time Series CV function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49d2e95-4584-4a67-982d-7a0cc4ce32e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split_timeseries(\n",
    "    df=df,\n",
    "    time_col=\"sched_depart_utc\",\n",
    "    test_fraction=0.2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f9de89-edcc-4539-bb4d-eb5b91167e27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, pos_weight = add_class_weights(train_df, label_col=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda14a23-ed8f-4524-9a6e-39b4159fd7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Testing Time Series CV function\n",
    "folds = time_series_cv_folds(\n",
    "    train_df,\n",
    "    time_col=\"sched_depart_utc\",\n",
    "    k=k,\n",
    "    overlap=overlap,\n",
    "    blocking=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f33ca572-5259-40f9-8b86-f0fe9b896938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48385f3b-f583-4f25-9f16-cbb6a7da1b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Random Forest Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34627366-ee45-477f-a60e-98eca004b637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define random search param grid\n",
    "param_grid = []\n",
    "for _ in range(10):  # 10 random configs\n",
    "    param_grid.append({\n",
    "        \"numTrees\": random.choice([50, 100, 200]),\n",
    "        \"maxDepth\": random.choice([5, 10, 15]),\n",
    "        \"featureSubsetStrategy\": random.choice([\"auto\", \"sqrt\", \"log2\"])\n",
    "    })\n",
    "\n",
    "# Define other stages\n",
    "stages= []\n",
    "\n",
    "# Run custom tuner\n",
    "best_model, best_params, best_score = model_tuner(\n",
    "    model_class=RandomForestClassifier,\n",
    "    param_grid_list=param_grid,\n",
    "    folds=folds,\n",
    "    experiment_name=\"/Users/m.bakr@berkeley.edu/flight_delay_tuning\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Best F2 Score:\", best_score)\n",
    "print(\"Best Params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afb52e8-e579-420e-87b8-90008db2ebeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Hyperopt search space\n",
    "rf_space = {\n",
    "    \"numTrees\": hp.choice(\"numTrees\", [50, 100, 200]),\n",
    "    \"maxDepth\": hp.quniform(\"maxDepth\", 5, 15, 1),\n",
    "    \"featureSubsetStrategy\": hp.choice(\"featureSubsetStrategy\", [\"auto\", \"sqrt\", \"log2\"])\n",
    "}\n",
    "\n",
    "def rf_param_mapper(sampled: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"numTrees\": int(sampled[\"numTrees\"]),\n",
    "        \"maxDepth\": int(sampled[\"maxDepth\"]),\n",
    "        \"featureSubsetStrategy\": sampled[\"featureSubsetStrategy\"]\n",
    "    }\n",
    "\n",
    "objective = make_hyperopt_objective(\n",
    "    model_name=\"rf\",\n",
    "    folds=folds,\n",
    "    stages=stages,\n",
    "    param_space_converter=rf_param_mapper,\n",
    "    mlflow_experiment_name=\"RF_Hyperopt_Flight_Delay\"\n",
    ")\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=rf_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best Hyperopt Config:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45077b5f-4cab-49f7-95c1-cdd523abe432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fada31e0-5f7c-471b-ae24-4f1ecc5ec4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logreg_space = {\n",
    "    \"regParam\": hp.uniform(\"regParam\", 0.0, 0.5),\n",
    "    \"elasticNetParam\": hp.uniform(\"elasticNetParam\", 0.0, 1.0)\n",
    "}\n",
    "\n",
    "def logreg_param_mapper(sampled):\n",
    "    return {\n",
    "        \"regParam\": sampled[\"regParam\"],\n",
    "        \"elasticNetParam\": sampled[\"elasticNetParam\"],\n",
    "        \"maxIter\": 100\n",
    "    }\n",
    "\n",
    "logreg_obj = make_hyperopt_objective(\n",
    "    model_name=\"logreg\",\n",
    "    folds=folds,\n",
    "    param_space_converter=logreg_param_mapper,\n",
    "    mlflow_experiment_name=\"LogReg_Hyperopt\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_logreg = fmin(\n",
    "    fn=logreg_obj,\n",
    "    space=logreg_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Best Logistic Regression params:\", best_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69e3bfe3-0e29-49da-bdb2-912292cd9f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8cbb7cb-fc78-495b-abf8-75bd3fae71eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_space = {\n",
    "    \"eta\": hp.uniform(\"eta\", 0.01, 0.3),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "    \"num_round\": hp.quniform(\"num_round\", 50, 200, 10)\n",
    "}\n",
    "\n",
    "def xgb_param_mapper(sampled):\n",
    "    return {\n",
    "        \"eta\": sampled[\"eta\"],\n",
    "        \"max_depth\": int(sampled[\"max_depth\"]),\n",
    "        \"subsample\": sampled[\"subsample\"],\n",
    "        \"colsample_bytree\": sampled[\"colsample_bytree\"],\n",
    "        \"num_round\": int(sampled[\"num_round\"]),\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"num_workers\": 2,\n",
    "        \"verbosity\": 0\n",
    "    }\n",
    "xgb_obj = make_hyperopt_objective(\n",
    "    model_name=\"xgb\",\n",
    "    folds=folds,\n",
    "    param_space_converter=xgb_param_mapper,\n",
    "    mlflow_experiment_name=\"XGBoost_Hyperopt\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=xgb_obj,\n",
    "    space=xgb_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Best XGBoost params:\", best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e730c8e-f4fe-4881-9098-282c1aeffd2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803abca7-ec4b-4ad3-abff-fd523b2a9025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_space = {\n",
    "    \"hidden_layers\": hp.choice(\"hidden_layers\", [[64, 32], [128, 64], [100, 50]]),\n",
    "    \"stepSize\": hp.uniform(\"stepSize\", 0.01, 0.3),\n",
    "    \"maxIter\": hp.choice(\"maxIter\", [100, 200]),\n",
    "    \"blockSize\": hp.choice(\"blockSize\", [64, 128])\n",
    "}\n",
    "\n",
    "def mlp_param_mapper(sampled):\n",
    "    return {\n",
    "        \"layers\": [input_dim] + sampled[\"hidden_layers\"] + [2],\n",
    "        \"stepSize\": sampled[\"stepSize\"],\n",
    "        \"maxIter\": sampled[\"maxIter\"],\n",
    "        \"blockSize\": sampled[\"blockSize\"]\n",
    "    }\n",
    "\n",
    "mlp_obj = make_hyperopt_objective(\n",
    "    model_name=\"mlp\",\n",
    "    folds=folds,\n",
    "    param_space_converter=mlp_param_mapper,\n",
    "    mlflow_experiment_name=\"MLP_Hyperopt\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_mlp = fmin(\n",
    "    fn=mlp_obj,\n",
    "    space=mlp_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Best MLP params:\", best_mlp)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.15-mas-modeling-pipeline-with-tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
