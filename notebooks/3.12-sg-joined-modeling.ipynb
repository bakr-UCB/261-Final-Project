{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "027f948b-d6e7-498b-b53c-53cd6cda381a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e1da0c-a773-4e7f-948d-3f4e3fd880a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined_1y_cleaned_engineered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97011666-cbc6-4d4e-9ec2-5663ac03b865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = df.fillna({'turnaround_time_calc': 0}) #don't have enough information to calculate the turnaround time, so just assume something is going on that wont give us enough time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a14baa14-4537-47c9-a892-dc843a037fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec9db66-5c5a-4f72-804a-812960c33b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29625370-5bb1-4954-94ba-dfa34953e54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"1y\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds\n",
    "k = 5\n",
    "overlap = 0.2\n",
    "\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup) #using false since erica already computed\n",
    "compute_seasonality = False\n",
    "\n",
    "# define train/test split date\n",
    "min_test_dt = \"2019-10-01\"\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9e8105e-7c3e-4a44-a169-e9699ab8de75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710651f6-e7fe-4bc7-b877-213aa6794ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_handled = df.withColumns(\n",
    "    {\n",
    "        \"dep_hour_utc\": \n",
    "            F.hour(col(dep_utc_varname)),\n",
    "        \"outcome\":  \n",
    "            (F.when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\")\n",
    "            }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6264a587-a293-4a3a-ba82-592c184c0da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "\n",
    "\n",
    "df_train = df_handled.filter(F.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "df_test = df_handled.filter(F.col(dep_utc_varname) >= min_test_dt)\n",
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a201ff7-9a78-4197-a6a5-7528dee4964c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d1e079e4-266b-4e77-9ba8-154a1f0addfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits(df, k=5, blocking=False, dep_utc_varname=\"dep_datetime\", verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation\n",
    "    '''\n",
    "    n = df.count()\n",
    "    df = df.withColumn(\"row_id\", f.row_number()\n",
    "            .over(Window.partitionBy().orderBy(dep_utc_varname)))\n",
    "    chunk_size = np.floor(n/(k+1))\n",
    "\n",
    "    idx = np.arange(0,)\n",
    "    idx = np.arange(0,n,chunk_size)\n",
    "    idx[-1] = n-1\n",
    "    idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print('')\n",
    "        print(f'Number of validation datapoints for each fold is {chunk_size:,}')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    bin_edges = df.filter(f.col(\"row_id\").isin(idx)).select(\"row_id\",dep_utc_varname).toPandas()\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = bin_edges[dep_utc_varname][0]\n",
    "        else:\n",
    "            t_min_train = bin_edges[dep_utc_varname][i]\n",
    "        # define maximum training time\n",
    "        t_max_train = bin_edges[dep_utc_varname][i+1]\n",
    "        # define minimum test time\n",
    "        t_min_test = bin_edges[dep_utc_varname][i+1]\n",
    "        # define maximum test_time\n",
    "        t_max_test = bin_edges[dep_utc_varname][i+2]\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "014d3cba-9d0d-461d-90cd-9e5cf4f44518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits_with_overlap(df, k=3, blocking=False, overlap=0, dep_utc_varname=\"dep_datetime\", verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation\n",
    "    '''\n",
    "    n = df.count()\n",
    "    df = df.withColumn(\"row_id\", f.row_number()\n",
    "            .over(Window.partitionBy().orderBy(dep_utc_varname)))\n",
    "    total_width = k+1 - overlap*(k-1)\n",
    "    chunk_size = np.floor(n/total_width)\n",
    "\n",
    "    idx = np.arange(0,)\n",
    "    idx = np.arange(0,n,chunk_size)\n",
    "    idx[-1] = n-1\n",
    "    idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print('')\n",
    "        print(f'Number of validation datapoints for each fold is {chunk_size:,}')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    df.cache()\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define indices\n",
    "        if i == 0:\n",
    "            train_min_idx = 1\n",
    "            train_max_idx = chunk_size\n",
    "        else:\n",
    "            train_min_idx += np.floor((1-overlap)*chunk_size)\n",
    "            train_max_idx += np.floor((1-overlap)*chunk_size)\n",
    "        test_min_idx = train_min_idx + chunk_size\n",
    "        test_max_idx = train_max_idx + chunk_size\n",
    "\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = df.filter(f.col(\"row_id\") == 1).select(dep_utc_varname).toPandas().values[0][0]\n",
    "        else:\n",
    "            t_min_train = df.filter(f.col(\"row_id\") == train_min_idx).select(dep_utc_varname).toPandas().values[0][0]\n",
    "        # define maximum training time\n",
    "        t_max_train = df.filter(f.col(\"row_id\") == train_max_idx).select(dep_utc_varname).toPandas().values[0][0]\n",
    "        # define minimum test time\n",
    "        t_min_test = df.filter(f.col(\"row_id\") == test_min_idx).select(dep_utc_varname).toPandas().values[0][0]\n",
    "        # define maximum test_time\n",
    "        t_max_test = df.filter(f.col(\"row_id\") == test_max_idx).select(dep_utc_varname).toPandas().values[0][0]\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949c3d01-5832-4089-a745-0158dd516c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get cross-validation split times\n",
    "# cv_cutoffs = get_cv_time_limits(df_train.select(dep_utc_varname), k=k, blocking=True, \n",
    "#     dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "cv_cutoffs = get_cv_time_limits_with_overlap(df_train.select(dep_utc_varname), k=k, blocking=True, overlap=overlap,\n",
    "    dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get cross-validation split times\n",
    "cv_cutoffs = get_cv_time_limits(df_train.select(dep_utc_varname), k=3, blocking=True, \n",
    "    dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37265166-1734-48d5-b8cd-9412ac618185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality_data(df, fold, k):\n",
    "  \"\"\"\n",
    "  Look up seasonlaity features from saved seasonality model.\n",
    "  \"\"\"\n",
    "  if fold == 'full':\n",
    "      fn_model = f\"seasonality_model_{period}_train.parquet\"\n",
    "  else:\n",
    "      fn_model = f\"seasonality_model_{period}_cv{fold}of{k}_overlap{overlap}.parquet\"\n",
    "  model = spark.read.parquet(f\"{team_BASE_DIR}/interim/{fn_model}\")\n",
    "\n",
    "  joined_df = df.join(model, \n",
    "                    (df[\"ORIGIN\"] == model[\"ORIGIN\"]) & \n",
    "                    (df[\"DAY_OF_WEEK\"] == model[\"dow\"]) & \n",
    "                    (df[\"dep_hour_utc\"] == model[\"hour\"]),\n",
    "                    how=\"left\").drop(model[\"ORIGIN\"])\n",
    "  \n",
    "  return joined_df\n",
    "\n",
    "\n",
    "# CODE BELOW DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def upsample(train_df,verbose=False):\n",
    "  '''Upsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = non_delay_count / delay_count\n",
    "\n",
    "  train_delay = train_df.filter(f.col('outcome') == 0)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 1).sample(withReplacement=True, fraction=keep_percent,seed=42)\n",
    "  train_upsampled = train_delay.union(train_non_delay)\n",
    "  return train_upsampled\n",
    "\n",
    "\n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n",
    "\n",
    "def cv_eval(preds):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  return F2, pr\n",
    "\n",
    "def timeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform time series split k-fold cross validation \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    # elif sampling=='weights':\n",
    "    #   train_df = add_class_weights(train_df).cache()\n",
    "      \n",
    "    #print info on train and dev set for this fold\n",
    "    if verbose:\n",
    "      print('    TRAIN set for fold {} goes from {} to {}, count is {:,} flights ({})'.format((i+1), \n",
    "                                                                                      train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      train_df.count(),\n",
    "                                                                                      sampling + '-sampled' if sampling else 'no sampling'))\n",
    "      print('    DEV set for fold {} goes from {} to {}, count is {:,} flights'.format((i+1), \n",
    "                                                                                      dev_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      dev_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      dev_df.count()))\n",
    "      \n",
    "    # TODO: remove once feat engineering applied outside\n",
    "    train_df = get_seasonality_data(train_df, i, k)\n",
    "    train_df = train_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "    dev_df = get_seasonality_data(dev_df, i, k)\n",
    "    dev_df = dev_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "\n",
    "    # print(train_df.dtypes)\n",
    "    # print(dev_df.dtypes)\n",
    "        \n",
    "    # Fit params on the model\n",
    "    model = pipeline.fit(train_df)\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "  # # Train on full df\n",
    "  # print('Training on full train dataset, and validating on dev dataset with best parameters from CV:')\n",
    "  # print(best_parameters)\n",
    "    \n",
    "  # if verbose:\n",
    "  #   print('    TRAIN set for best parameter fitted model goes from {} to {}, count is {:,} flights ({})'.format(train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.count(),\n",
    "  #                                                                                                    sampling + '-sampled' if sampling else 'no sampling'))\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8560b26e-c783-451c-abd0-393b8dcc280a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6629c6-9d35-44cf-b90e-3e584baa1f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/modeling_checkpoints\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "878299aa-e16e-4d84-b6f3-5ec45de112da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d301af5-120c-4ba6-ad2d-658c395a4db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\"]\n",
    "\n",
    "# date related columns\n",
    "date_cols = [\"YEAR\",\"QUARTER\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "\n",
    "# prior & current flight cols\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\", \"origin_type\", \"dest_type\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",\"DAY_OF_WEEK\",\"ORIGIN\",\"dep_hour_utc\",dep_utc_varname]\n",
    "\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols]\n",
    "categorical_cols = [*flight_metadata_cols, *date_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f0e2c82e-d2ea-41fb-bf16-c3c4399ed795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "ncols = [*num_weather_cols, *num_flight_cols]\n",
    "# Count NaNs in numeric columns\n",
    "nan_counts_numeric = df_handled.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in ncols])\n",
    "\n",
    "# Count NaNs in categorical columns\n",
    "nan_counts_categorical = df_handled.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in categorical_cols])\n",
    "\n",
    "display(nan_counts_numeric)\n",
    "display(nan_counts_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cf6ec79a-dd9b-40ae-aeb9-6b90b10540ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # List to hold the stages of the pipeline\n",
    "# stages = []\n",
    "\n",
    "# # Index and encode each categorical column\n",
    "# for column in categorical_cols:\n",
    "#     indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\",handleInvalid=\"keep\")\n",
    "#     encoder = OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_vec\",handleInvalid=\"keep\")\n",
    "#     stages += [indexer, encoder]\n",
    "# # define encoded categorical feature names\n",
    "# categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "# # assemble features\n",
    "# features = numeric_cols + categorical_vec_columns\n",
    "# assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid='keep')\n",
    "\n",
    "# # scale features\n",
    "# scaler = MinMaxScaler(inputCol=\"features\", \\\n",
    "#     outputCol=\"features_scaled\")\n",
    "\n",
    "# # logistic regression model\n",
    "# lr = LogisticRegression(featuresCol='features_scaled', \\\n",
    "#     labelCol='outcome',maxIter=50)\n",
    "\n",
    "# # construct pipeline object from all components\n",
    "# pipeline = Pipeline(stages=stages+[assembler,scaler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36975152-b9f1-49da-a890-06cfcaa2465c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# 1. Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# 4. Update feature list to include imputed columns\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "features = numeric_cols + categorical_vec_columns\n",
    "\n",
    "# 5. Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 6. Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "# 7. Logistic regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features_scaled\", \n",
    "    labelCol=\"outcome\", \n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "# Build final pipeline\n",
    "pipeline = Pipeline(stages=stages + [assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16ac16f7-f6dd-459d-a429-0ce77e43f195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "336a51c5-8688-4d94-a3f3-c54184a3a523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "091a47fb-d4fc-4454-af59-3e127dbc481f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e82b3fbd-1fd7-4b6d-8f41-b0eb2defe5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation actual baseline\n",
    "\n",
    "df_train_downsampled = downsample(df_train).cache()\n",
    "df_train_seasonal = get_seasonality_data(df_train_downsampled, 'full', k).cache()\n",
    "df_test_seasonal = get_seasonality_data(df_test, 'full', k).cache()\n",
    "model = pipeline.fit(df_train_seasonal)\n",
    "dev_pred = model.transform(df_test_seasonal)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04aa3473-c017-4e81-93df-b2f8426924d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed3fa00-59a4-4ee2-95b6-b54d9f6bbfec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3dee1d-1512-4604-93bf-fab08b06fce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation actual baseline\n",
    "\n",
    "df_train_downsampled = downsample(df_train).cache()\n",
    "df_train_seasonal = get_seasonality_data(df_train_downsampled, 'full', k).cache()\n",
    "df_test_seasonal = get_seasonality_data(df_test, 'full', k).cache()\n",
    "model = pipeline.fit(df_train_seasonal)\n",
    "dev_pred = model.transform(df_test_seasonal)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8504a54c-3955-47fe-a9b2-0192e9a30960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609ecd30-e836-49d4-addb-95ea0539cd16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\"]\n",
    "\n",
    "# date related columns\n",
    "date_cols = [\"YEAR\",\"QUARTER\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "\n",
    "# prior & current flight cols\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_elapsed_time_calc_raw']\n",
    "\n",
    "bool_flight_cols = ['priorflight_isdeparted', \n",
    "                    'priorflight_isarrived_calc',\n",
    "                    'priorflight_isdelayed_calc']\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\", \"origin_type\", \"dest_type\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",\"DAY_OF_WEEK\",\"ORIGIN\",\"dep_hour_utc\",dep_utc_varname]\n",
    "\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *num_flight_cols]\n",
    "categorical_cols = [*flight_metadata_cols, *date_cols, *bool_flight_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d648e35-240b-4ebc-992d-59b8f861a00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ncols = [*num_weather_cols, *num_flight_cols]\n",
    "null_counts_numeric = df_train.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in ncols])\n",
    "null_counts_categorical = df_train.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in categorical_cols])\n",
    "\n",
    "display(null_counts_numeric)\n",
    "display(null_counts_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1faadf66-08fc-4497-8e00-9d28511f89bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# 1. Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# 4. Update feature list to include imputed columns\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "features = numeric_cols + categorical_vec_columns\n",
    "\n",
    "# 5. Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# 6. Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "# 7. Logistic regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features_scaled\", \n",
    "    labelCol=\"outcome\", \n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "# Build final pipeline\n",
    "pipeline = Pipeline(stages=stages + [assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b67113c2-df70-49e4-840f-7f7460dcc51d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d9f5c2-9758-438a-9ae0-b4b5f453be4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "import pyspark.sql.functions as f \n",
    "df_train_downsampled = downsample(df_train)\n",
    "df_train_seasonal = get_seasonality_data(df_train_downsampled, 'full', k)\n",
    "df_train_seasonal.cache()\n",
    "df_test_seasonal = get_seasonality_data(df_test, 'full', k)\n",
    "df_test_seasonal.cache()\n",
    "model = pipeline.fit(df_train_seasonal)\n",
    "dev_pred = model.transform(df_test_seasonal)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3caa1bf-abb9-4c7d-9c6c-faf90b134b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd7446f-7ba1-4ed1-af9c-754fae385214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90fcca1-f64b-453b-a5d4-311b6fcb0a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833f0d4e-1532-482a-88cb-3e702ce861f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "df_train_downsampled = downsample(df_train).cache()\n",
    "df_train_seasonal = get_seasonality_data(df_train_downsampled, 'full', k).cache()\n",
    "df_test_seasonal = get_seasonality_data(df_test, 'full', k).cache()\n",
    "model = pipeline.fit(df_train_seasonal)\n",
    "dev_pred = model.transform(df_test_seasonal)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c1d01ad-61ba-4fd5-be48-034b800829c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f7d0c2-59ba-4fd1-9fea-2dd8354b92db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Interaction\n",
    "\n",
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "\n",
    "\n",
    "# Interaction feature engineering\n",
    "interaction1 = Interaction(\n",
    "    inputCols=[\"origin_HourlyPrecipitation\", \"origin_HourlyWetBulbTemperature\"],\n",
    "    outputCol=\"PrecipTemp\"\n",
    ")\n",
    "\n",
    "interaction2 = Interaction(\n",
    "    inputCols=[\"origin_type_vec\", \"turnaround_time_calc\"],\n",
    "    outputCol=\"airport_turnaround\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "features = numeric_cols + categorical_vec_columns + [\"PrecipTemp\", \"airport_turnaround\"]\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features_scaled\", \n",
    "    labelCol=\"outcome\", \n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "# Build final pipeline\n",
    "pipeline = Pipeline(stages=stages + [interaction1, interaction2, assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e55c45a-67f1-4422-b857-dc6e763d1e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d20709-0c26-4392-aa39-d3e9c2a96535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "\n",
    "\n",
    "\n",
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbeb744-3793-41de-89e7-e17efa1b7c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "# split into train and test\n",
    "\n",
    "df_train = df_handled.filter(F.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "df_test = df_handled.filter(F.col(dep_utc_varname) >= min_test_dt)\n",
    "df_test.cache()\n",
    "\n",
    "df_train_downsampled = downsample(df_train).cache() #downsample\n",
    "\n",
    "df_train_seasonal= get_seasonality_data(df_train_downsampled, 'full', k).cache()\n",
    "df_test_seasonal = get_seasonality_data(df_test, 'full', k).cache()\n",
    "\n",
    "model = pipeline.fit(df_train_seasonal)\n",
    "dev_pred = model.transform(df_test_seasonal)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645fcdb1-4666-44f7-b025-6e78ae420239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06668a9-0951-4424-9e00-369e416ef150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c866ee-8032-4493-818f-e209e89e9717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a5ee37-bb4b-468a-a309-c4a991b54436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_pipeline(params):\n",
    "\n",
    "    # List to hold the stages of the pipeline\n",
    "    stages = []\n",
    "\n",
    "    # Index and encode categorical columns\n",
    "    for column in categorical_cols:\n",
    "        indexer = StringIndexer(\n",
    "            inputCol=column, \n",
    "            outputCol=column + \"_index\", \n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        encoder = OneHotEncoder(\n",
    "            inputCol=column + \"_index\", \n",
    "            outputCol=column + \"_vec\", \n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        stages += [indexer, encoder]\n",
    "\n",
    "    categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "\n",
    "\n",
    "    # Interaction feature engineering\n",
    "    interaction1 = Interaction(\n",
    "        inputCols=[\"origin_HourlyPrecipitation\", \"origin_HourlyWetBulbTemperature\"],\n",
    "        outputCol=\"PrecipTemp\"\n",
    "    )\n",
    "\n",
    "    interaction2 = Interaction(\n",
    "        inputCols=[\"origin_type_vec\", \"turnaround_time_calc\"],\n",
    "        outputCol=\"airport_turnaround\"\n",
    "    )\n",
    "\n",
    "\n",
    "    features = numeric_cols + categorical_vec_columns + [\"PrecipTemp\", \"airport_turnaround\"]\n",
    "\n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=features, \n",
    "        outputCol=\"features\", \n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\"\n",
    "    )\n",
    "\n",
    "    # Logistic regression model\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features_scaled\", \n",
    "        labelCol=\"outcome\", \n",
    "        maxIter=50,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    # Build final pipeline\n",
    "    pipeline = Pipeline(stages=stages + [interaction1, interaction2, assembler, scaler, lr])\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ad4b95-02f4-4268-9812-67f61c73dbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regParam': [0.0, 0.01, 0.1], \n",
    "    'elasticNetParam': [0.0, 0.5, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9fddec-bbd5-48cc-b57c-52639f4e096f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, cv_cutoffs, param_grid, 'lr', create_pipeline, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4475b8ee-adaf-4fd1-a571-0095e07d701b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def timeSeriesSplitCV(df, cv_info, param_grid, model_type, pipeline_func, \n",
    "                     sampling=None, metric='f2', verbose=True, dep_utc_varname='dep_utc'):\n",
    "    \"\"\"\n",
    "    Revised grid search using pipeline reconstruction\n",
    "    \"\"\"\n",
    "    k = len(cv_info)\n",
    "    \n",
    "    # Generate parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(product(*param_values))\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    for p in param_combinations:\n",
    "        # Convert tuple to parameter dictionary\n",
    "        params = dict(zip(param_names, p))\n",
    "        \n",
    "        # Create new pipeline with current parameters\n",
    "        pipeline = pipeline_func(params)  # Modified from original get_model approach\n",
    "        \n",
    "        print(f\"\\nEvaluating parameters: {params}\")\n",
    "        \n",
    "        fold_scores = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            # Existing fold setup\n",
    "            train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \n",
    "                                (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "            dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \n",
    "                             (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache()\n",
    "            \n",
    "            # Existing sampling and feature engineering\n",
    "            if sampling == 'down':\n",
    "                train_df = downsample(train_df).cache()\n",
    "            elif sampling == 'up':\n",
    "                train_df = upsample(train_df).cache()\n",
    "            \n",
    "            train_df = get_seasonality_data(train_df, i, k).fillna(0)\n",
    "            dev_df = get_seasonality_data(dev_df, i, k).fillna(0)\n",
    "            \n",
    "            # Train and evaluate\n",
    "            model = pipeline.fit(train_df)\n",
    "            dev_pred = model.transform(dev_df)\n",
    "            score = cv_eval(dev_pred)[0] if metric == 'f2' else cv_eval(dev_pred)[1]\n",
    "            fold_scores.append(score)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'    Fold {i+1} {metric}: {score:.4f}')\n",
    "        \n",
    "        avg_score = np.mean(fold_scores)\n",
    "        print(f'Average {metric} for {params}: {avg_score:.4f}')\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = params\n",
    "    \n",
    "    print(\"\\n=== Best Parameters ===\")\n",
    "    print(best_params)\n",
    "    print(f\"Best Average {metric}: {best_score:.4f}\")\n",
    "    \n",
    "    return best_params, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aca07f2-34aa-4816-bb3e-66cefe5177e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8196da10-acea-49d7-87b7-d24669fe1245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3089b4-39e6-43e4-a30e-17ef33dfb1e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "=== Best Parameters ===\n",
    "\n",
    "{'regParam': 0.1, 'elasticNetParam': 0.0}\n",
    "\n",
    "Best Average f2: 0.5895\n",
    "\n",
    "({'regParam': 0.1, 'elasticNetParam': 0.0}, 0.5894799999999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2f5af0-8b89-4a0d-af88-e9f1e28c9c95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Interaction\n",
    "\n",
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "\n",
    "\n",
    "# Interaction feature engineering\n",
    "interaction1 = Interaction(\n",
    "    inputCols=[\"origin_HourlyPrecipitation\", \"origin_HourlyWetBulbTemperature\"],\n",
    "    outputCol=\"PrecipTemp\"\n",
    ")\n",
    "\n",
    "interaction2 = Interaction(\n",
    "    inputCols=[\"origin_type_vec\", \"turnaround_time_calc\"],\n",
    "    outputCol=\"airport_turnaround\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "features = numeric_cols + categorical_vec_columns + [\"PrecipTemp\", \"airport_turnaround\"]\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features_scaled\", \n",
    "    labelCol=\"outcome\", \n",
    "    maxIter=50,\n",
    "    regParam = .1\n",
    ")\n",
    "\n",
    "# Build final pipeline\n",
    "pipeline = Pipeline(stages=stages + [interaction1, interaction2, assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d73951-cc3e-47d9-8e34-ce4fa5a9bd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "# split into train and test\n",
    "\n",
    "df_train = df_handled.filter(F.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "df_test = df_handled.filter(F.col(dep_utc_varname) >= min_test_dt)\n",
    "df_test.cache()\n",
    "\n",
    "df_train_downsampled = downsample(df_train).cache() #downsample\n",
    "\n",
    "df_train_seasonal= get_seasonality_data(df_train_downsampled, 'full', k).cache()\n",
    "df_test_seasonal = get_seasonality_data(df_test, 'full', k).cache()\n",
    "\n",
    "model = pipeline.fit(df_train_seasonal)\n",
    "dev_pred = model.transform(df_test_seasonal)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaba55bd-601f-4def-a082-b259656574c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Train seasonality models for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831adc2c-61e3-4984-a797-764958c5d2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# informed by: https://www.databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html\n",
    "\n",
    "def forecast_delay(history_pd: pd.DataFrame) -> pd.DataFrame: \n",
    "    \n",
    "    # define Prophet model\n",
    "    model = Prophet(\n",
    "        interval_width=0.9,\n",
    "        growth='linear',\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        # holidays=us_holidays,\n",
    "        # seasonality_mode='multiplicative'\n",
    "    )\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(history_pd)\n",
    "    \n",
    "    # configure predictions\n",
    "    future_pd = model.make_future_dataframe(\n",
    "        periods=24*7, \n",
    "        freq='h',\n",
    "        include_history=False\n",
    "    )\n",
    "    \n",
    "    # make predictions\n",
    "    results_pd = model.predict(future_pd)\n",
    "\n",
    "    # ref date and dow\n",
    "    ref_date = history_pd.ds.iloc[0].date()\n",
    "    ref_dow = history_pd.DAY_OF_WEEK[0]\n",
    "\n",
    "    # helper function: get day of the week,\n",
    "    # using reference date and dow\n",
    "    def get_dow(x,ref_date,dow):\n",
    "        d_days = (x.date() - ref_date).days + dow\n",
    "        d_days = d_days%7\n",
    "        if d_days == 0:\n",
    "            d_days = 7\n",
    "        return d_days\n",
    "\n",
    "    # get dow for forecasted points\n",
    "    results_pd['dow'] = results_pd.ds.apply(lambda x: get_dow(x,ref_date,ref_dow))\n",
    "\n",
    "    # get hour for forecasted points\n",
    "    results_pd['hour'] = results_pd.ds.apply(lambda x: x.hour)\n",
    "\n",
    "    # store origin\n",
    "    results_pd['ORIGIN'] = history_pd.ORIGIN.iloc[0]\n",
    "        \n",
    "    # return components\n",
    "    return results_pd[['dow','hour','weekly','daily','ORIGIN']]\n",
    "\n",
    "schema = StructType([StructField('dow', LongType(), True),\n",
    "                     StructField('hour', LongType(), True),\n",
    "                     StructField('weekly', DoubleType(), True),\n",
    "                     StructField('daily', DoubleType(), True),\n",
    "                     StructField('ORIGIN', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3583341c-b65e-4213-bd4e-005b21d1ff44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality_data(df, fold, k):\n",
    "    if fold == 'full':\n",
    "        fn_model = f\"seasonality_model_{period}_train.parquet\"\n",
    "    else:\n",
    "        fn_model = f\"seasonality_model_{period}_cv{fold}of{k}.parquet\"\n",
    "        \n",
    "    model = spark.read.parquet(f\"{team_BASE_DIR}/interim/{fn_model}\")\n",
    "\n",
    "    joined_df = df.join(model, \n",
    "                     (df[\"ORIGIN\"] == model[\"ORIGIN\"]) & \n",
    "                     (df[\"DAY_OF_WEEK\"] == model[\"dow\"]) & \n",
    "                     (df[\"dep_hour_utc\"] == model[\"hour\"]),\n",
    "                     how=\"left\").drop(model[\"ORIGIN\"])\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "# display(get_seasonality_data(df_train.limit(10), 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4aa8a8-0eee-46a9-9655-8ef389670e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(get_seasonality_data(df_test.limit(10), 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99c46b2-0646-4273-8e61-039814fef520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47508fe2-2bf8-4219-8279-5b95abbadef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality( df, t_min, t_max, \n",
    "    dep_utc_varname=dep_utc_varname, delay_varname=\"DEP_DELAY\", \n",
    "    forecast_fn=forecast_delay, schema=schema ):\n",
    "\n",
    "    return (\n",
    "        df.filter((df[dep_utc_varname] >= t_min) & \\\n",
    "            (df[dep_utc_varname] < t_max))\n",
    "        .withColumnRenamed(delay_varname, \"y\")\n",
    "        .withColumnRenamed(dep_utc_varname, \"ds\")\n",
    "        .groupBy('ORIGIN')\n",
    "            .applyInPandas(forecast_fn, schema=schema)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53913682-beee-47d2-9f66-20151ae8913b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if compute_seasonality:\n",
    "    # train seasonality model for each cross validation split\n",
    "    for i in range(k):\n",
    "        # train seasonality model for this cross validation split\n",
    "        model = get_seasonality(df_train, cv_cutoffs[\"train_min\"][i], cv_cutoffs[\"train_max\"][i])\n",
    "        # write out\n",
    "        fn_out = f\"seasonality_model_{period}_cv{i}of{k}.parquet\"\n",
    "        model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")\n",
    "\n",
    "    # train seasonality model for full training data\n",
    "    model = get_seasonality(df_train, datetime(1970,1,1), min_test_dt)\n",
    "    # write out\n",
    "    fn_out = f\"seasonality_model_{period}_train.parquet\"\n",
    "    model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "370449e3-ccbb-480e-bf6c-13ad96f022dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Troubleshooting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624e0be0-902a-421a-b4f7-19c237739133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred.groupBy('prediction').count().show() #why :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a87da18-d0db-4f31-9b81-8f9db13a90cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred.groupBy('outcome').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b217dc41-f10d-42a0-8b71-8b16a5a13459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_seasonal.groupBy('outcome').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26779c94-ce2d-48c5-92a9-636f0bb73c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test_seasonal.groupBy('outcome').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a726c376-3439-40d4-95b0-db58a2e8d5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad1c449a-2784-49d7-8557-a4a6125430b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1725d5c-8184-419c-971d-0bf90b8cff30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# lag sandbox - ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "485233fa-d1fb-497c-8412-de2899166450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WindowConditions = Window.partitionBy(\"TAIL_NUM\").orderBy(\"sched_depart_utc\")\n",
    "\n",
    "WhenConditions = (f.col(\"ORIGIN\") == f.col(\"priorflight_dest\")) & (f.col(\"priorflight_sched_deptime\") >= f.col(\"twentysix_hours_prior_depart_UTC\"))\n",
    "\n",
    "\n",
    "def add_lags(df):\n",
    "\n",
    "    result_df = (df\n",
    "                 .withColumn(\"priorflight_origin\",\n",
    "                             lag(\"ORIGIN\").over(WindowConditions))\n",
    "                 .withColumn(\"priorflight_dest\",\n",
    "                             lag(\"DEST\").over(WindowConditions))\n",
    "                 .withColumn('priorflight_cancelled_true',  #~~~~true cancellation status, assumed known (?)\n",
    "                             lag('CANCELLED').over(WindowConditions))\n",
    "                 .withColumn(\"twentysix_hours_prior_depart_UTC\",\n",
    "                             (f.col(\"two_hours_prior_depart_UTC\") - f.expr(\"INTERVAL 24 HOURS\")).cast(\"timestamp\"))\n",
    "                             \n",
    "                 \n",
    "                 .withColumn(\"priorflight_sched_deptime\",\n",
    "                             f.when(WhenConditions, lag(\"sched_depart_utc\").over(WindowConditions))\n",
    "                             .otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_elapsed_time_calc\", #~~~crs estimated\n",
    "                             lag(\"CRS_ELAPSED_TIME\").over(WindowConditions))\n",
    "                    .withColumn(\"priorflight_elapsed_time_calc\", #~~~turned into interval\n",
    "                                f.when(WhenConditions,f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_elapsed_time_calc\"))\n",
    "                                .otherwise(None)\n",
    "                    )\n",
    "\n",
    "                 .withColumn(\"priorflight_depdelay_true\", #~~~true dep delay\n",
    "                             f.when(WhenConditions,lag(\"DEP_DELAY\").over(WindowConditions))\n",
    "                             .otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "\n",
    "                 .withColumn(\"special_cases\", f.when(WhenConditions, 1).otherwise(f.lit(0.0)))\n",
    "\n",
    "                 .withColumn(\"priorflight_deptime_true\", #~~~true dep time based on true dep delay\n",
    "                     f.when(WhenConditions, (f.col(\"priorflight_sched_deptime\") + \n",
    "                     (f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_depdelay_true\"))))\n",
    "                     .otherwise(None)\n",
    "                 )\n",
    "                 \n",
    "                 \n",
    "                 .withColumn(\"priorflight_isdeparted\", #~~~ only 1 when we definitely knew it left already\n",
    "                             f.when((f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")) \n",
    "                                    & WhenConditions, 1).otherwise(0) #we don't really know about the prior flight for the when conditions\n",
    "                                 ) \n",
    "                 .withColumn(\"priorflight_depdelay_calc\", #~~~estimated dep delay\n",
    "                        f.when(\n",
    "                            # Case 1: Flight departed BEFORE observation window ***** add in when conditions\n",
    "                            ((f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")) & WhenConditions),\n",
    "                            f.col(\"priorflight_depdelay_true\")  # Full delay known\n",
    "                        ).when(\n",
    "                            # Case 2: Flight scheduled to depart BEFORE window, but departed DURING observation window\n",
    "                            (f.col(\"priorflight_sched_deptime\") <= f.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                            (f.col(\"priorflight_deptime_true\") > f.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                            WhenConditions,\n",
    "                            (f.col(\"two_hours_prior_depart_UTC\").cast('long') - f.col(\"priorflight_sched_deptime\").cast('long')) / 60  # Partial delay\n",
    "                        ).otherwise(\n",
    "                            f.lit(0.0)  # Flight scheduled to depart AFTER window; we know nothing and assume departed on time\n",
    "                        )\n",
    "                    )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_deptime_calc\", #~~~estimated dep time based on estimated dep delay\n",
    "                                f.col(\"priorflight_sched_deptime\") + \n",
    "                                (f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_depdelay_calc\")) \n",
    "                            )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_isdelayed_calc\", #~~~estimated delay indicator ** ADJUSTED FOR CANCELLED\n",
    "                             f.when( ((f.col(\"priorflight_depdelay_calc\") >= 15) | (f.col('priorflight_cancelled_true') == 1)), 1).otherwise(0)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"elapsed_time_true\", #~~~true elapsed time for current flight\n",
    "                             f.when(WhenConditions,(f.col(\"AIR_TIME\") + f.col(\"TAXI_IN\") + f.col(\"TAXI_OUT\")\n",
    "                             ).cast(\"int\")).otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"arr_time_true\", #~~~true arrival time based on true dp time + true elased time\n",
    "\n",
    "                                f.col(\"arr_datetime\").cast(\"timestamp\") +\n",
    "                                (f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"ARR_DELAY\"))\n",
    "                        )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_arr_time_true\", #~~~true prior flight arrival time\n",
    "                             lag(\"arr_time_true\").over(WindowConditions)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_isarrived_calc\", #~~~estimated arrival indicator based on whether flight landed before window\n",
    "                             f.when((f.col(\"priorflight_arr_time_true\") <= f.col(\"two_hours_prior_depart_UTC\")) & WhenConditions,1).otherwise(0)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_arr_time_calc\", #~~~estimated arrival time based on 3 scenarios\n",
    "                        f.when(\n",
    "                            f.col(\"priorflight_isarrived_calc\") == 1,  # Case 1: Dep before window, arr after window\n",
    "                            f.col(\"priorflight_arr_time_true\") #so we know the info\n",
    "                        ).when(\n",
    "                            (f.col(\"priorflight_isarrived_calc\") == 0) &  # Case 2: Dep before window, arr after window\n",
    "                            (f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")), \n",
    "                            f.col(\"priorflight_deptime_true\") + f.col(\"priorflight_elapsed_time_calc\")\n",
    "                        ).otherwise(\n",
    "                            f.col(\"priorflight_deptime_calc\") + f.col(\"priorflight_elapsed_time_calc\")  # dep after window, arr after window\n",
    "                        ))\n",
    "                 \n",
    "                 .withColumn(\"turnaround_time_calc\", \n",
    "                             #~~~estimated how much time we have between estimated arrival of prior flight and scheduled departure of current flight\n",
    "                    (f.when(WhenConditions,\n",
    "                        ((f.col(\"sched_depart_utc\").cast(\"long\") - \n",
    "                        f.col(\"priorflight_arr_time_calc\").cast(\"long\")) / 60\n",
    "                    ).cast(\"double\"))\n",
    "                    ).otherwise(None))\n",
    "    )\n",
    "\n",
    "\n",
    "    #fill in edge case values\n",
    "            #~~ 1. if prior flight is cancelled and there was nothing to pull,\n",
    "            #~~ 2. if prior flight dest != current flight origin, \n",
    "            #~~ 3. if it has been >26 hrs since last flight,\n",
    "        # impute:\n",
    "            #turnaround time\n",
    "            #prior flight delay estimation\n",
    "    window2 =  Window.partitionBy(\"ORIGIN\",\"DEST\").orderBy(\"sched_depart_utc\").rowsBetween(-10, -1)\n",
    "    \n",
    "\n",
    "    result_df = (result_df\n",
    "                .withColumn(\"turnaround_time_calc\", \n",
    "                            f.when(((~WhenConditions) | (f.col('priorflight_cancelled_true') == 1)),\n",
    "                            last(f.col(\"turnaround_time_calc\"), ignorenulls=True)\n",
    "                            .over(window2)\n",
    "                                ).otherwise(f.col(\"turnaround_time_calc\"))\n",
    "                )\n",
    "                .withColumn(\"priorflight_depdelay_calc\", \n",
    "                            f.when(((~WhenConditions) | (f.col('priorflight_cancelled_true') == 1)),\n",
    "                            last(f.col(\"priorflight_depdelay_calc\"), ignorenulls=True)\n",
    "                            .over(window2)\n",
    "                                ).otherwise(f.col(\"priorflight_depdelay_calc\"))\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "full = add_lags(out) #first pass to correctly get the current cancelled flights\n",
    "noncancelled=add_lags(out.filter(f.col(\"CANCELLED\") == 0)) #next pass to correctly skip prior cancelled flights for current non cancelled flights\n",
    "result=full.filter(f.col('CANCELLED')==1).unionByName(noncancelled) #combine the two \n",
    "\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "855f68ad-581b-4fd9-88a6-1201d6914481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "redundant=[ 'priorflight_origin',\n",
    " 'priorflight_dest',\n",
    " 'twentysix_hours_prior_depart_UTC',\n",
    " 'priorflight_sched_deptime',\n",
    " 'priorflight_elapsed_time_calc',\n",
    " 'priorflight_depdelay_true',\n",
    " 'priorflight_deptime_true',\n",
    " 'priorflight_depdelay_calc',\n",
    " 'priorflight_deptime_calc',\n",
    " 'priorflight_isdelayed_calc',\n",
    " 'elapsed_time_true',\n",
    " 'arr_time_true',\n",
    " 'priorflight_arr_time_true',\n",
    " 'priorflight_isarrived_calc',\n",
    " 'priorflight_arr_time_calc',\n",
    " 'turnaround_time_calc',\n",
    "]\n",
    "\n",
    "out=out.select([c for c in out.columns if c not in redundant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11f42ba-49f7-4ec2-8274-841fa4160120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = out.withColumn('arr_datetime', F.col('arr_datetime').cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1bd9b0-ce61-4086-91ed-ac78e2a0b6ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "348d9085-aa19-4cc5-8ba6-596c61f968d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')=='259NV').filter(f.col('sched_depart_utc').contains('2019-01-02')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "06094348-b65b-460d-b644-91f156c58271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col(\"ORIGIN\") != f.col(\"priorflight_dest\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b36c047-655a-46da-9713-2e58fcb46ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "769b23b3-a792-4cec-bafd-aa50d73574c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WindowConditions = Window.partitionBy(\"TAIL_NUM\").orderBy(\"sched_depart_utc\")\n",
    "\n",
    "display(result.withColumn(\"prior_cancelled\", \n",
    "                  lag(\"CANCELLED\").over(WindowConditions)).filter(f.col('prior_cancelled')==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32b7d21c-cb9d-41c2-8c5b-4877723380ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "impute missing turnaroudns based on current origin -> destination ema something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "37c6e822-8f5f-4406-bc4b-f6b70c18af15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')==\"N102UW\").filter(f.col('sched_depart_utc').contains('2019-04-09')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d214d82-6d5d-4cb5-a113-b4b68fdd73ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.filter(f.col('turnaround_time_calc').isNull()).groupBy('special_cases','priorflight_cancelled_true').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f4ed156-f793-4fe2-8da5-b8635cddc10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.filter(f.col('turnaround_time_calc').isNull()).groupBy('special_cases','priorflight_cancelled_true').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec1f6be-47d2-4f09-93a6-df09dd84ad19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef10162-3d7c-4ac7-9c77-94ce14d4a892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')=='215NV').orderBy('sched_depart_utc')\n",
    "        .select('sched_depart_utc','ORIGIN','DEST','priorflight_origin','priorflight_dest',\n",
    "                'CANCELLED','priorflight_cancelled_true','priorflight_isarrived_calc','priorflight_deptime_true',\n",
    "                'arr_time_true','priorflight_arr_time_true','priorflight_deptime_calc','priorflight_arr_time_calc','priorflight_elapsed_time_calc','turnaround_time_calc','special_cases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607bb23e-ff35-4538-9007-e58afeb26f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('turnaround_time_calc').isNull()).filter(f.col('priorflight_cancelled_true')==0).filter(f.col('CANCELLED')==0).orderBy('TAIL_NUM','sched_depart_utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c3d5e4-6148-4598-b339-3e5df06c3401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')==\"N102UW\").filter(f.col('sched_depart_utc') >= '2019-03-20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f6af8b-7a02-4e60-8763-219846ca6ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')==\"N102UW\").filter(f.col('sched_depart_utc') >= '2019-03-20'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfd15eb4-94d3-47c7-ac89-4434d79348b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "to clean:\n",
    "prior flight is cancelled, current flight is cancelled: impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed2b80b4-fa4f-4c97-83f8-050ad2a7bec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7531ecba-1046-42c4-8255-527804ad8767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. A good, B good, C good\n",
    "2. A good, B cancelled, C good\n",
    "3. A cancelled, B cancelled, C good **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71589e33-2b40-4281-9b7c-58208f394431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c25e1a-9c7c-496f-b3b1-73b9f2b0c8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# prior_noncancelled=add_lags(full.filter(f.col(\"priorflight_cancelled_true\") == 0))\n",
    "# result2=full.filter(f.col('priorflight_cancelled_true')==1).unionByName(prior_noncancelled)\n",
    "#doesnt resolve issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee8dc9de-8b91-4f63-82a2-33bc67044d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed797ad-5388-450c-8c14-623a6c6f72d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.filter(f.col('DEP_DELAY').isNull()).groupBy('CANCELLED').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "955a167e-fe31-4e03-a092-9403f663315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "null_counts = result.select(\n",
    "    [f.count(f.when(f.col(c).cast(\"long\").isNull() | f.isnan(f.col(c).cast(\"long\")), c)).alias(c) \n",
    "     for c in result.columns]\n",
    ")\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e661571-df29-46b9-88b0-697aefd80db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('priorflight_arr_time_calc').isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2989a01-5f51-4b25-89ac-8a5b001ebfab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(f.col('TAIL_NUM').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc5635c-67b3-4783-9c2d-3e3f23df9ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "output_path = \"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined_1y_weather_cleaned_combo_lags.parquet\"\n",
    "(\n",
    "    result.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour variable (needed for seasonality)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b48f564-6cf2-41bb-9b00-d48f11b5da1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.12-sg-joined-modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
