{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7242c0b-e377-4108-94d5-687826addb3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lagged Features, Re-Engineered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c8e70e1-90a2-4689-8a95-afb18457a5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fac819-914e-4e37-a3fe-855c96df8427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf, lag, pandas_udf, isnan, array, array_contains, explode, lit, countDistinct, first, last, unix_timestamp, to_date, to_timestamp, date_format, date_add\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0b56cb-849a-4af0-9498-f8e8cfe7bba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0b6fc6-dd49-4b2d-8230-dd9dc767304a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/modeling_checkpoints\")\n",
    "\n",
    "# read in joined, cleaned, engineered dataset\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_1y_weather_cleaned_combo.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41cbff4-b790-4e27-b78a-3c0b6dc0ea73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## get arr_time column UTC-ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d76dbaf-aae7-418d-84b5-fd1ebcc6b64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_utc(yyyymmdd, dep_hhmm, arr_hhmm, dep_tz, arr_tz, flight_dur):\n",
    "    \"\"\"\n",
    "    Create UTC timestamp from flights table columns\n",
    "    yyyymmdd = FL_DATE\n",
    "    dep_hhmm = CRS_DEP_TIME\n",
    "    arr_hhmm = CRS_ARR_TIME\n",
    "    dep_tz = origin_timezone\n",
    "    arr_tz = dest_timezone\n",
    "    flight_dur = CRS_ELAPSED_TIME (for sanity check of arrival time)\n",
    "\n",
    "    Returns UTC time stamp, (cast to string)\n",
    "    \"\"\"\n",
    "\n",
    "    dep_hhmm = int(dep_hhmm)\n",
    "    arr_hhmm = int(arr_hhmm)\n",
    "\n",
    "    yyyy,MM,dd = yyyymmdd.split('-')\n",
    "    yyyy = int(yyyy) # get year\n",
    "    MM = int(MM) # get month\n",
    "    dd = int(dd) # get day\n",
    "\n",
    "    dep_hh = dep_hhmm//100 # get hour\n",
    "    dep_mm = dep_hhmm%100 # get minute\n",
    "    if dep_hh == 24:\n",
    "        dep_hh = 0\n",
    "        dep_shift = True\n",
    "    else:\n",
    "        dep_shift = False\n",
    "\n",
    "    arr_hh = arr_hhmm//100 # get hour\n",
    "    arr_mm = arr_hhmm%100\n",
    "    if arr_hh == 24:\n",
    "        arr_hh = 0\n",
    "        arr_shift = True\n",
    "    else:\n",
    "        arr_shift = False\n",
    "\n",
    "    # create datetime variable for departure\n",
    "    dt_dep = datetime(yyyy,MM,dd,dep_hh,dep_mm)\n",
    "    if dep_shift:\n",
    "        dt_dep += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    dep_local = pytz.timezone(dep_tz).localize(dt_dep)\n",
    "    # convert to UTC\n",
    "    dep_utc = dep_local.astimezone(pytz.utc)\n",
    "\n",
    "    # create datetime variable for arrival\n",
    "    dt_arr = datetime(yyyy,MM,dd,arr_hh,arr_mm)\n",
    "    if arr_shift:\n",
    "        dt_arr += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    arr_local = pytz.timezone(arr_tz).localize(dt_arr)\n",
    "    # convert to UTC\n",
    "    arr_utc = arr_local.astimezone(pytz.utc)\n",
    "\n",
    "    if dep_utc > arr_utc:\n",
    "        arr_utc += timedelta(days=1)\n",
    "\n",
    "    # # sanity check\n",
    "    # arr_utc_SC = dep_utc + timedelta(minutes=flight_dur)\n",
    "\n",
    "    dt_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "    # return UTC datetime, cast to string\n",
    "    # return (dep_utc.strftime(dt_format), arr_utc.strftime(dt_format), arr_utc_SC.strftime(dt_format))\n",
    "    return (dep_utc.strftime(dt_format), arr_utc.strftime(dt_format))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"dep_datetime\", StringType(), False),\n",
    "    StructField(\"arr_datetime\", StringType(), False),\n",
    "])\n",
    "\n",
    "dt_udf = udf(to_utc, schema)\n",
    "\n",
    "out = df.withColumn('processed', dt_udf(col(\"FL_DATE\"), col(\"CRS_DEP_TIME\"), col(\"CRS_ARR_TIME\"), col(\"origin_timezone\"), col(\"dest_timezone\"), col(\"CRS_ELAPSED_TIME\"))).cache()\n",
    "\n",
    "cols = [c for c in out.columns if c != \"processed\"]\n",
    "cols += [\"processed.dep_datetime\",\"processed.arr_datetime\"]\n",
    "out = out.select(cols)\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acd9767b-0d6d-422a-80f8-6c095b43315f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handle Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb85ab53-eb48-451c-afbe-db2b8c89c9a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cd5f1c-9c52-4109-b0d9-46e32775bafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out=out.withColumn(\"CRS_ELAPSED_TIME\",\n",
    "              F.when(F.col(\"CRS_ELAPSED_TIME\").isNull(),\n",
    "                     (F.col(\"arr_datetime\")-F.col(\"sched_depart_utc\")).cast('int'))\n",
    "              .otherwise(F.col(\"CRS_ELAPSED_TIME\"))\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afbb1cca-fed4-4af3-9a19-da6a23b4bb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimized Lag Columns Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb63e0bb-9c23-4e50-a58d-02c65e9827e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "WhenConditions = (f.col(\"ORIGIN\") == f.col(\"priorflight_dest\")) & (f.col(\"priorflight_sched_deptime\") >= f.col(\"twentysix_hours_prior_depart_UTC\"))\n",
    "\n",
    "def add_lags_optimized(df):\n",
    "    # Define windows once\n",
    "    aircraft_window = Window.partitionBy(\"TAIL_NUM\").orderBy(\"sched_depart_utc\")\n",
    "    route_window = Window.partitionBy(\"ORIGIN\", \"DEST\").orderBy(\"sched_depart_utc\").rowsBetween(-100, -1)\n",
    "\n",
    "    # Precompute all lagged columns in single pass\n",
    "    lagged_cols = [\n",
    "        F.lag(\"ORIGIN\").over(aircraft_window).alias(\"priorflight_origin\"),\n",
    "        F.lag(\"DEST\").over(aircraft_window).alias(\"priorflight_dest\"),\n",
    "        F.lag(\"CANCELLED\").over(aircraft_window).alias(\"priorflight_cancelled_true\"),\n",
    "        F.lag(\"sched_depart_utc\").over(aircraft_window).alias(\"priorflight_sched_deptime\"),\n",
    "        F.lag(\"CRS_ELAPSED_TIME\").over(aircraft_window).alias(\"priorflight_elapsed_time_calc_raw\"),\n",
    "        F.lag(\"DEP_DELAY\").over(aircraft_window).alias(\"priorflight_depdelay_true_raw\"),\n",
    "        F.lag(\"arr_datetime\").over(aircraft_window).alias(\"priorflight_arr_time_true\")\n",
    "    ]\n",
    "\n",
    "    # Base transformations\n",
    "    base_df = (df\n",
    "        .withColumn(\"twentysix_hours_prior_depart_UTC\", \n",
    "                   (F.col(\"two_hours_prior_depart_UTC\") - F.expr(\"INTERVAL 24 HOURS\")).cast(\"timestamp\"))\n",
    "        .select(\"*\", *lagged_cols)\n",
    "    )\n",
    "\n",
    "    # Precompute common conditions\n",
    "    WhenConditions = (\n",
    "        (F.col(\"ORIGIN\") == F.col(\"priorflight_dest\")) & \n",
    "        (F.col(\"priorflight_sched_deptime\") >= F.col(\"twentysix_hours_prior_depart_UTC\"))\n",
    "    )\n",
    "    valid_prior = WhenConditions & (F.col(\"priorflight_cancelled_true\") != 1)\n",
    "\n",
    "    # Core calculations\n",
    "    result_df = (base_df\n",
    "        .withColumn(\"priorflight_elapsed_time_calc\",\n",
    "            F.when(valid_prior,\n",
    "                F.expr(\"INTERVAL 1 MINUTE\") * F.col(\"priorflight_elapsed_time_calc_raw\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"priorflight_depdelay_true\",\n",
    "            F.when(valid_prior, F.col(\"priorflight_depdelay_true_raw\"))\n",
    "        )\n",
    "        .withColumn(\"priorflight_deptime_true\",\n",
    "            F.when(valid_prior,\n",
    "                F.col(\"priorflight_sched_deptime\") + \n",
    "                (F.expr(\"INTERVAL 1 MINUTE\") * F.col(\"priorflight_depdelay_true\"))\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"priorflight_isdeparted\",\n",
    "            F.when(\n",
    "                (F.col(\"priorflight_deptime_true\") <= F.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                valid_prior, 1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "        .withColumn(\"priorflight_depdelay_calc\",\n",
    "            F.when(\n",
    "                (F.col(\"priorflight_deptime_true\") <= F.col(\"two_hours_prior_depart_UTC\")) & valid_prior,\n",
    "                F.col(\"priorflight_depdelay_true\")\n",
    "            ).when(\n",
    "                (F.col(\"priorflight_sched_deptime\") <= F.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                (F.col(\"priorflight_deptime_true\") > F.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                valid_prior,\n",
    "                (F.col(\"two_hours_prior_depart_UTC\").cast('long') - \n",
    "                 F.col(\"priorflight_sched_deptime\").cast('long')) / 60\n",
    "            ).otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\"priorflight_deptime_calc\",\n",
    "            F.col(\"priorflight_sched_deptime\") + \n",
    "            (F.expr(\"INTERVAL 1 MINUTE\") * F.col(\"priorflight_depdelay_calc\"))\n",
    "        )\n",
    "        .withColumn(\"priorflight_isdelayed_calc\",\n",
    "            F.when(\n",
    "                (F.col(\"priorflight_depdelay_calc\") >= 15) | \n",
    "                (F.col('priorflight_cancelled_true') == 1), 1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "        .withColumn(\"elapsed_time_true\",\n",
    "            F.when(valid_prior,\n",
    "                (F.col(\"AIR_TIME\") + F.col(\"TAXI_IN\") + F.col(\"TAXI_OUT\")).cast(\"int\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"arr_time_true\",\n",
    "            F.col(\"arr_datetime\").cast(\"timestamp\") +\n",
    "            (F.expr(\"INTERVAL 1 MINUTE\") * F.col(\"ARR_DELAY\"))\n",
    "        )\n",
    "        .withColumn(\"priorflight_isarrived_calc\",\n",
    "            F.when(\n",
    "                (F.col(\"priorflight_arr_time_true\") <= F.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                valid_prior, 1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "        .withColumn(\"priorflight_arr_time_calc\",\n",
    "            F.when(\n",
    "                F.col(\"priorflight_isarrived_calc\") == 1,\n",
    "                F.col(\"priorflight_arr_time_true\")\n",
    "            ).when(\n",
    "                (F.col(\"priorflight_isarrived_calc\") == 0) &\n",
    "                (F.col(\"priorflight_deptime_true\") <= F.col(\"two_hours_prior_depart_UTC\")), \n",
    "                F.col(\"priorflight_deptime_true\") + F.col(\"priorflight_elapsed_time_calc\")\n",
    "            ).otherwise(\n",
    "                F.col(\"priorflight_deptime_calc\") + F.col(\"priorflight_elapsed_time_calc\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"turnaround_time_calc\",\n",
    "            F.when(valid_prior,\n",
    "                ((F.col(\"sched_depart_utc\").cast(\"long\") - \n",
    "                  F.col(\"priorflight_arr_time_calc\").cast(\"long\")) / 60).cast(\"double\")\n",
    "            )\n",
    "        )\n",
    "        # Edge case handling\n",
    "        .withColumn(\"turnaround_time_calc\",\n",
    "            F.when(\n",
    "                (~valid_prior) | (F.col(\"priorflight_cancelled_true\") == 1),\n",
    "                F.last(\"turnaround_time_calc\", ignorenulls=True).over(route_window)\n",
    "            ).otherwise(F.col(\"turnaround_time_calc\"))\n",
    "        )\n",
    "        .withColumn(\"priorflight_depdelay_calc\",\n",
    "            F.when(\n",
    "                (~valid_prior) | (F.col(\"priorflight_cancelled_true\") == 1),\n",
    "                F.last(\"priorflight_depdelay_calc\", ignorenulls=True).over(route_window)\n",
    "            ).otherwise(F.col(\"priorflight_depdelay_calc\"))\n",
    "        )\n",
    "    ).cache()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Optimized execution flow\n",
    "def optimized_pipeline(input_df):\n",
    "    # Single pass processing with conditional logic\n",
    "    return add_lags_optimized(\n",
    "        input_df.withColumn(\"is_cancelled\", F.col(\"CANCELLED\") == 1)\n",
    "    )\n",
    "\n",
    "# Execute pipeline\n",
    "result = optimized_pipeline(out)\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7e7f9b-aa8d-45d6-b1ca-ecd0cacd0889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aaa1dd3-701f-4501-bf47-3bd39bd8f1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.count() #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "854eb0d1-b54f-40aa-919c-0db7ec2733a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7642559c-a156-436b-9346-bbe54eb6c524",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1743781427753}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM').isNotNull()).orderBy('TAIL_NUM','sched_depart_utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15988ab3-4cb4-428c-aa75-d6fe74f0ccc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_path = \"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined_1y_cleaned_engineered.parquet\"\n",
    "(\n",
    "    result.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(output_path)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c894141c-2474-4da9-92b2-7a77deb1afbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Delay indicator null count: {result.filter(F.col('priorflight_isdelayed_calc').isNull()).count()}\")\n",
    "\n",
    "print(f\"Departed indicator null count: {result.filter(F.col('priorflight_isdeparted').isNull()).count()}\")\n",
    "\n",
    "print(f\"Arrival indicator null count: {result.filter(F.col('priorflight_isarrived_calc').isNull()).count()}\")\n",
    "\n",
    "print(f\"Est. Turnaround Time Null Count: {result.filter(F.col('turnaround_time_calc').isNull()).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c171ba5-5e8e-4479-a97d-0b893999e0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Delay estimate null count: {result.filter(F.col('priorflight_isdelayed_calc').isNull()).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "438ea80c-c6ee-4e36-b7e7-0744c4329e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(F.col('turnaround_time_calc').isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3423750c-6397-4e40-b8b8-01444beff9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "result = result.fillna({'turnaround_time_calc': 0}) #don't have enough information to calculate the turnaround time, so just assume something is going on that wont give us enough time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88e6f453-a83c-4e6f-80ec-f9b0ac59cfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "027f948b-d6e7-498b-b53c-53cd6cda381a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e1da0c-a773-4e7f-948d-3f4e3fd880a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined_1y_cleaned_engineered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97011666-cbc6-4d4e-9ec2-5663ac03b865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.fillna({'turnaround_time_calc': 0}) #don't have enough information to calculate the turnaround time, so just assume something is going on that wont give us enough time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a14baa14-4537-47c9-a892-dc843a037fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec9db66-5c5a-4f72-804a-812960c33b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29625370-5bb1-4954-94ba-dfa34953e54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"1y\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds\n",
    "k = 3\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup) #using false since erica already computed\n",
    "compute_seasonality = False\n",
    "\n",
    "# define train/test split date\n",
    "min_test_dt = \"2019-10-01\"\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710651f6-e7fe-4bc7-b877-213aa6794ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_handled = df.withColumns(\n",
    "    {\n",
    "        \"dep_hour_utc\": \n",
    "            F.hour(col(dep_utc_varname)),\n",
    "        \"outcome\":  \n",
    "            (F.when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\")\n",
    "            }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "\n",
    "df_train = df_handled.filter(F.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "df_test = df_handled.filter(F.col(dep_utc_varname) >= min_test_dt)\n",
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a201ff7-9a78-4197-a6a5-7528dee4964c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e079e4-266b-4e77-9ba8-154a1f0addfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits(df, k=3, blocking=False, dep_utc_varname=\"dep_datetime\", verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation\n",
    "    '''\n",
    "    n = df.count()\n",
    "    df = df.withColumn(\"row_id\", f.row_number()\n",
    "            .over(Window.partitionBy().orderBy(dep_utc_varname)))\n",
    "    chunk_size = np.floor(n/(k+1))\n",
    "\n",
    "    idx = np.arange(0,)\n",
    "    idx = np.arange(0,n,chunk_size)\n",
    "    idx[-1] = n-1\n",
    "    idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print('')\n",
    "        print(f'Number of validation datapoints for each fold is {chunk_size:,}')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    bin_edges = df.filter(f.col(\"row_id\").isin(idx)).select(\"row_id\",dep_utc_varname).toPandas()\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = bin_edges[dep_utc_varname][0]\n",
    "        else:\n",
    "            t_min_train = bin_edges[dep_utc_varname][i]\n",
    "        # define maximum training time\n",
    "        t_max_train = bin_edges[dep_utc_varname][i+1]\n",
    "        # define minimum test time\n",
    "        t_min_test = bin_edges[dep_utc_varname][i+1]\n",
    "        # define maximum test_time\n",
    "        t_max_test = bin_edges[dep_utc_varname][i+2]\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get cross-validation split times\n",
    "cv_cutoffs = get_cv_time_limits(df_train.select(dep_utc_varname), k=3, blocking=True, \n",
    "    dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37265166-1734-48d5-b8cd-9412ac618185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality_data(df, fold, k):\n",
    "  \"\"\"\n",
    "  Look up seasonlaity features from saved seasonality model.\n",
    "  \"\"\"\n",
    "  if fold == 'full':\n",
    "      fn_model = f\"seasonality_model_{period}_train.parquet\"\n",
    "  else:\n",
    "      fn_model = f\"seasonality_model_{period}_cv{fold}of{k}.parquet\"\n",
    "  model = spark.read.parquet(f\"{team_BASE_DIR}/interim/{fn_model}\")\n",
    "\n",
    "  joined_df = df.join(model, \n",
    "                    (df[\"ORIGIN\"] == model[\"ORIGIN\"]) & \n",
    "                    (df[\"DAY_OF_WEEK\"] == model[\"dow\"]) & \n",
    "                    (df[\"dep_hour_utc\"] == model[\"hour\"]),\n",
    "                    how=\"left\").drop(model[\"ORIGIN\"])\n",
    "  \n",
    "  return joined_df\n",
    "\n",
    "# CODE BELOW DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def upsample(train_df,verbose=False):\n",
    "  '''Upsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = non_delay_count / delay_count\n",
    "\n",
    "  train_delay = train_df.filter(f.col('outcome') == 0)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 1).sample(withReplacement=True, fraction=keep_percent,seed=42)\n",
    "  train_upsampled = train_delay.union(train_non_delay)\n",
    "  return train_upsampled\n",
    "\n",
    "\n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n",
    "\n",
    "def cv_eval(preds):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  return F2, pr\n",
    "\n",
    "def timeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timSeriesSplit k-fold cross validation \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    # elif sampling=='weights':\n",
    "    #   train_df = add_class_weights(train_df).cache()\n",
    "      \n",
    "    #print info on train and dev set for this fold\n",
    "    if verbose:\n",
    "      print('    TRAIN set for fold {} goes from {} to {}, count is {:,} flights ({})'.format((i+1), \n",
    "                                                                                      train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      train_df.count(),\n",
    "                                                                                      sampling + '-sampled' if sampling else 'no sampling'))\n",
    "      print('    DEV set for fold {} goes from {} to {}, count is {:,} flights'.format((i+1), \n",
    "                                                                                      dev_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      dev_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      dev_df.count()))\n",
    "      \n",
    "    # TODO: remove once feat engineering applied outside\n",
    "    train_df = get_seasonality_data(train_df, i, k)\n",
    "    train_df = train_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "    dev_df = get_seasonality_data(dev_df, i, k)\n",
    "    dev_df = dev_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "\n",
    "    # print(train_df.dtypes)\n",
    "    # print(dev_df.dtypes)\n",
    "        \n",
    "    # Fit params on the model\n",
    "    model = pipeline.fit(train_df)\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "  # # Train on full df\n",
    "  # print('Training on full train dataset, and validating on dev dataset with best parameters from CV:')\n",
    "  # print(best_parameters)\n",
    "    \n",
    "  # if verbose:\n",
    "  #   print('    TRAIN set for best parameter fitted model goes from {} to {}, count is {:,} flights ({})'.format(train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.count(),\n",
    "  #                                                                                                    sampling + '-sampled' if sampling else 'no sampling'))\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff111903-d019-486b-aaf0-6ac0fbac1f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8560b26e-c783-451c-abd0-393b8dcc280a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb6629c6-9d35-44cf-b90e-3e584baa1f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/modeling_checkpoints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d301af5-120c-4ba6-ad2d-658c395a4db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\"]\n",
    "\n",
    "# date related columns\n",
    "date_cols = [\"YEAR\",\"QUARTER\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "\n",
    "# prior & current flight cols\n",
    "num_flight_cols = ['turnaround_time_calc', 'priorflight_depdelay_calc','DISTANCE','CRS_ELAPSED_TIME','priorflight_elapsed_time_calc_raw']\n",
    "bool_flight_cols = ['priorflight_isdeparted', 'priorflight_isarrived_calc','priorflight_isdelayed_calc']\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\", \"origin_type\", \"dest_type\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",\"DAY_OF_WEEK\",\"ORIGIN\",\"dep_hour_utc\",dep_utc_varname]\n",
    "\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *num_flight_cols]\n",
    "categorical_cols = [*bool_flight_cols, *flight_metadata_cols, *date_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e2c82e-d2ea-41fb-bf16-c3c4399ed795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "ncols = [*num_weather_cols, *num_flight_cols]\n",
    "# Count NaNs in numeric columns\n",
    "nan_counts_numeric = df_handled.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in ncols])\n",
    "\n",
    "# Count NaNs in categorical columns\n",
    "nan_counts_categorical = df_handled.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in categorical_cols])\n",
    "\n",
    "display(nan_counts_numeric)\n",
    "display(nan_counts_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6ec79a-dd9b-40ae-aeb9-6b90b10540ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # List to hold the stages of the pipeline\n",
    "# stages = []\n",
    "\n",
    "# # Index and encode each categorical column\n",
    "# for column in categorical_cols:\n",
    "#     indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\",handleInvalid=\"keep\")\n",
    "#     encoder = OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_vec\",handleInvalid=\"keep\")\n",
    "#     stages += [indexer, encoder]\n",
    "# # define encoded categorical feature names\n",
    "# categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "# # assemble features\n",
    "# features = numeric_cols + categorical_vec_columns\n",
    "# assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid='keep')\n",
    "\n",
    "# # scale features\n",
    "# scaler = MinMaxScaler(inputCol=\"features\", \\\n",
    "#     outputCol=\"features_scaled\")\n",
    "\n",
    "# # logistic regression model\n",
    "# lr = LogisticRegression(featuresCol='features_scaled', \\\n",
    "#     labelCol='outcome',maxIter=50)\n",
    "\n",
    "# # construct pipeline object from all components\n",
    "# pipeline = Pipeline(stages=stages+[assembler,scaler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36975152-b9f1-49da-a890-06cfcaa2465c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# 1. Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# 2. Median imputation for numerical columns\n",
    "impute_cols = [\n",
    "    \"turnaround_time_calc\", \n",
    "    \"priorflight_depdelay_calc\", \n",
    "    \"priorflight_elapsed_time_calc_raw\"\n",
    "]\n",
    "imputer = Imputer(\n",
    "    inputCols=impute_cols,\n",
    "    outputCols=[col + \"_imputed\" for col in impute_cols],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "stages += [imputer]\n",
    "\n",
    "\n",
    "# 4. Update feature list to include imputed columns\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "numeric_cols_imputed = [col + \"_imputed\" for col in impute_cols] + \\\n",
    "    [col for col in numeric_cols if col not in impute_cols]\n",
    "\n",
    "features = numeric_cols_imputed + categorical_vec_columns\n",
    "\n",
    "# 5. Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 6. Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "# 7. Logistic regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features_scaled\", \n",
    "    labelCol=\"outcome\", \n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "# Build final pipeline\n",
    "pipeline = Pipeline(stages=stages + [assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16ac16f7-f6dd-459d-a429-0ce77e43f195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "336a51c5-8688-4d94-a3f3-c54184a3a523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "091a47fb-d4fc-4454-af59-3e127dbc481f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6ddaf0-8e09-45fe-a390-8ada97a63e68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "df_train_downsampled = downsample(df_train).cache()\n",
    "df_train_seasonal = get_seasonality_data(df_train_downsampled, 'full', k).cache()\n",
    "df_test_seasonal = get_seasonality_data(df_test, 'full', k).cache()\n",
    "model = pipeline.fit(df_train_seasonal)\n",
    "dev_pred = model.transform(df_test_seasonal)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a910a2bf-c4fa-4da6-ae9a-3a6d0e186d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**F2 SCORE: Prior Features Model**\n",
    "\n",
    "Train/cv: 0.5861666666666666\n",
    "\n",
    "Test: 0.5727"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c1d01ad-61ba-4fd5-be48-034b800829c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71ae72b-4067-4017-a2c4-862c3124203d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Interaction\n",
    "from pyspark.ml.feature import Imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f7d0c2-59ba-4fd1-9fea-2dd8354b92db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# 1. Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# 2. Median imputation for numerical columns\n",
    "impute_cols = [\n",
    "    \"turnaround_time_calc\", \n",
    "    \"priorflight_depdelay_calc\", \n",
    "    \"priorflight_elapsed_time_calc_raw\"\n",
    "]\n",
    "imputer = Imputer(\n",
    "    inputCols=impute_cols,\n",
    "    outputCols=[col + \"_imputed\" for col in impute_cols],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "stages += [imputer]\n",
    "\n",
    "# 3. Interaction feature engineering\n",
    "interaction = Interaction(\n",
    "    inputCols=[\"origin_HourlyPrecipitation\", \"origin_HourlyWetBulbTemperature\"],\n",
    "    outputCol=\"interactedCol\"\n",
    ")\n",
    "\n",
    "# 4. Update feature list to include imputed columns\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "numeric_cols_imputed = [col + \"_imputed\" for col in impute_cols] + \\\n",
    "    [col for col in numeric_cols if col not in impute_cols]\n",
    "\n",
    "features = numeric_cols_imputed + categorical_vec_columns + [\"interactedCol\"]\n",
    "\n",
    "# 5. Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 6. Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "# 7. Logistic regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features_scaled\", \n",
    "    labelCol=\"outcome\", \n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "# Build final pipeline\n",
    "pipeline = Pipeline(stages=stages + [interaction, assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00205f9c-2d61-4a3e-a373-5e9aed57c1d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "\n",
    "\n",
    "\n",
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbeb744-3793-41de-89e7-e17efa1b7c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "# split into train and test\n",
    "\n",
    "df_train = df_handled.filter(F.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "df_test = df_handled.filter(F.col(dep_utc_varname) >= min_test_dt)\n",
    "df_test.cache()\n",
    "\n",
    "df_train_downsampled = downsample(df_train).cache() #downsample\n",
    "\n",
    "df_train_seasonal= get_seasonality_data(df_train_downsampled, 'full', k).cache()\n",
    "df_test_seasonal = get_seasonality_data(df_test, 'full', k).cache()\n",
    "\n",
    "model = pipeline.fit(df_train_seasonal)\n",
    "dev_pred = model.transform(df_test_seasonal)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "370449e3-ccbb-480e-bf6c-13ad96f022dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### checking predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624e0be0-902a-421a-b4f7-19c237739133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred.groupBy('prediction').count().show() #why :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a87da18-d0db-4f31-9b81-8f9db13a90cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred.groupBy('outcome').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b217dc41-f10d-42a0-8b71-8b16a5a13459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_seasonal.groupBy('outcome').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26779c94-ce2d-48c5-92a9-636f0bb73c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test_seasonal.groupBy('outcome').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaba55bd-601f-4def-a082-b259656574c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train seasonality models for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831adc2c-61e3-4984-a797-764958c5d2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# informed by: https://www.databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html\n",
    "\n",
    "def forecast_delay(history_pd: pd.DataFrame) -> pd.DataFrame: \n",
    "    \n",
    "    # define Prophet model\n",
    "    model = Prophet(\n",
    "        interval_width=0.9,\n",
    "        growth='linear',\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        # holidays=us_holidays,\n",
    "        # seasonality_mode='multiplicative'\n",
    "    )\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(history_pd)\n",
    "    \n",
    "    # configure predictions\n",
    "    future_pd = model.make_future_dataframe(\n",
    "        periods=24*7, \n",
    "        freq='h',\n",
    "        include_history=False\n",
    "    )\n",
    "    \n",
    "    # make predictions\n",
    "    results_pd = model.predict(future_pd)\n",
    "\n",
    "    # ref date and dow\n",
    "    ref_date = history_pd.ds.iloc[0].date()\n",
    "    ref_dow = history_pd.DAY_OF_WEEK[0]\n",
    "\n",
    "    # helper function: get day of the week,\n",
    "    # using reference date and dow\n",
    "    def get_dow(x,ref_date,dow):\n",
    "        d_days = (x.date() - ref_date).days + dow\n",
    "        d_days = d_days%7\n",
    "        if d_days == 0:\n",
    "            d_days = 7\n",
    "        return d_days\n",
    "\n",
    "    # get dow for forecasted points\n",
    "    results_pd['dow'] = results_pd.ds.apply(lambda x: get_dow(x,ref_date,ref_dow))\n",
    "\n",
    "    # get hour for forecasted points\n",
    "    results_pd['hour'] = results_pd.ds.apply(lambda x: x.hour)\n",
    "\n",
    "    # store origin\n",
    "    results_pd['ORIGIN'] = history_pd.ORIGIN.iloc[0]\n",
    "        \n",
    "    # return components\n",
    "    return results_pd[['dow','hour','weekly','daily','ORIGIN']]\n",
    "\n",
    "schema = StructType([StructField('dow', LongType(), True),\n",
    "                     StructField('hour', LongType(), True),\n",
    "                     StructField('weekly', DoubleType(), True),\n",
    "                     StructField('daily', DoubleType(), True),\n",
    "                     StructField('ORIGIN', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3583341c-b65e-4213-bd4e-005b21d1ff44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality_data(df, fold, k):\n",
    "    if fold == 'full':\n",
    "        fn_model = f\"seasonality_model_{period}_train.parquet\"\n",
    "    else:\n",
    "        fn_model = f\"seasonality_model_{period}_cv{fold}of{k}.parquet\"\n",
    "    model = spark.read.parquet(f\"{team_BASE_DIR}/interim/{fn_model}\")\n",
    "\n",
    "    joined_df = df.join(model, \n",
    "                     (df[\"ORIGIN\"] == model[\"ORIGIN\"]) & \n",
    "                     (df[\"DAY_OF_WEEK\"] == model[\"dow\"]) & \n",
    "                     (df[\"dep_hour_utc\"] == model[\"hour\"]),\n",
    "                     how=\"left\").drop(model[\"ORIGIN\"])\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "# display(get_seasonality_data(df_train.limit(10), 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4aa8a8-0eee-46a9-9655-8ef389670e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(get_seasonality_data(df_test.limit(10), 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53913682-beee-47d2-9f66-20151ae8913b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if compute_seasonality:\n",
    "    # train seasonality model for each cross validation split\n",
    "    for i in range(k):\n",
    "        # train seasonality model for this cross validation split\n",
    "        model = get_seasonality(df_train, cv_cutoffs[\"train_min\"][i], cv_cutoffs[\"train_max\"][i])\n",
    "        # write out\n",
    "        fn_out = f\"seasonality_model_{period}_cv{i}of{k}.parquet\"\n",
    "        model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")\n",
    "\n",
    "    # train seasonality model for full training data\n",
    "    model = get_seasonality(df_train, datetime(1970,1,1), min_test_dt)\n",
    "    # write out\n",
    "    fn_out = f\"seasonality_model_{period}_train.parquet\"\n",
    "    model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1725d5c-8184-419c-971d-0bf90b8cff30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# lag sandbox - ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "485233fa-d1fb-497c-8412-de2899166450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WindowConditions = Window.partitionBy(\"TAIL_NUM\").orderBy(\"sched_depart_utc\")\n",
    "\n",
    "WhenConditions = (f.col(\"ORIGIN\") == f.col(\"priorflight_dest\")) & (f.col(\"priorflight_sched_deptime\") >= f.col(\"twentysix_hours_prior_depart_UTC\"))\n",
    "\n",
    "\n",
    "def add_lags(df):\n",
    "\n",
    "    result_df = (df\n",
    "                 .withColumn(\"priorflight_origin\",\n",
    "                             lag(\"ORIGIN\").over(WindowConditions))\n",
    "                 .withColumn(\"priorflight_dest\",\n",
    "                             lag(\"DEST\").over(WindowConditions))\n",
    "                 .withColumn('priorflight_cancelled_true',  #~~~~true cancellation status, assumed known (?)\n",
    "                             lag('CANCELLED').over(WindowConditions))\n",
    "                 .withColumn(\"twentysix_hours_prior_depart_UTC\",\n",
    "                             (f.col(\"two_hours_prior_depart_UTC\") - f.expr(\"INTERVAL 24 HOURS\")).cast(\"timestamp\"))\n",
    "                             \n",
    "                 \n",
    "                 .withColumn(\"priorflight_sched_deptime\",\n",
    "                             f.when(WhenConditions, lag(\"sched_depart_utc\").over(WindowConditions))\n",
    "                             .otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_elapsed_time_calc\", #~~~crs estimated\n",
    "                             lag(\"CRS_ELAPSED_TIME\").over(WindowConditions))\n",
    "                    .withColumn(\"priorflight_elapsed_time_calc\", #~~~turned into interval\n",
    "                                f.when(WhenConditions,f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_elapsed_time_calc\"))\n",
    "                                .otherwise(None)\n",
    "                    )\n",
    "\n",
    "                 .withColumn(\"priorflight_depdelay_true\", #~~~true dep delay\n",
    "                             f.when(WhenConditions,lag(\"DEP_DELAY\").over(WindowConditions))\n",
    "                             .otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "\n",
    "                 .withColumn(\"special_cases\", f.when(WhenConditions, 1).otherwise(f.lit(0.0)))\n",
    "\n",
    "                 .withColumn(\"priorflight_deptime_true\", #~~~true dep time based on true dep delay\n",
    "                     f.when(WhenConditions, (f.col(\"priorflight_sched_deptime\") + \n",
    "                     (f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_depdelay_true\"))))\n",
    "                     .otherwise(None)\n",
    "                 )\n",
    "                 \n",
    "                 \n",
    "                 .withColumn(\"priorflight_isdeparted\", #~~~ only 1 when we definitely knew it left already\n",
    "                             f.when((f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")) \n",
    "                                    & WhenConditions, 1).otherwise(0) #we don't really know about the prior flight for the when conditions\n",
    "                                 ) \n",
    "                 .withColumn(\"priorflight_depdelay_calc\", #~~~estimated dep delay\n",
    "                        f.when(\n",
    "                            # Case 1: Flight departed BEFORE observation window ***** add in when conditions\n",
    "                            ((f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")) & WhenConditions),\n",
    "                            f.col(\"priorflight_depdelay_true\")  # Full delay known\n",
    "                        ).when(\n",
    "                            # Case 2: Flight scheduled to depart BEFORE window, but departed DURING observation window\n",
    "                            (f.col(\"priorflight_sched_deptime\") <= f.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                            (f.col(\"priorflight_deptime_true\") > f.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                            WhenConditions,\n",
    "                            (f.col(\"two_hours_prior_depart_UTC\").cast('long') - f.col(\"priorflight_sched_deptime\").cast('long')) / 60  # Partial delay\n",
    "                        ).otherwise(\n",
    "                            f.lit(0.0)  # Flight scheduled to depart AFTER window; we know nothing and assume departed on time\n",
    "                        )\n",
    "                    )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_deptime_calc\", #~~~estimated dep time based on estimated dep delay\n",
    "                                f.col(\"priorflight_sched_deptime\") + \n",
    "                                (f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_depdelay_calc\")) \n",
    "                            )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_isdelayed_calc\", #~~~estimated delay indicator ** ADJUSTED FOR CANCELLED\n",
    "                             f.when( ((f.col(\"priorflight_depdelay_calc\") >= 15) | (f.col('priorflight_cancelled_true') == 1)), 1).otherwise(0)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"elapsed_time_true\", #~~~true elapsed time for current flight\n",
    "                             f.when(WhenConditions,(f.col(\"AIR_TIME\") + f.col(\"TAXI_IN\") + f.col(\"TAXI_OUT\")\n",
    "                             ).cast(\"int\")).otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"arr_time_true\", #~~~true arrival time based on true dp time + true elased time\n",
    "\n",
    "                                f.col(\"arr_datetime\").cast(\"timestamp\") +\n",
    "                                (f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"ARR_DELAY\"))\n",
    "                        )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_arr_time_true\", #~~~true prior flight arrival time\n",
    "                             lag(\"arr_time_true\").over(WindowConditions)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_isarrived_calc\", #~~~estimated arrival indicator based on whether flight landed before window\n",
    "                             f.when((f.col(\"priorflight_arr_time_true\") <= f.col(\"two_hours_prior_depart_UTC\")) & WhenConditions,1).otherwise(0)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_arr_time_calc\", #~~~estimated arrival time based on 3 scenarios\n",
    "                        f.when(\n",
    "                            f.col(\"priorflight_isarrived_calc\") == 1,  # Case 1: Dep before window, arr after window\n",
    "                            f.col(\"priorflight_arr_time_true\") #so we know the info\n",
    "                        ).when(\n",
    "                            (f.col(\"priorflight_isarrived_calc\") == 0) &  # Case 2: Dep before window, arr after window\n",
    "                            (f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")), \n",
    "                            f.col(\"priorflight_deptime_true\") + f.col(\"priorflight_elapsed_time_calc\")\n",
    "                        ).otherwise(\n",
    "                            f.col(\"priorflight_deptime_calc\") + f.col(\"priorflight_elapsed_time_calc\")  # dep after window, arr after window\n",
    "                        ))\n",
    "                 \n",
    "                 .withColumn(\"turnaround_time_calc\", \n",
    "                             #~~~estimated how much time we have between estimated arrival of prior flight and scheduled departure of current flight\n",
    "                    (f.when(WhenConditions,\n",
    "                        ((f.col(\"sched_depart_utc\").cast(\"long\") - \n",
    "                        f.col(\"priorflight_arr_time_calc\").cast(\"long\")) / 60\n",
    "                    ).cast(\"double\"))\n",
    "                    ).otherwise(None))\n",
    "    )\n",
    "\n",
    "\n",
    "    #fill in edge case values\n",
    "            #~~ 1. if prior flight is cancelled and there was nothing to pull,\n",
    "            #~~ 2. if prior flight dest != current flight origin, \n",
    "            #~~ 3. if it has been >26 hrs since last flight,\n",
    "        # impute:\n",
    "            #turnaround time\n",
    "            #prior flight delay estimation\n",
    "    window2 =  Window.partitionBy(\"ORIGIN\",\"DEST\").orderBy(\"sched_depart_utc\").rowsBetween(-10, -1)\n",
    "    \n",
    "\n",
    "    result_df = (result_df\n",
    "                .withColumn(\"turnaround_time_calc\", \n",
    "                            f.when(((~WhenConditions) | (f.col('priorflight_cancelled_true') == 1)),\n",
    "                            last(f.col(\"turnaround_time_calc\"), ignorenulls=True)\n",
    "                            .over(window2)\n",
    "                                ).otherwise(f.col(\"turnaround_time_calc\"))\n",
    "                )\n",
    "                .withColumn(\"priorflight_depdelay_calc\", \n",
    "                            f.when(((~WhenConditions) | (f.col('priorflight_cancelled_true') == 1)),\n",
    "                            last(f.col(\"priorflight_depdelay_calc\"), ignorenulls=True)\n",
    "                            .over(window2)\n",
    "                                ).otherwise(f.col(\"priorflight_depdelay_calc\"))\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "full = add_lags(out) #first pass to correctly get the current cancelled flights\n",
    "noncancelled=add_lags(out.filter(f.col(\"CANCELLED\") == 0)) #next pass to correctly skip prior cancelled flights for current non cancelled flights\n",
    "result=full.filter(f.col('CANCELLED')==1).unionByName(noncancelled) #combine the two \n",
    "\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "855f68ad-581b-4fd9-88a6-1201d6914481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "redundant=[ 'priorflight_origin',\n",
    " 'priorflight_dest',\n",
    " 'twentysix_hours_prior_depart_UTC',\n",
    " 'priorflight_sched_deptime',\n",
    " 'priorflight_elapsed_time_calc',\n",
    " 'priorflight_depdelay_true',\n",
    " 'priorflight_deptime_true',\n",
    " 'priorflight_depdelay_calc',\n",
    " 'priorflight_deptime_calc',\n",
    " 'priorflight_isdelayed_calc',\n",
    " 'elapsed_time_true',\n",
    " 'arr_time_true',\n",
    " 'priorflight_arr_time_true',\n",
    " 'priorflight_isarrived_calc',\n",
    " 'priorflight_arr_time_calc',\n",
    " 'turnaround_time_calc',\n",
    "]\n",
    "\n",
    "out=out.select([c for c in out.columns if c not in redundant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11f42ba-49f7-4ec2-8274-841fa4160120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = out.withColumn('arr_datetime', F.col('arr_datetime').cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1bd9b0-ce61-4086-91ed-ac78e2a0b6ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "348d9085-aa19-4cc5-8ba6-596c61f968d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')=='259NV').filter(f.col('sched_depart_utc').contains('2019-01-02')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "06094348-b65b-460d-b644-91f156c58271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col(\"ORIGIN\") != f.col(\"priorflight_dest\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b36c047-655a-46da-9713-2e58fcb46ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "769b23b3-a792-4cec-bafd-aa50d73574c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WindowConditions = Window.partitionBy(\"TAIL_NUM\").orderBy(\"sched_depart_utc\")\n",
    "\n",
    "display(result.withColumn(\"prior_cancelled\", \n",
    "                  lag(\"CANCELLED\").over(WindowConditions)).filter(f.col('prior_cancelled')==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32b7d21c-cb9d-41c2-8c5b-4877723380ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "impute missing turnaroudns based on current origin -> destination ema something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "37c6e822-8f5f-4406-bc4b-f6b70c18af15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')==\"N102UW\").filter(f.col('sched_depart_utc').contains('2019-04-09')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d214d82-6d5d-4cb5-a113-b4b68fdd73ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.filter(f.col('turnaround_time_calc').isNull()).groupBy('special_cases','priorflight_cancelled_true').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f4ed156-f793-4fe2-8da5-b8635cddc10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.filter(f.col('turnaround_time_calc').isNull()).groupBy('special_cases','priorflight_cancelled_true').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec1f6be-47d2-4f09-93a6-df09dd84ad19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef10162-3d7c-4ac7-9c77-94ce14d4a892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')=='215NV').orderBy('sched_depart_utc')\n",
    "        .select('sched_depart_utc','ORIGIN','DEST','priorflight_origin','priorflight_dest',\n",
    "                'CANCELLED','priorflight_cancelled_true','priorflight_isarrived_calc','priorflight_deptime_true',\n",
    "                'arr_time_true','priorflight_arr_time_true','priorflight_deptime_calc','priorflight_arr_time_calc','priorflight_elapsed_time_calc','turnaround_time_calc','special_cases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607bb23e-ff35-4538-9007-e58afeb26f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('turnaround_time_calc').isNull()).filter(f.col('priorflight_cancelled_true')==0).filter(f.col('CANCELLED')==0).orderBy('TAIL_NUM','sched_depart_utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c3d5e4-6148-4598-b339-3e5df06c3401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')==\"N102UW\").filter(f.col('sched_depart_utc') >= '2019-03-20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f6af8b-7a02-4e60-8763-219846ca6ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')==\"N102UW\").filter(f.col('sched_depart_utc') >= '2019-03-20'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfd15eb4-94d3-47c7-ac89-4434d79348b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "to clean:\n",
    "prior flight is cancelled, current flight is cancelled: impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed2b80b4-fa4f-4c97-83f8-050ad2a7bec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7531ecba-1046-42c4-8255-527804ad8767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. A good, B good, C good\n",
    "2. A good, B cancelled, C good\n",
    "3. A cancelled, B cancelled, C good **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71589e33-2b40-4281-9b7c-58208f394431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c25e1a-9c7c-496f-b3b1-73b9f2b0c8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# prior_noncancelled=add_lags(full.filter(f.col(\"priorflight_cancelled_true\") == 0))\n",
    "# result2=full.filter(f.col('priorflight_cancelled_true')==1).unionByName(prior_noncancelled)\n",
    "#doesnt resolve issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee8dc9de-8b91-4f63-82a2-33bc67044d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed797ad-5388-450c-8c14-623a6c6f72d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.filter(f.col('DEP_DELAY').isNull()).groupBy('CANCELLED').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "955a167e-fe31-4e03-a092-9403f663315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "null_counts = result.select(\n",
    "    [f.count(f.when(f.col(c).cast(\"long\").isNull() | f.isnan(f.col(c).cast(\"long\")), c)).alias(c) \n",
    "     for c in result.columns]\n",
    ")\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e661571-df29-46b9-88b0-697aefd80db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('priorflight_arr_time_calc').isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2989a01-5f51-4b25-89ac-8a5b001ebfab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(f.col('TAIL_NUM').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc5635c-67b3-4783-9c2d-3e3f23df9ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "output_path = \"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined_1y_weather_cleaned_combo_lags.parquet\"\n",
    "(\n",
    "    result.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour variable (needed for seasonality)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b48f564-6cf2-41bb-9b00-d48f11b5da1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.12-sg-joined-feateng-modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
