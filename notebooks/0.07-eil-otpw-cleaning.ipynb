{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242b11e6-dc13-4cb9-878a-09819a738b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Flights data cleaning\n",
    "Erica Landreth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710c981e-0d20-4191-a292-6ad2af4e6bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Restart the Python kernel\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5938cb3c-204b-434b-ad96-65ceac820c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f26655-f249-48f3-a3e3-cf69842e276f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtering to relevant rows/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de4b43c-303f-489a-aa3a-3847e0cd78ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbf4380-cc98-4c56-98ca-f2d76f48f8d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"dbfs:/mnt/mids-w261/OTPW_12M/OTPW_12M/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224029e0-3bde-4598-8895-8463b38c5e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load flights data\n",
    "\n",
    "# dataset = 'parquet_airlines_data_3m' # 3 months\n",
    "# dataset = 'parquet_airlines_data_1y' # 1 year\n",
    "# dataset = \"OTPW_3M_2015\"\n",
    "dataset = \"OTPW_12M_2015\"\n",
    "\n",
    "# df_flights = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"dbfs:/mnt/mids-w261/{dataset}.csv\")\n",
    "df_flights = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"dbfs:/mnt/mids-w261/OTPW_12M/OTPW_12M/{dataset}.csv.gz\").cache()\n",
    "shape_orig = (df_flights.count(), len(df_flights.dtypes))\n",
    "display(df_flights)\n",
    "print(f\"Original shape: {shape_orig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d74631-9a9d-4443-b020-576c94723719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define columns to drop\n",
    "# columns related to diversion: not enough data to use the diversion info\n",
    "div_cols = [col for col in df_flights.columns if col.startswith('DIV') and col != \"DIVERTED\"]\n",
    "# redundant carrier ID's (EDA indicated that OP_UNIQUE_CARRIER is sufficient)\n",
    "xtra_carrier_cols = [\"OP_CARRIER_AIRLINE_ID\",\"OP_CARRIER\"]\n",
    "# redundant airport ID's (EDA indicated that ORIGIN/DEST and *_AIRPORT_SEQ_ID are sufficient)\n",
    "xtra_airport_cols = [ \\\n",
    "  \"ORIGIN_AIRPORT_ID\",\"ORIGIN_CITY_MARKET_ID\",\"ORIGIN_STATE_ABR\",\"ORIGIN_STATE_NM\",\"ORIGIN_WAC\", \\\n",
    "  \"DEST_AIRPORT_ID\",\"DEST_CITY_MARKET_ID\",\"DEST_STATE_ABR\",\"DEST_STATE_NM\",\"DEST_WAC\"]\n",
    "# redundant flight info (could be recreated if need be)\n",
    "xtra_flight_cols = [\"WHEELS_OFF\",\"WHEELS_ON\",\"FLIGHTS\",\"ACTUAL_ELAPSED_TIME\",\"DISTANCE_GROUP\"]\n",
    "# redundant delay status info (could be recreated if need be)\n",
    "xtra_time_cols = [\"DEP_TIME\",\"DEP_DELAY_NEW\",\"DEP_DEL15\",\"DEP_DELAY_GROUP\",\"ARR_TIME\",\"ARR_DELAY_NEW\",\"ARR_DEL15\",\"ARR_DELAY_GROUP\"]\n",
    "\n",
    "## fields to keep\n",
    "# core features: useful for ML features and/or feature engineering\n",
    "core_feats = [\"FL_DATE\",\"OP_UNIQUE_CARRIER\",\"TAIL_NUM\",\"OP_CARRIER_FL_NUM\",\"ORIGIN\",\"DEST\",\"CRS_DEP_TIME\",\"DEP_DELAY\",\"CRS_ARR_TIME\",\"ARR_DELAY\",\"CANCELLED\",\"DIVERTED\",\"CRS_ELAPSED_TIME\",\"AIR_TIME\",\"DISTANCE\"]\n",
    "# we may or may not end up using these, but they can't easily be recreated later, so we'll keep them to be cautious\n",
    "on_the_fence = [\"ORIGIN_AIRPORT_SEQ_ID\",\"DEST_AIRPORT_SEQ_ID\",\"TAXI_OUT\",\"TAXI_IN\"]\n",
    "# useful for time series analysis\n",
    "time_series = [\"QUARTER\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"DEP_TIME_BLK\",\"ARR_TIME_BLK\",\"YEAR\"]\n",
    "# useful to sanity check that joins are successful\n",
    "sanity_check = [\"ORIGIN_CITY_NAME\",\"DEST_CITY_NAME\",\"ORIGIN_STATE_FIPS\",\"DEST_STATE_FIPS\"]\n",
    "# provides reasoning for cancellations, delays, and returns to gate\n",
    "delay_info = [col for col in df_flights.columns if col.endswith(\"_DELAY\") and col not in core_feats] + [\"CANCELLATION_CODE\"] + [\"FIRST_DEP_TIME\",\"LONGEST_ADD_GTIME\",\"TOTAL_ADD_GTIME\"]\n",
    "    # Note: cancellation codes are: \"A\" for carrier-caused, \"B\" for weather, \"C\" for National Aviation System, and \"D\" for security\n",
    "\n",
    "all_cols = div_cols+xtra_carrier_cols+xtra_airport_cols+xtra_flight_cols+xtra_time_cols+core_feats+on_the_fence+time_series+sanity_check+delay_info\n",
    "\n",
    "missing = [col for col in df_flights.columns if col not in all_cols]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e308a2-fe19-429e-9b23-4c8f8a4a8143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define columns to keep\n",
    "keep_me = core_feats + on_the_fence + time_series + sanity_check + delay_info\n",
    "keep_me += [c for c in df_flights.columns if \"Hourly\" in c]\n",
    "keep_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab2acb5-8ff8-4b89-a366-66531255d7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "keep_me += ['REM', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'STATION', 'Sunrise', 'Sunset'] #shruti wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6d0355-0b64-4909-9964-a1cd248670be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights = df_flights.select(keep_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99af8fa5-07f6-45e6-a61f-b7ce5fde6800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter to columns of interest, and de-dupe\n",
    "# df_flights = df_flights.select(keep_me).distinct()\n",
    "\n",
    "# manual!!! in prev run, saw that the distinct did not do anything\n",
    "df_flights = df_flights.select(keep_me).cache()\n",
    "\n",
    "shape_filt = (df_flights.count(), len(df_flights.dtypes))\n",
    "display(df_flights)\n",
    "print(f\"Filtered shape: {shape_filt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f2c94c-10f2-4433-9938-2fa2d3347639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # sanity check: we expect half the records after de-dupe\n",
    "# shape_orig[0]/shape_filt[0] == 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c7f2cb-4085-442a-98ae-271e8211e05d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # filter to those rows with outcome info populated\n",
    "# df_flights = df_flights.filter( (col(\"DEP_DELAY\").isNotNull()) | (col(\"CANCELLED\") == 1) | (col(\"DIVERTED\") == 1) ).cache()\n",
    "# shape_outcome = (df_flights.count(), len(df_flights.dtypes))\n",
    "# print(f\"Shape after filtering for populated outcome vars: {shape_outcome}\")\n",
    "\n",
    "# manual !!! saw that above block did not filter anything in previous run\n",
    "\n",
    "# filter to those with populated info for basic flight metadata\n",
    "df_flights = df_flights.filter( (col(\"OP_UNIQUE_CARRIER\").isNotNull()) & (col(\"ORIGIN\").isNotNull()) & (col(\"DEST\").isNotNull()) & (col(\"FL_DATE\").isNotNull()) & (col(\"CRS_DEP_TIME\").isNotNull()) & (col(\"CRS_ARR_TIME\").isNotNull()) ).cache()\n",
    "shape_basic = (df_flights.count(), len(df_flights.dtypes))\n",
    "print(f\"Shape after filtering for basic feature info: {shape_basic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183da413-8777-4f63-befa-15782664a382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Converting flight data to UTC time zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c1dd8dd-5afe-40ae-b954-fd1fe335c41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create time zone reference file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2395a85-0408-4bb5-98b4-a19272e85cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The below code was used to look up the time zone for each airport. The resulting time zone info was saved out to parquet, so from this point on, just load the time zone parquet (see below for path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba380239-d0bb-42ff-b393-d56461f65dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install timezonefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873c32a9-8b38-468f-b472-298ec8e25e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # imports\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "# from timezonefinder import TimezoneFinder\n",
    "# import pytz\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # load stations data\n",
    "# df_stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/\")\n",
    "\n",
    "# # get unique airport info from stations table\n",
    "# df_locs = df_stations.select('neighbor_call','neighbor_lat','neighbor_lon').distinct()\n",
    "# display(df_locs)\n",
    "\n",
    "# # define function to look up time zones\n",
    "# def find_timezone(lat, lng):\n",
    "#     tf = TimezoneFinder()\n",
    "#     timezone_str = tf.timezone_at(lat=lat, lng=lng)\n",
    "#     return timezone_str if timezone_str else \"Unknown\"\n",
    "\n",
    "# # define udf for time zone lookup\n",
    "# find_timezone_udf = udf(find_timezone, StringType())\n",
    "\n",
    "# # add time zone column\n",
    "# df_locs = df_locs.withColumn(\"timezone\", find_timezone_udf(col(\"neighbor_lat\"), col(\"neighbor_lon\")))\n",
    "\n",
    "# # save df_time zone info as a parquet file\n",
    "# folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "# df_locs.write.parquet(f\"{folder_path}/external/tz_lookup.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc8069e-2444-4bc6-b328-ded42b824aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "\n",
    "# df_tz = spark.read.parquet(f\"dbfs:/student-groups/Group_4_1/external/tz_lookup.parquet\")\n",
    "\n",
    "# # manually fill in missing time zone info\n",
    "# # note: neighbor_call is ICAO\n",
    "# BBG = Row(neighbor_call=\"BBG\", neighbor_lat=36.53856729627892, neighbor_lon=-93.19908127077512, timezone=\"America/Chicago\")\n",
    "# KOGS = Row(neighbor_call=\"KOGS\", neighbor_lat=44.6820707679313, neighbor_lon=-75.47692203483886, timezone=\"America/New_York\")\n",
    "# NSTU = Row(neighbor_call=\"NSTU\", neighbor_lat=-14.329024376251269, neighbor_lon=-170.71329690482548, timezone=\"Pacific/Pago_Pago\")\n",
    "# PGSN = Row(neighbor_call=\"PGSN\", neighbor_lat=15.11974288544001, neighbor_lon=145.7282788950688, timezone=\"Pacific/Saipan\")\n",
    "# PGUM = Row(neighbor_call=\"PGUM\", neighbor_lat=13.48562402083883, neighbor_lon=144.8001485238768, timezone=\"Pacific/Guam\")\n",
    "# TJPS = Row(neighbor_call=\"TJPS\", neighbor_lat=18.01055087987774, neighbor_lon=-66.56323216254391, timezone=\"America/Puerto_Rico\")\n",
    "# TJSJ = Row(neighbor_call=\"TJSJ\", neighbor_lat=18.457160454103658, neighbor_lon=-66.0974759565605, timezone=\"America/Puerto_Rico\")\n",
    "# US_0571 = Row(neighbor_call=\"US-0571\", neighbor_lat=48.25780621107438, neighbor_lon=-103.74169879360201, timezone=\"America/Chicago\")\n",
    "\n",
    "# man_df = spark.createDataFrame([BBG,KOGS,NSTU,PGSN,PGUM,TJPS,TJSJ,US_0571])\n",
    "# df_tz = df_tz.union(man_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4ef2e2-6308-4388-a368-99bdaa9ffea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "# df_tz.write.mode('overwrite').parquet(f\"{folder_path}/external/tz_lookup_manually_adjusted.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feaab843-45ca-428c-a572-5ef537dab2b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apply time zones to create full cleaned flights table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99852409-c3a2-471b-8d3b-590f454cb677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airports = spark.read.option(\"header\",\"true\").csv(f\"dbfs:/mnt/mids-w261/airport-codes_csv.csv\")\n",
    "df_tz = spark.read.parquet(f\"dbfs:/student-groups/Group_4_1/external/tz_lookup_manually_adjusted.parquet\")\n",
    "\n",
    "# start with a very, very small flight data sample\n",
    "tmp_flights = df_flights.limit(10)\n",
    "\n",
    "# create temporary views\n",
    "df_flights.createOrReplaceTempView(\"df_flights\")\n",
    "df_airports.createOrReplaceTempView(\"df_airports\")\n",
    "df_tz.createOrReplaceTempView(\"df_tz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528a1ef3-e063-4c56-a7c9-12319baae167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define all columns of the flights table (for use in SELECT statement)\n",
    "flights_cols = \"x.\" + \", x.\".join(df_flights.columns)\n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "WITH origin AS(\n",
    "SELECT  {flights_cols},\n",
    "        x.FL_DATE as date,\n",
    "        x.CRS_DEP_TIME as dep_time,\n",
    "        x.CRS_ARR_TIME as arr_time,\n",
    "        x.ORIGIN as origin_iata,\n",
    "        x.DEST as dest_iata,\n",
    "        a.ident as origin_icao\n",
    "FROM df_flights as x\n",
    "LEFT JOIN df_airports as a on x.ORIGIN = a.iata_code),\n",
    "\n",
    "origin_dest AS(\n",
    "SELECT  {flights_cols},\n",
    "        x.date,\n",
    "        x.dep_time,\n",
    "        x.arr_time,\n",
    "        x.origin_iata,\n",
    "        x.dest_iata,\n",
    "        x.origin_icao,\n",
    "        a.ident as dest_icao\n",
    "FROM origin as x\n",
    "LEFT JOIN df_airports as a on x.dest_iata = a.iata_code),\n",
    "\n",
    "origin_dest_tz1 AS(\n",
    "SELECT  {flights_cols},\n",
    "        x.date,\n",
    "        x.dep_time,\n",
    "        x.arr_time,\n",
    "        x.origin_iata,\n",
    "        tz.timezone as origin_tz,\n",
    "        x.dest_iata,\n",
    "        x.origin_icao,\n",
    "        x.dest_icao\n",
    "FROM origin_dest as x\n",
    "LEFT JOIN df_tz as tz on x.origin_icao = tz.neighbor_call\n",
    "),\n",
    "\n",
    "origin_dest_tz2 AS(\n",
    "SELECT  {flights_cols},\n",
    "        x.date,\n",
    "        x.dep_time,\n",
    "        x.arr_time,\n",
    "        x.origin_iata,\n",
    "        x.origin_tz,\n",
    "        x.dest_iata,\n",
    "        tz.timezone as dest_tz,\n",
    "        x.origin_icao,\n",
    "        x.dest_icao\n",
    "FROM origin_dest_tz1 as x\n",
    "LEFT JOIN df_tz as tz on x.dest_icao = tz.neighbor_call\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM origin_dest_tz2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "out = spark.sql(query).cache()\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61cb9a2-0b30-48c2-b269-7192547857bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shape_tz = (out.count(), len(out.dtypes))\n",
    "print(f\"Shape after adding time zone info: {shape_tz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9854d41-791c-497a-a545-ed5b4af402de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# double check that all the time zones were successfully populated\n",
    "tmp = out.filter( (col(\"origin_tz\").isNull()) | (col(\"dest_tz\").isNull()) )\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3aa9d8-6f42-479a-93ae-587d56a4ad57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_utc(yyyymmdd, dep_hhmm, arr_hhmm, dep_tz, arr_tz, flight_dur):\n",
    "    \"\"\"\n",
    "    Create UTC timestamp from flights table columns\n",
    "    yyyymmdd = FL_DATE\n",
    "    dep_hhmm = CRS_DEP_TIME\n",
    "    arr_hhmm = CRS_ARR_TIME\n",
    "    dep_tz = time zone from time zone table\n",
    "    arr_tz = time zone from time zone table\n",
    "    flight_dur = CRS_ELAPSED_TIME (for sanity check of arrival time)\n",
    "\n",
    "    Returns UTC time stamp, (cast to string)\n",
    "    \"\"\"\n",
    "\n",
    "    dep_hhmm = int(dep_hhmm)\n",
    "    arr_hhmm = int(arr_hhmm)\n",
    "\n",
    "    yyyy,MM,dd = yyyymmdd.split('-')\n",
    "    yyyy = int(yyyy) # get year\n",
    "    MM = int(MM) # get month\n",
    "    dd = int(dd) # get day\n",
    "\n",
    "    dep_hh = dep_hhmm//100 # get hour\n",
    "    dep_mm = dep_hhmm%100 # get minute\n",
    "    if dep_hh == 24:\n",
    "        dep_hh = 0\n",
    "        dep_shift = True\n",
    "    else:\n",
    "        dep_shift = False\n",
    "\n",
    "    arr_hh = arr_hhmm//100 # get hour\n",
    "    arr_mm = arr_hhmm%100\n",
    "    if arr_hh == 24:\n",
    "        arr_hh = 0\n",
    "        arr_shift = True\n",
    "    else:\n",
    "        arr_shift = False\n",
    "\n",
    "    # create datetime variable for departure\n",
    "    dt_dep = datetime(yyyy,MM,dd,dep_hh,dep_mm)\n",
    "    if dep_shift:\n",
    "        dt_dep += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    dep_local = pytz.timezone(dep_tz).localize(dt_dep)\n",
    "    # convert to UTC\n",
    "    dep_utc = dep_local.astimezone(pytz.utc)\n",
    "\n",
    "    # create datetime variable for arrival\n",
    "    dt_arr = datetime(yyyy,MM,dd,arr_hh,arr_mm)\n",
    "    if arr_shift:\n",
    "        dt_arr += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    arr_local = pytz.timezone(arr_tz).localize(dt_arr)\n",
    "    # convert to UTC\n",
    "    arr_utc = arr_local.astimezone(pytz.utc)\n",
    "\n",
    "    if dep_utc > arr_utc:\n",
    "        arr_utc += timedelta(days=1)\n",
    "\n",
    "    # # sanity check\n",
    "    # arr_utc_SC = dep_utc + timedelta(minutes=flight_dur)\n",
    "\n",
    "    dt_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "    # return UTC datetime, cast to string\n",
    "    # return (dep_utc.strftime(dt_format), arr_utc.strftime(dt_format), arr_utc_SC.strftime(dt_format))\n",
    "    return (dep_utc.strftime(dt_format), arr_utc.strftime(dt_format))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"dep_datetime\", StringType(), False),\n",
    "    StructField(\"arr_datetime\", StringType(), False),\n",
    "    # StructField(\"arr_datetime_SANITYCHECK\", StringType(), False)\n",
    "])\n",
    "\n",
    "dt_udf = udf(to_utc, schema)\n",
    "\n",
    "out = out.withColumn('processed', dt_udf(col(\"date\"), col(\"dep_time\"), col(\"arr_time\"), col(\"origin_tz\"), col(\"dest_tz\"), col(\"CRS_ELAPSED_TIME\"))).cache()\n",
    "\n",
    "cols = [c for c in out.columns if c != \"processed\"]\n",
    "# cols += [\"processed.dep_datetime\",\"processed.arr_datetime\",\"processed.arr_datetime_SANITYCHECK\"]\n",
    "cols += [\"processed.dep_datetime\",\"processed.arr_datetime\"]\n",
    "out = out.select(cols).cache()\n",
    "\n",
    "# out = out.withColumn(\"dep_timestamp\", F.to_timestamp(col('dep_datetime')).alias('dep_timestamp'))\n",
    "# out = out.withColumn(\"arr_timestamp\", F.to_timestamp(col('arr_datetime')).alias('arr_timestamp'))\n",
    "# out = out.withColumn(\"arr_timestamp_SANITYCHECK\", F.to_timestamp(col('arr_datetime_SANITYCHECK')).alias('arr_timestamp_SANITYCHECK'))\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2cc3ac-74db-4eaf-8456-ca94a58488ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove redundant variables that were added during the join process\n",
    "redundant = [\"date\",\"dep_time\",\"arr_time\"]\n",
    "# note ORIGIN and DEST are technically redudant, but will keep for now\n",
    "\n",
    "out = out.drop(*redundant).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d073ad94-621d-4b61-9fc0-0d67faee939f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final shape\n",
    "shape_final = (out.count(), len(out.dtypes))\n",
    "print(f\"Shape after cleaning: {shape_final}\")\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f01d93-7f93-475f-9600-4ded1481e4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write cleaned output to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb24b310-6b39-41d6-bcdf-bf70bd96abf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = \"OTPW_12M_2015\"\n",
    "folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "out.write.mode(\"overwrite\").parquet(f\"{folder_path}/interim/{dataset}_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076e14b2-5551-40ec-b1e9-0f92c9de492e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check that write was successful\n",
    "df = spark.read.parquet(f\"{folder_path}/interim/{dataset}_clean.parquet\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d68a54-2068-4945-8232-f92e8111386c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{folder_path}/interim/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14455cfd-d774-4e75-a23d-72ebd24a374d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Weather Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf84b67f-7d72-4ecb-a6c8-ae322925baba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Double checks nulls/duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201886c3-f362-4e3e-8c35-df21939ee578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = \"OTPW_12M_2015\"\n",
    "df_flights = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"dbfs:/mnt/mids-w261/OTPW_12M/OTPW_12M/{dataset}.csv.gz\")\n",
    "display(df_flights.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaae457-1738-421e-a95d-a58ea222ea0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5148062-5d68-44c2-8538-54e2e1960f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(out\n",
    "        .filter(F.col('CANCELLED')==0)\n",
    "        .groupBy('TAIL_NUM','dep_datetime')\n",
    "        .count()\n",
    "        .filter(F.col('count') > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717b4b40-6a66-42bd-abe7-e4063f938d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(out\n",
    "        .filter(F.col('CANCELLED')==0)\n",
    "        .filter((F.col('DEP_DELAY') == 0) | F.col('DEP_DELAY').isNull())\n",
    "        .groupBy('TAIL_NUM','dep_datetime')\n",
    "        .count()\n",
    "        .filter(F.col('count') > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30047939-4e14-454f-a746-b816a7f09586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "N812SK\t2015-04-07T17:15:00\t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9df060f-ee1c-4063-935e-491037ecfd27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(out.filter(F.col('TAIL_NUM')=='N812SK').filter(F.col('FL_DATE')=='2015-04-07'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6cc5cd-e107-47fc-8cf6-09876e55779f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(out.filter(F.col('TAIL_NUM')=='N928WN').filter(F.col('FL_DATE')=='2015-03-28'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d041c97b-243b-4e63-b7e2-45dede6e58d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Duplicates based on tail number/datetime always occur due to cancellations or due to delays which cause replicated looking datetimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f935c8b2-ee72-42ba-8f44-13d9cc3b3a48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ade5ca2-68f5-4c9f-8f55-947ff76cc4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### METAR (Whole df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9430f540-aef9-410a-90d1-ef0597fc4d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = \"OTPW_12M_2015\"\n",
    "folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "df = spark.read.parquet(f\"{folder_path}/interim/{dataset}_clean.parquet\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "499bdafa-95b6-4724-b0a6-532b9226fc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pull wind data from REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d9d3c61-3ae9-4b0a-9b7f-9dc784c8498b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64beec96-fd7f-4d14-9446-6bd50f755941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_interpolate = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        \"HourlyWindSpeed\",\n",
    "        F.when(\n",
    "            F.col(\"HourlyWindSpeed\").isNull(),\n",
    "            # Extract sustained wind speed from METAR groups\n",
    "            F.regexp_extract(\n",
    "                F.col(\"REM\"),\n",
    "                r'\\b(\\d{3})(\\d{2,3})(?:G(\\d{2,3}))?KT\\b',  # Regex pattern\n",
    "                2  # Capture group for sustained wind speed\n",
    "            ).cast(\"int\")\n",
    "        ).otherwise(F.col(\"HourlyWindSpeed\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"HourlyWindGustSpeed\",\n",
    "        F.when(\n",
    "            F.col(\"HourlyWindGustSpeed\").isNull(),\n",
    "            F.greatest(\n",
    "                # Regular wind gust (G group)\n",
    "                F.regexp_extract(\n",
    "                    F.col(\"REM\"),\n",
    "                    r'\\b(\\d{3})(\\d{2,3})(?:G(\\d{2,3}))?KT\\b',\n",
    "                    3\n",
    "                ).cast(\"int\"),\n",
    "                # Peak wind gust (PK WND group)\n",
    "                F.regexp_extract(\n",
    "                    F.col(\"REM\"),\n",
    "                    r'PK WND (\\d{3})(\\d{2,3})/(\\d{4})',  # PK WND pattern\n",
    "                    2  # Capture group for peak wind speed\n",
    "                ).cast(\"int\")\n",
    "            )\n",
    "        ).otherwise(F.col(\"HourlyWindGustSpeed\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21ed9a6-ed50-46f6-83a5-f1f629bb247b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_interpolate = (df_interpolate \\\n",
    "    .withColumn(\n",
    "        'HourlyPrecipitation',\n",
    "        F.when(\n",
    "            (F.col(\"HourlyPrecipitation\").isNull()) | (F.col(\"HourlyPrecipitation\") == '*'),\n",
    "            (F.regexp_extract(F.col(\"REM\"), r\" P(\\d+)\", 1).cast(\"int\") * 0.01) # hundredths of inch kept in \"remarks\" section\n",
    "        ).otherwise(F.col(\"HourlyPrecipitation\"))\n",
    "    ) \\\n",
    "    .withColumn('HourlyPrecipitation', F.regexp_replace('HourlyPrecipitation', 'T', '0.01')) \\\n",
    "    .withColumn(\n",
    "        'HourlyPrecipitation',\n",
    "        F.regexp_extract('HourlyPrecipitation', r\"[0-9]+(\\.[0-9]+)?\", 0) # Match digits\n",
    "    ) \\\n",
    "    .withColumn('HourlyPrecipitation', F.col('HourlyPrecipitation').cast(DoubleType())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5518f302-6e3f-4a77-859c-fe7211b732ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can see that this did not significantly improve null counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f6e676-c187-4941-85b2-87f3596fd2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Original hourly wind speed null count: {df.filter(F.col('HourlyWindSpeed').isNull()).count()}\")\n",
    "print(f\"After REM extraction: {df_interpolate.filter(F.col('HourlyWindSpeed').isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a437bcb-6c2e-41b2-a16a-26799fa49088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original hourly wind gust speed null count: {df.filter(F.col('HourlyWindGustSpeed').isNull()).count()}\")\n",
    "print(f\"After REM extraction: {df_interpolate.filter(F.col('HourlyWindGustSpeed').isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eced805b-0e89-4bd2-b40c-ce59bf9234ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original precipitation null count: {df.filter(F.col('HourlyPrecipitation').isNull()).count()}\")\n",
    "print(f\"After REM extraction: {df_interpolate.filter(F.col('HourlyPrecipitation').isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76f9390b-c083-4e99-83e1-8fcb2a4cc554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exponential smoothing window (fold-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0247982-3194-479a-a0fc-0b8a9b92847a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_interpolate = df_interpolate.repartition(\"STATION\", F.date_trunc(\"hour\", \"dep_datetime\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c779bba6-5d90-4030-bc65-4330cbb1267d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def exponential_smoothing_udf(alpha: float):\n",
    "    \"\"\"exponential smoother\"\"\"\n",
    "    def _smooth(values):\n",
    "        if not values:\n",
    "            return 0.0  # Default to 0.0 if no non-null values\n",
    "        total = 0.0\n",
    "        sum_weights = 0.0\n",
    "        for idx, value in enumerate(reversed(values)):\n",
    "            if value is not None:\n",
    "                weight = (1 - alpha) ** idx\n",
    "                total += value * weight\n",
    "                sum_weights += weight\n",
    "        return total / sum_weights if sum_weights != 0 else 0.0\n",
    "    return F.udf(_smooth, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0bf6c78-2d2d-4106-b9db-ce7148f2bffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#go back to apply -2 hour constraint for test set(s) (?)\n",
    "def smooth_column(df, col_name, alpha=0.5, window_size=6):\n",
    "    \"\"\"Apply exponential smoothing to a single column;\n",
    "    if there are no non-null col values, return 0.\"\"\"\n",
    "    window_spec = Window.partitionBy(\"STATION\") \\\n",
    "                       .orderBy(\"dep_datetime\") \\\n",
    "                       .rowsBetween(-window_size, -1)\n",
    "    \n",
    "    return df.withColumn(\n",
    "        f\"non_null_{col_name}\",\n",
    "        F.expr(f\"\"\"\n",
    "            slice(\n",
    "                filter(\n",
    "                    collect_list({col_name}) OVER (PARTITION BY STATION ORDER BY dep_datetime),\n",
    "                    x -> x is not null\n",
    "                ),\n",
    "                greatest(\n",
    "                    size(\n",
    "                        filter(\n",
    "                            collect_list({col_name}) OVER (PARTITION BY STATION ORDER BY dep_datetime),\n",
    "                            x -> x is not null\n",
    "                        )\n",
    "                    ) - {window_size - 1},\n",
    "                    1\n",
    "                ),\n",
    "                {window_size}\n",
    "            )\n",
    "        \"\"\")\n",
    "    ).withColumn(\n",
    "        f\"smoothed_{col_name}\",\n",
    "        exponential_smoothing_udf(alpha)(F.col(f\"non_null_{col_name}\"))\n",
    "    ).withColumn(\n",
    "        col_name,\n",
    "        F.coalesce(F.col(col_name), F.col(f\"smoothed_{col_name}\"), lit(0.0))\n",
    "    ).drop(f\"non_null_{col_name}\", f\"smoothed_{col_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e4bef4-6da5-410b-acbb-ae3b333f9ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hourly_cols = [col for col in df_interpolate.columns if col.startswith(\"Hourly\")] #assumed cols that need interpolation\n",
    "\n",
    "def smooth_multiple_columns(df, columns=[col for col in df.columns if col.startswith(\"Hourly\")], alpha=0.5, window_size=6):\n",
    "    \"\"\"Apply smoothing to multiple columns; params:\n",
    "            alpha: exponential decay factor\n",
    "            window_size: number of non-null previous records to use\"\"\"\n",
    "    for col in columns:\n",
    "        df = smooth_column(df, col, alpha, window_size)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8cbda32-953d-49a8-92dc-a14445dfa069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b5297f-0cbc-4fbe-8483-fd204a14fc90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ALPHA = 0.5  # Smoothing factor\n",
    "NON_NULL_WINDOW_SIZE = 6  # Use last 6 non-null values\n",
    "\n",
    "# Window to collect all historical non-null values up to current row\n",
    "window_collect = Window.partitionBy(\"STATION\") \\\n",
    "                      .orderBy(\"dep_datetime\") \\\n",
    "                      .rowsBetween(-6, -1)\n",
    "\n",
    "# Step 1: Collect non-null values and extract last 6\n",
    "df_with_non_null = df_interpolate.withColumn(\n",
    "    \"non_null_precip\",\n",
    "    F.expr(f\"\"\"\n",
    "        slice(\n",
    "            filter(\n",
    "                collect_list(HourlyPrecipitation) OVER (PARTITION BY STATION ORDER BY dep_datetime),\n",
    "                x -> x is not null\n",
    "            ),\n",
    "            greatest(\n",
    "                size(\n",
    "                    filter(\n",
    "                        collect_list(HourlyPrecipitation) OVER (PARTITION BY STATION ORDER BY dep_datetime),\n",
    "                        x -> x is not null\n",
    "                    )\n",
    "                ) - {NON_NULL_WINDOW_SIZE - 1},\n",
    "                1\n",
    "            ),\n",
    "            {NON_NULL_WINDOW_SIZE}\n",
    "        )\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# Step 2: Apply exponential smoothing UDF\n",
    "def exponential_weighted_avg(precip_list):\n",
    "    total = 0.0\n",
    "    sum_weights = 0.0\n",
    "    for idx, value in enumerate(reversed(precip_list)):\n",
    "        weight = (1 - ALPHA) ** idx  # idx=0 (most recent), idx=5 (oldest of 6)\n",
    "        total += value * weight\n",
    "        sum_weights += weight\n",
    "    return total / sum_weights if sum_weights != 0 else None\n",
    "\n",
    "exponential_udf = F.udf(exponential_weighted_avg, DoubleType())\n",
    "\n",
    "df_smoothed = df_with_non_null.withColumn(\n",
    "    \"smoothed_precip\",\n",
    "    exponential_udf(\"non_null_precip\")\n",
    ").withColumn(\n",
    "    \"HourlyPrecipitation\",\n",
    "    F.coalesce(F.col(\"HourlyPrecipitation\"), F.col(\"smoothed_precip\"),lit(0.0))\n",
    ").drop(\"non_null_precip\", \"smoothed_precip\")\n",
    "\n",
    "display(df_smoothed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe0e6e3-49ea-4cc5-aeb1-0f6ba9a1dc9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_interpolate.filter(F.col(\"HourlyPrecipitation\").isNull()).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64774317-2665-4ecb-b999-a7a770772202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_interpolate.filter(F.col('STATION')=='72645704825').filter(F.col('HourlyPrecipitation').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665035a3-3e27-44ec-8580-51af9d1864ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_smoothed.filter(F.col('STATION')=='72645704825').filter(F.col('dep_datetime')=='2015-07-22T22:27:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9cb7ff6-0b29-44c9-9d6a-1a7fec3c79da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NN-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfccedd3-88d0-45af-b8b8-1e86568e280f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c1e0c62-9cc8-4587-b0b1-255fcf031be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nn_interpolate = df_interpolate.repartition(1000, \"STATION\")\n",
    "stations = stations.repartition(1000, \"station_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3ac3966-eec4-4742-a24c-665543b49246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Toy example: Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68b1462-011f-497f-9092-df9ab5b3068e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing = nn_interpolate.filter(F.col(\"HourlyPrecipitation\").isNull()) \\\n",
    "                   .select(\"STATION\", \"dep_datetime\", \"HourlyPrecipitation\")\n",
    "non_missing = nn_interpolate.filter(F.col(\"HourlyPrecipitation\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688dbdaf-b031-4c9d-b31e-c31b4cf7abc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(missing_neighbors.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c098b0b3-db75-43ef-aa65-e0fe8eb0ddbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nn_interpolate = nn_interpolate.repartition(\"STATION\", F.date_trunc(\"hour\", \"dep_datetime\"))\n",
    "stations = stations.repartition(\"station_id\")\n",
    "\n",
    "\n",
    "\n",
    "#filter and cache core datasets\n",
    "non_missing = nn_interpolate.filter(F.col(\"HourlyPrecipitation\").isNotNull()) \\\n",
    "                           .select(\"STATION\", \"dep_datetime\", \"HourlyPrecipitation\") \\\n",
    "                           .persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "missing = nn_interpolate.filter(F.col(\"HourlyPrecipitation\").isNull()) \\\n",
    "                        .select(\"STATION\", \"dep_datetime\", \"HourlyPrecipitation\")\n",
    "\n",
    "#time bucketing\n",
    "time_window = 4 * 3600  # 4 hours in seconds\n",
    "non_missing_time = non_missing.withColumn(\n",
    "    \"time_bucket\",\n",
    "    F.expr(\"cast((cast(dep_datetime as long) / 7200) as int) * 7200\")  # 2hr buckets\n",
    ").repartition(\"STATION\", \"time_bucket\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24cb015-a5ef-4fa8-86ee-403abd1c16b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join missing stations with neighbor IDs\n",
    "missing_neighbors = missing.alias(\"m\").join(\n",
    "    stations.alias(\"s\"),\n",
    "    F.col(\"m.STATION\") == F.col(\"s.station_id\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    \"m.STATION\",\n",
    "    \"m.dep_datetime\",\n",
    "    \"s.neighbor_id\",\n",
    "    \"m.HourlyPrecipitation\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a004817e-1890-42c7-86ba-3d7919a08f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "non_missing_time=non_missing_time.repartition(\"STATION\", \"time_bucket\")\n",
    "\n",
    "\n",
    "missing_neighbors = missing_neighbors.repartition(\"neighbor_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa70faa-0a2e-4f2d-aa40-6831b313318f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# get non-missing neighbors \n",
    "imputed = (\n",
    "    missing_neighbors.alias(\"mn\")\n",
    "    .join(\n",
    "        non_missing_time.alias(\"nn\"),\n",
    "        (F.col(\"mn.neighbor_id\") == F.col(\"nn.STATION\")) &  # join on neighbor ID\n",
    "        (F.col(\"mn.dep_datetime\").cast(\"long\") - F.col(\"nn.dep_datetime\").cast(\"long\")).between(7200, 14400),\n",
    "        \"left\"\n",
    "    )\n",
    "    .withColumn(\"time_diff\", F.abs(F.col(\"mn.dep_datetime\") - F.col(\"nn.dep_datetime\")))\n",
    "    .withColumn(\"rnk\", F.row_number().over(\n",
    "        Window.partitionBy(\"mn.STATION\", \"mn.dep_datetime\")\n",
    "              .orderBy(F.asc(\"time_diff\"))\n",
    "    ))\n",
    "    .filter(F.col(\"rnk\") == 1)\n",
    "    .select(\n",
    "        \"mn.STATION\",\n",
    "        \"mn.dep_datetime\",\n",
    "        F.coalesce(\"mn.HourlyPrecipitation\", \"nn.HourlyPrecipitation\").alias(\"HourlyPrecipitation\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17fa9d3f-808c-4988-88de-e0b92f2a1d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(non_missing_time.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d208233-e6e1-4167-8aa0-d38a7ff2c76f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Align partitioning for spatial-temporal joins\n",
    "nn_interpolate = nn_interpolate.repartition(1000, \"STATION\", F.date_trunc(\"hour\", \"dep_datetime\"))\n",
    "stations = stations.repartition(1000, \"station_id\")\n",
    "\n",
    "# 3. Filter datasets without caching\n",
    "non_missing = nn_interpolate.filter(F.col(\"HourlyPrecipitation\").isNotNull()) \\\n",
    "                           .select(\"STATION\", \"dep_datetime\", \"HourlyPrecipitation\")\n",
    "\n",
    "missing = nn_interpolate.filter(F.col(\"HourlyPrecipitation\").isNull()) \\\n",
    "                        .select(\"STATION\", \"dep_datetime\", \"HourlyPrecipitation\")\n",
    "\n",
    "# 4. Time bucketing (for temporal alignment)\n",
    "time_window = 4 * 3600  # 4 hours in seconds\n",
    "non_missing_time = non_missing.withColumn(\n",
    "    \"time_bucket\",\n",
    "    F.expr(\"cast((cast(dep_datetime as long) / 7200) as int) * 7200\")  # 2-hour buckets\n",
    ").repartition(1000, \"STATION\", \"time_bucket\")\n",
    "\n",
    "# 5. Join missing stations with neighbor IDs\n",
    "missing_neighbors = missing.alias(\"m\").join(\n",
    "    stations.alias(\"s\"),\n",
    "    F.col(\"m.STATION\") == F.col(\"s.station_id\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    \"m.STATION\",\n",
    "    \"m.dep_datetime\",\n",
    "    \"s.neighbor_id\",\n",
    "    \"m.HourlyPrecipitation\"\n",
    ")\n",
    "\n",
    "# 6. Temporal join with range constraints\n",
    "imputed = (\n",
    "    missing_neighbors.alias(\"mn\")\n",
    "    .join(\n",
    "        non_missing_time.alias(\"nn\"),\n",
    "        (F.col(\"mn.neighbor_id\") == F.col(\"nn.STATION\")) & \n",
    "        (F.col(\"mn.dep_datetime\").cast(\"long\") - F.col(\"nn.dep_datetime\").cast(\"long\")).between(7200, 14400),\n",
    "        \"left\"\n",
    "    )\n",
    "    .withColumn(\"time_diff\", F.abs(F.col(\"mn.dep_datetime\").cast(\"long\") - F.col(\"nn.dep_datetime\").cast(\"long\")))\n",
    "    .withColumn(\n",
    "        \"rnk\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"mn.STATION\", \"mn.dep_datetime\")\n",
    "                  .orderBy(F.asc(\"time_diff\"))\n",
    "        )\n",
    "    )\n",
    "    .filter(F.col(\"rnk\") == 1)\n",
    "    .select(\n",
    "        \"mn.STATION\",\n",
    "        \"mn.dep_datetime\",\n",
    "        F.coalesce(F.col(\"mn.HourlyPrecipitation\"), F.col(\"nn.HourlyPrecipitation\")).alias(\"HourlyPrecipitation\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd73f1d-ecd7-4a85-993f-36a991fad53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(imputed.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e08be01f-4088-473c-b543-939438ce03f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(imputed.filter(F.col('HourlyPrecipitation').isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e06ca8-4c98-41e8-b273-fc34c1dfda7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c80ed4-90d0-4415-8713-d5efba3407b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nn_interpolate = nn_interpolate.withColumn(\n",
    "    \"dep_datetime\", \n",
    "    F.to_timestamp(F.col(\"dep_datetime\"))  # Convert to timestamp if stored as string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e734020-5de5-49a6-b37a-11c8d2e254cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(nn_interpolate.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad42d65a-3fcb-4991-866c-c0fe6b368111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c58693-bcf2-49ad-858f-4150ba0042b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#naive (non-weighted) forward-smooth window spec\n",
    "# window_spec = Window.partitionBy(\"STATION\") \\\n",
    "#                    .orderBy(\"dep_datetime\") \\\n",
    "#                    .rowsBetween(-8, -1)\n",
    "\n",
    "# df_smoothed = df_interpolate.withColumn(\n",
    "#     \"HourlyPrecipitation\",\n",
    "#     F.when(F.col('HourlyPrecipitation').isNull(),\n",
    "#     F.coalesce(\n",
    "#         F.col(\"HourlyPrecipitation\"),\n",
    "#         F.avg(F.col(\"HourlyPrecipitation\")).over(window_spec)\n",
    "#     )\n",
    "# ).otherwise(F.col('HourlyPrecipitation'))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba0e094-cceb-4198-b369-5d498a8b5e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define exponential smoothing parameters\n",
    "ALPHA = 0.3  # Smoothing factor (0 < ALPHA < 1) - need to tune\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "# Create window to collect past [window_size] values (excluding current row)\n",
    "window_spec = Window.partitionBy(\"STATION\") \\\n",
    "                   .orderBy(\"dep_datetime\") \\\n",
    "                   .rowsBetween(-WINDOW_SIZE, -1)\n",
    "\n",
    "# Step 1: Collect past precipitation values into a list\n",
    "df_with_list = df_interpolate.withColumn(\n",
    "    \"past_precip\",\n",
    "    F.collect_list(\"HourlyPrecipitation\").over(window_spec)\n",
    ")\n",
    "\n",
    "# UDF for exponential smoothing\n",
    "def exponential_weighted_avg(precip_list):\n",
    "    if not precip_list:\n",
    "        return None\n",
    "    total = 0.0\n",
    "    sum_weights = 0.0\n",
    "    # Reverse to process newest first (higher weights)\n",
    "    for idx, value in enumerate(reversed(precip_list)):\n",
    "        if value is not None:\n",
    "            weight = (1 - ALPHA) ** idx  # idx=0 (newest), idx=6 (oldest)\n",
    "            total += value * weight\n",
    "            sum_weights += weight\n",
    "    return total / sum_weights if sum_weights != 0 else None\n",
    "\n",
    "exponential_udf = F.udf(exponential_weighted_avg, DoubleType())\n",
    "\n",
    "# Apply smoothing and fill nulls\n",
    "df_smoothed = df_with_list.withColumn(\n",
    "    \"smoothed_precip\",\n",
    "    exponential_udf(\"past_precip\")\n",
    ").withColumn(\n",
    "    \"HourlyPrecipitation\",\n",
    "    F.coalesce(F.col(\"HourlyPrecipitation\"), F.col(\"smoothed_precip\"))\n",
    ").drop(\"past_precip\", \"smoothed_precip\")\n",
    "\n",
    "# Show results\n",
    "display(df_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4374cb1-4abd-40ac-9719-9171806b4c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_smoothed.filter(F.col('HourlyPrecipitation').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea1e451-0f2b-4c30-9f75-65a1ae781530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nn = df_smoothed \\\n",
    "    .join(stations, F.col('STATION') == stations.station_id, \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09accd26-2bfc-4b32-a177-cb9bf58168c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fill = nn.filter(F.col('HourlyPrecipitation').isNotNull()) \\\n",
    "    .select(F.col('STATION').alias('fill_station'), F.col('HourlyPrecipitation').alias('fill_HourlyPrecipitation'))\n",
    "\n",
    "empty = nn.filter(F.col('HourlyPrecipitation').isNull()).alias('df1') \\\n",
    "    .join(fill.alias('fill'), F.col('df1.neighbor_id') == F.col('fill.fill_station'), 'left') \\\n",
    "    .withColumn('HourlyPrecipitation', F.coalesce(F.col('df1.HourlyPrecipitation'), F.col('fill.fill_HourlyPrecipitation')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b80cae-492f-420f-9727-d9cd728410b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empty.filter(F.col('HourlyPrecipitation').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa4fa62-d147-4aa3-a754-3139eb545389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f24e68-bf1a-4471-b7a2-1170300f5524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eab7e0d-523c-48a4-b78a-1beb5b092a2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time-based "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0.07-eil-otpw-cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
