{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242b11e6-dc13-4cb9-878a-09819a738b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Flights data cleaning\n",
    "Erica Landreth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710c981e-0d20-4191-a292-6ad2af4e6bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Restart the Python kernel\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5938cb3c-204b-434b-ad96-65ceac820c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f26655-f249-48f3-a3e3-cf69842e276f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtering to relevant rows/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de4b43c-303f-489a-aa3a-3847e0cd78ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea03f216-9ba6-4828-a155-d68bda4c3cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"dbfs:/mnt/mids-w261/OTPW_3M_2015.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224029e0-3bde-4598-8895-8463b38c5e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load flights data\n",
    "\n",
    "# dataset = 'parquet_airlines_data_3m' # 3 months\n",
    "# dataset = 'parquet_airlines_data_1y' # 1 year\n",
    "dataset = \"OTPW_3M_2015\"\n",
    "df_flights = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"dbfs:/mnt/mids-w261/{dataset}.csv\")\n",
    "shape_orig = (df_flights.count(), len(df_flights.dtypes))\n",
    "display(df_flights)\n",
    "print(f\"Original shape: {shape_orig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d74631-9a9d-4443-b020-576c94723719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define columns to drop\n",
    "# columns related to diversion: not enough data to use the diversion info\n",
    "div_cols = [col for col in df_flights.columns if col.startswith('DIV') and col != \"DIVERTED\"]\n",
    "# redundant carrier ID's (EDA indicated that OP_UNIQUE_CARRIER is sufficient)\n",
    "xtra_carrier_cols = [\"OP_CARRIER_AIRLINE_ID\",\"OP_CARRIER\"]\n",
    "# redundant airport ID's (EDA indicated that ORIGIN/DEST and *_AIRPORT_SEQ_ID are sufficient)\n",
    "xtra_airport_cols = [ \\\n",
    "  \"ORIGIN_AIRPORT_ID\",\"ORIGIN_CITY_MARKET_ID\",\"ORIGIN_STATE_ABR\",\"ORIGIN_STATE_NM\",\"ORIGIN_WAC\", \\\n",
    "  \"DEST_AIRPORT_ID\",\"DEST_CITY_MARKET_ID\",\"DEST_STATE_ABR\",\"DEST_STATE_NM\",\"DEST_WAC\"]\n",
    "# redundant flight info (could be recreated if need be)\n",
    "xtra_flight_cols = [\"WHEELS_OFF\",\"WHEELS_ON\",\"FLIGHTS\",\"ACTUAL_ELAPSED_TIME\",\"DISTANCE_GROUP\"]\n",
    "# redundant delay status info (could be recreated if need be)\n",
    "xtra_time_cols = [\"DEP_TIME\",\"DEP_DELAY_NEW\",\"DEP_DEL15\",\"DEP_DELAY_GROUP\",\"ARR_TIME\",\"ARR_DELAY_NEW\",\"ARR_DEL15\",\"ARR_DELAY_GROUP\"]\n",
    "\n",
    "## fields to keep\n",
    "# core features: useful for ML features and/or feature engineering\n",
    "core_feats = [\"FL_DATE\",\"OP_UNIQUE_CARRIER\",\"TAIL_NUM\",\"OP_CARRIER_FL_NUM\",\"ORIGIN\",\"DEST\",\"CRS_DEP_TIME\",\"DEP_DELAY\",\"CRS_ARR_TIME\",\"ARR_DELAY\",\"CANCELLED\",\"DIVERTED\",\"CRS_ELAPSED_TIME\",\"AIR_TIME\",\"DISTANCE\"]\n",
    "# we may or may not end up using these, but they can't easily be recreated later, so we'll keep them to be cautious\n",
    "on_the_fence = [\"ORIGIN_AIRPORT_SEQ_ID\",\"DEST_AIRPORT_SEQ_ID\",\"TAXI_OUT\",\"TAXI_IN\"]\n",
    "# useful for time series analysis\n",
    "time_series = [\"QUARTER\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"DEP_TIME_BLK\",\"ARR_TIME_BLK\",\"YEAR\"]\n",
    "# useful to sanity check that joins are successful\n",
    "sanity_check = [\"ORIGIN_CITY_NAME\",\"DEST_CITY_NAME\",\"ORIGIN_STATE_FIPS\",\"DEST_STATE_FIPS\"]\n",
    "# provides reasoning for cancellations, delays, and returns to gate\n",
    "delay_info = [col for col in df_flights.columns if col.endswith(\"_DELAY\") and col not in core_feats] + [\"CANCELLATION_CODE\"] + [\"FIRST_DEP_TIME\",\"LONGEST_ADD_GTIME\",\"TOTAL_ADD_GTIME\"]\n",
    "    # Note: cancellation codes are: \"A\" for carrier-caused, \"B\" for weather, \"C\" for National Aviation System, and \"D\" for security\n",
    "\n",
    "all_cols = div_cols+xtra_carrier_cols+xtra_airport_cols+xtra_flight_cols+xtra_time_cols+core_feats+on_the_fence+time_series+sanity_check+delay_info\n",
    "\n",
    "missing = [col for col in df_flights.columns if col not in all_cols]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e308a2-fe19-429e-9b23-4c8f8a4a8143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define columns to keep\n",
    "keep_me = core_feats + on_the_fence + time_series + sanity_check + delay_info\n",
    "keep_me += [c for c in df_flights.columns if \"Hourly\" in c]\n",
    "keep_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99af8fa5-07f6-45e6-a61f-b7ce5fde6800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter to columns of interest, and de-dupe\n",
    "df_flights = df_flights.select(keep_me).distinct()\n",
    "\n",
    "shape_filt = (df_flights.count(), len(df_flights.dtypes))\n",
    "display(df_flights)\n",
    "print(f\"Filtered shape: {shape_filt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f2c94c-10f2-4433-9938-2fa2d3347639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sanity check: we expect half the records after de-dupe\n",
    "shape_orig[0]/shape_filt[0] == 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c7f2cb-4085-442a-98ae-271e8211e05d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shape_tz = (df_flights.count(), len(df_flights.dtypes))\n",
    "print(f\"Shape after adding time zone info: {shape_tz}\")\n",
    "\n",
    "# filter to those rows with outcome info populated\n",
    "df_flights = df_flights.filter( (col(\"DEP_DELAY\").isNotNull()) | (col(\"CANCELLED\") == 1) | (col(\"DIVERTED\") == 1) )\n",
    "shape_outcome = (df_flights.count(), len(df_flights.dtypes))\n",
    "print(f\"Shape after filtering for populated outcome vars: {shape_outcome}\")\n",
    "\n",
    "# filter to those with populated info for basic flight metadata\n",
    "df_flights = df_flights.filter( (col(\"OP_UNIQUE_CARRIER\").isNotNull()) & (col(\"ORIGIN\").isNotNull()) & (col(\"DEST\").isNotNull()) & (col(\"FL_DATE\").isNotNull()) & (col(\"CRS_DEP_TIME\").isNotNull()) & (col(\"CRS_ARR_TIME\").isNotNull()) )\n",
    "shape_basic = (df_flights.count(), len(df_flights.dtypes))\n",
    "print(f\"Shape after filtering for basic feature info: {shape_basic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183da413-8777-4f63-befa-15782664a382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Converting flight data to UTC time zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c1dd8dd-5afe-40ae-b954-fd1fe335c41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create time zone reference file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2395a85-0408-4bb5-98b4-a19272e85cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The below code was used to look up the time zone for each airport. The resulting time zone info was saved out to parquet, so from this point on, just load the time zone parquet (see below for path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba380239-d0bb-42ff-b393-d56461f65dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install timezonefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873c32a9-8b38-468f-b472-298ec8e25e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # imports\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "# from timezonefinder import TimezoneFinder\n",
    "# import pytz\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # load stations data\n",
    "# df_stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/\")\n",
    "\n",
    "# # get unique airport info from stations table\n",
    "# df_locs = df_stations.select('neighbor_call','neighbor_lat','neighbor_lon').distinct()\n",
    "# display(df_locs)\n",
    "\n",
    "# # define function to look up time zones\n",
    "# def find_timezone(lat, lng):\n",
    "#     tf = TimezoneFinder()\n",
    "#     timezone_str = tf.timezone_at(lat=lat, lng=lng)\n",
    "#     return timezone_str if timezone_str else \"Unknown\"\n",
    "\n",
    "# # define udf for time zone lookup\n",
    "# find_timezone_udf = udf(find_timezone, StringType())\n",
    "\n",
    "# # add time zone column\n",
    "# df_locs = df_locs.withColumn(\"timezone\", find_timezone_udf(col(\"neighbor_lat\"), col(\"neighbor_lon\")))\n",
    "\n",
    "# # save df_time zone info as a parquet file\n",
    "# folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "# df_locs.write.parquet(f\"{folder_path}/external/tz_lookup.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc8069e-2444-4bc6-b328-ded42b824aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "\n",
    "# df_tz = spark.read.parquet(f\"dbfs:/student-groups/Group_4_1/external/tz_lookup.parquet\")\n",
    "\n",
    "# # manually fill in missing time zone info\n",
    "# # note: neighbor_call is ICAO\n",
    "# BBG = Row(neighbor_call=\"BBG\", neighbor_lat=36.53856729627892, neighbor_lon=-93.19908127077512, timezone=\"America/Chicago\")\n",
    "# KOGS = Row(neighbor_call=\"KOGS\", neighbor_lat=44.6820707679313, neighbor_lon=-75.47692203483886, timezone=\"America/New_York\")\n",
    "# NSTU = Row(neighbor_call=\"NSTU\", neighbor_lat=-14.329024376251269, neighbor_lon=-170.71329690482548, timezone=\"Pacific/Pago_Pago\")\n",
    "# PGSN = Row(neighbor_call=\"PGSN\", neighbor_lat=15.11974288544001, neighbor_lon=145.7282788950688, timezone=\"Pacific/Saipan\")\n",
    "# PGUM = Row(neighbor_call=\"PGUM\", neighbor_lat=13.48562402083883, neighbor_lon=144.8001485238768, timezone=\"Pacific/Guam\")\n",
    "# TJPS = Row(neighbor_call=\"TJPS\", neighbor_lat=18.01055087987774, neighbor_lon=-66.56323216254391, timezone=\"America/Puerto_Rico\")\n",
    "# TJSJ = Row(neighbor_call=\"TJSJ\", neighbor_lat=18.457160454103658, neighbor_lon=-66.0974759565605, timezone=\"America/Puerto_Rico\")\n",
    "# US_0571 = Row(neighbor_call=\"US-0571\", neighbor_lat=48.25780621107438, neighbor_lon=-103.74169879360201, timezone=\"America/Chicago\")\n",
    "\n",
    "# man_df = spark.createDataFrame([BBG,KOGS,NSTU,PGSN,PGUM,TJPS,TJSJ,US_0571])\n",
    "# df_tz = df_tz.union(man_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4ef2e2-6308-4388-a368-99bdaa9ffea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "# df_tz.write.mode('overwrite').parquet(f\"{folder_path}/external/tz_lookup_manually_adjusted.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feaab843-45ca-428c-a572-5ef537dab2b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apply time zones to create full cleaned flights table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99852409-c3a2-471b-8d3b-590f454cb677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airports = spark.read.option(\"header\",\"true\").csv(f\"dbfs:/mnt/mids-w261/airport-codes_csv.csv\")\n",
    "df_tz = spark.read.parquet(f\"dbfs:/student-groups/Group_4_1/external/tz_lookup_manually_adjusted.parquet\")\n",
    "\n",
    "# start with a very, very small flight data sample\n",
    "tmp_flights = df_flights.limit(10)\n",
    "\n",
    "# create temporary views\n",
    "df_flights.createOrReplaceTempView(\"df_flights\")\n",
    "df_airports.createOrReplaceTempView(\"df_airports\")\n",
    "df_tz.createOrReplaceTempView(\"df_tz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528a1ef3-e063-4c56-a7c9-12319baae167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define all columns of the flights table (for use in SELECT statement)\n",
    "flights_cols = \"x.\" + \", x.\".join(df_flights.columns)\n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "WITH origin AS(\n",
    "SELECT  {flights_cols},\n",
    "        x.FL_DATE as date,\n",
    "        x.CRS_DEP_TIME as dep_time,\n",
    "        x.CRS_ARR_TIME as arr_time,\n",
    "        x.ORIGIN as origin_iata,\n",
    "        x.DEST as dest_iata,\n",
    "        a.ident as origin_icao\n",
    "FROM df_flights as x\n",
    "LEFT JOIN df_airports as a on x.ORIGIN = a.iata_code),\n",
    "\n",
    "origin_dest AS(\n",
    "SELECT  {flights_cols},\n",
    "        x.date,\n",
    "        x.dep_time,\n",
    "        x.arr_time,\n",
    "        x.origin_iata,\n",
    "        x.dest_iata,\n",
    "        x.origin_icao,\n",
    "        a.ident as dest_icao\n",
    "FROM origin as x\n",
    "LEFT JOIN df_airports as a on x.dest_iata = a.iata_code),\n",
    "\n",
    "origin_dest_tz1 AS(\n",
    "SELECT  {flights_cols},\n",
    "        x.date,\n",
    "        x.dep_time,\n",
    "        x.arr_time,\n",
    "        x.origin_iata,\n",
    "        tz.timezone as origin_tz,\n",
    "        x.dest_iata,\n",
    "        x.origin_icao,\n",
    "        x.dest_icao\n",
    "FROM origin_dest as x\n",
    "LEFT JOIN df_tz as tz on x.origin_icao = tz.neighbor_call\n",
    "),\n",
    "\n",
    "origin_dest_tz2 AS(\n",
    "SELECT  {flights_cols},\n",
    "        x.date,\n",
    "        x.dep_time,\n",
    "        x.arr_time,\n",
    "        x.origin_iata,\n",
    "        x.origin_tz,\n",
    "        x.dest_iata,\n",
    "        tz.timezone as dest_tz,\n",
    "        x.origin_icao,\n",
    "        x.dest_icao\n",
    "FROM origin_dest_tz1 as x\n",
    "LEFT JOIN df_tz as tz on x.dest_icao = tz.neighbor_call\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM origin_dest_tz2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "out = spark.sql(query)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9854d41-791c-497a-a545-ed5b4af402de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# double check that all the time zones were successfully populated\n",
    "tmp = out.filter( (col(\"origin_tz\").isNull()) | (col(\"dest_tz\").isNull()) )\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3aa9d8-6f42-479a-93ae-587d56a4ad57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_utc(yyyymmdd, dep_hhmm, arr_hhmm, dep_tz, arr_tz, flight_dur):\n",
    "    \"\"\"\n",
    "    Create UTC timestamp from flights table columns\n",
    "    yyyymmdd = FL_DATE\n",
    "    dep_hhmm = CRS_DEP_TIME\n",
    "    arr_hhmm = CRS_ARR_TIME\n",
    "    dep_tz = time zone from time zone table\n",
    "    arr_tz = time zone from time zone table\n",
    "    flight_dur = CRS_ELAPSED_TIME (for sanity check of arrival time)\n",
    "\n",
    "    Returns UTC time stamp, (cast to string)\n",
    "    \"\"\"\n",
    "\n",
    "    dep_hhmm = int(dep_hhmm)\n",
    "    arr_hhmm = int(arr_hhmm)\n",
    "\n",
    "    yyyy,MM,dd = yyyymmdd.split('-')\n",
    "    yyyy = int(yyyy) # get year\n",
    "    MM = int(MM) # get month\n",
    "    dd = int(dd) # get day\n",
    "\n",
    "    dep_hh = dep_hhmm//100 # get hour\n",
    "    dep_mm = dep_hhmm%100 # get minute\n",
    "    if dep_hh == 24:\n",
    "        dep_hh = 0\n",
    "        dep_shift = True\n",
    "    else:\n",
    "        dep_shift = False\n",
    "\n",
    "    arr_hh = arr_hhmm//100 # get hour\n",
    "    arr_mm = arr_hhmm%100\n",
    "    if arr_hh == 24:\n",
    "        arr_hh = 0\n",
    "        arr_shift = True\n",
    "    else:\n",
    "        arr_shift = False\n",
    "\n",
    "    # create datetime variable for departure\n",
    "    dt_dep = datetime(yyyy,MM,dd,dep_hh,dep_mm)\n",
    "    if dep_shift:\n",
    "        dt_dep += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    dep_local = pytz.timezone(dep_tz).localize(dt_dep)\n",
    "    # convert to UTC\n",
    "    dep_utc = dep_local.astimezone(pytz.utc)\n",
    "\n",
    "    # create datetime variable for arrival\n",
    "    dt_arr = datetime(yyyy,MM,dd,arr_hh,arr_mm)\n",
    "    if arr_shift:\n",
    "        dt_arr += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    arr_local = pytz.timezone(arr_tz).localize(dt_arr)\n",
    "    # convert to UTC\n",
    "    arr_utc = arr_local.astimezone(pytz.utc)\n",
    "\n",
    "    if dep_utc > arr_utc:\n",
    "        arr_utc += timedelta(days=1)\n",
    "\n",
    "    # # sanity check\n",
    "    # arr_utc_SC = dep_utc + timedelta(minutes=flight_dur)\n",
    "\n",
    "    dt_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "    # return UTC datetime, cast to string\n",
    "    # return (dep_utc.strftime(dt_format), arr_utc.strftime(dt_format), arr_utc_SC.strftime(dt_format))\n",
    "    return (dep_utc.strftime(dt_format), arr_utc.strftime(dt_format))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"dep_datetime\", StringType(), False),\n",
    "    StructField(\"arr_datetime\", StringType(), False),\n",
    "    # StructField(\"arr_datetime_SANITYCHECK\", StringType(), False)\n",
    "])\n",
    "\n",
    "dt_udf = udf(to_utc, schema)\n",
    "\n",
    "out = out.withColumn('processed', dt_udf(col(\"date\"), col(\"dep_time\"), col(\"arr_time\"), col(\"origin_tz\"), col(\"dest_tz\"), col(\"CRS_ELAPSED_TIME\")))\n",
    "\n",
    "cols = [c for c in out.columns if c != \"processed\"]\n",
    "# cols += [\"processed.dep_datetime\",\"processed.arr_datetime\",\"processed.arr_datetime_SANITYCHECK\"]\n",
    "cols += [\"processed.dep_datetime\",\"processed.arr_datetime\"]\n",
    "out = out.select(cols)\n",
    "\n",
    "# out = out.withColumn(\"dep_timestamp\", F.to_timestamp(col('dep_datetime')).alias('dep_timestamp'))\n",
    "# out = out.withColumn(\"arr_timestamp\", F.to_timestamp(col('arr_datetime')).alias('arr_timestamp'))\n",
    "# out = out.withColumn(\"arr_timestamp_SANITYCHECK\", F.to_timestamp(col('arr_datetime_SANITYCHECK')).alias('arr_timestamp_SANITYCHECK'))\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbf78ea-29bb-48a4-9226-d1e2699cf47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2cc3ac-74db-4eaf-8456-ca94a58488ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove redundant variables that were added during the join process\n",
    "redundant = [\"date\",\"dep_time\",\"arr_time\"]\n",
    "# note ORIGIN and DEST are technically redudant, but will keep for now\n",
    "\n",
    "out = out.drop(*redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d073ad94-621d-4b61-9fc0-0d67faee939f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final shape\n",
    "shape_final = (out.count(), len(out.dtypes))\n",
    "print(f\"Shape after cleaning: {shape_final}\")\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f01d93-7f93-475f-9600-4ded1481e4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write cleaned output to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb24b310-6b39-41d6-bcdf-bf70bd96abf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "out.write.parquet(f\"{folder_path}/interim/{dataset}_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076e14b2-5551-40ec-b1e9-0f92c9de492e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check that write was successful\n",
    "df = spark.read.parquet(f\"{folder_path}/interim/{dataset}_clean.parquet\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d68a54-2068-4945-8232-f92e8111386c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{folder_path}/interim/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201886c3-f362-4e3e-8c35-df21939ee578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0.07-eil-otpw-cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
