{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea57f82-909d-40c5-a5ef-ff6655abcf9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a6327e-1199-4273-86b3-bb53c01e3c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Configure Spark settings for better performance\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder\\\n",
    "#     .config(\"spark.executor.memory\", \"16g\")\\\n",
    "#     .config(\"spark.executor.cores\", 4)\\\n",
    "#     .appName('Final Project Training')\\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "# spark.conf.set(\"spark.default.parallelism\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d62c3-ed6a-47d0-bb21-3c0faee08bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, when\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0961268-eafb-4755-9d8c-7541236ade55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cbc4b7a-aaa3-4ba6-8df3-e98af097300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7e45df-c877-46e1-b5f9-fa843a68c574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de30d1e2-82a4-461f-a323-79e9ca866d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds and overlap\n",
    "k = 5\n",
    "overlap = 0.2\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup)\n",
    "compute_seasonality = False\n",
    "apply_seasonality = False\n",
    "\n",
    "# define train/test split date\n",
    "if period == \"3m\":\n",
    "    min_test_dt = \"2015-03-01\"\n",
    "elif period == \"1y\":\n",
    "    min_test_dt = \"2019-10-01\"\n",
    "elif period == \"\":\n",
    "    min_test_dt = \"2019-01-01\"\n",
    "print(f\"Min test set date for {period} dataset: {min_test_dt}\")\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7820b43-a23e-488f-a7f3-d9b07493cf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data and perform simple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "\n",
    "# read in joined, cleaned dataset\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather_{period}.parquet\") # !!!\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_weather_cleaned_combo.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather{period}_v1.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned.parquet\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned_pr_v2.parquet\")\n",
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour and date variables (needed for seasonality and CV splits, respectively)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname))) \\\n",
    "    .withColumn(\"dep_date_utc\", to_date(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5455f469-d289-4a5c-935a-ebe06e5f16d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Group by the year and count the number of records for each year\n",
    "# df_year_counts = df.groupBy(\"YEAR\").count()\n",
    "\n",
    "# # Display the result\n",
    "# display(df_year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "# df_train.cache()\n",
    "# print(f\"Train data: {df_train.count()} records\")\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt) \\\n",
    "    .filter(f.col(dep_utc_varname) < \"2020-01-01\")\n",
    "# df_test.cache()\n",
    "# print(f\"Test data: {df_test.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa30565b-2ab1-45f9-ac57-8832abc556d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get cross-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc820cf1-c179-4279-8fe0-505d1758f9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits_by_days_with_overlap(df, k=3, blocking=False, overlap=0, dep_utc_varname=dep_utc_varname, verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation, based on # days in dataset\n",
    "    '''\n",
    "    \n",
    "    min_date = df.select(f.min(\"dep_date_utc\")).collect()[0][0]\n",
    "    max_date = df.select(f.max(\"dep_date_utc\")).collect()[0][0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "    total_width = k+1 - overlap*(k-1)\n",
    "    chunk_size = np.ceil(n_days/total_width) # last chunk may be slightly smaller than the others\n",
    "\n",
    "    # idx = np.arange(0,)\n",
    "    # idx = np.arange(0,n_days,chunk_size)\n",
    "    # idx[-1] = n_days-1\n",
    "    # idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Splitting data into {k} folds with {overlap} overlap')\n",
    "        print(f'Min date: {min_date}, max date: {max_date}')\n",
    "        print(f'{chunk_size:,} days per fold')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define indices based on chunk size and overlap\n",
    "        if i == 0:\n",
    "            train_min_offset = 0\n",
    "            train_max_offset = chunk_size\n",
    "        else:\n",
    "            train_min_offset += np.ceil((1-overlap)*chunk_size)\n",
    "            train_max_offset += np.floor((1-overlap)*chunk_size)\n",
    "        test_min_offset = train_max_offset\n",
    "        test_max_offset = test_min_offset + chunk_size\n",
    "\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = min_date\n",
    "        else:\n",
    "            t_min_train = min_date + timedelta(days=train_min_offset)\n",
    "        # define maximum training time\n",
    "        t_max_train = min_date + timedelta(days=train_max_offset)\n",
    "        # define minimum test time\n",
    "        t_min_test = min_date + timedelta(days=test_min_offset)\n",
    "        # define maximum test_time\n",
    "        t_max_test = min_date + timedelta(days=test_max_offset)\n",
    "\n",
    "        if t_max_test > max_date + timedelta(1):\n",
    "            t_max_test = max_date + timedelta(1)\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        print(\"(Note that the max dates are non-inclusive)\")\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc02f46-6e68-4780-91f9-e82265e6a3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs = [\n",
    "    {\"train_min\": \"2014-12-31\", \"train_max\": \"2015-10-09\", \"test_min\": \"2015-10-09\", \"test_max\": \"2016-07-17\"},\n",
    "    {\"train_min\": \"2015-08-14\", \"train_max\": \"2016-05-21\",\"test_min\": \"2016-05-21\", \"test_max\": \"2017-02-27\"},\n",
    "    {\"train_min\": \"2016-03-27\", \"train_max\": \"2017-01-01\",\"test_min\": \"2017-01-01\", \"test_max\": \"2017-10-10\"},\n",
    "    {\"train_min\": \"2016-11-08\", \"train_max\": \"2017-08-14\",\"test_min\": \"2017-08-14\", \"test_max\": \"2018-05-23\"},\n",
    "    {\"train_min\": \"2017-06-22\", \"train_max\": \"2018-03-27\",\"test_min\": \"2018-03-27\", \"test_max\": \"2019-01-01\"}\n",
    "    ]\n",
    "cv_cutoffs = pd.DataFrame(cv_cutoffs)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # get cross-validation split times\n",
    "# cv_cutoffs = get_cv_time_limits_by_days_with_overlap(df_train.select(\"dep_date_utc\"), k=k, blocking=True, overlap=overlap,\n",
    "#     dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "# cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515efe49-587f-4653-bc21-e31da2242f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLP Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28b4cc5c-3f12-479d-9a67-f5e209eb025d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b646e1c1-a29d-4f78-8d3e-c60d15aeafc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4461e37f-c3e4-4af4-9f4c-f0d619adc004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_series_cv_folds(\n",
    "    df,\n",
    "    time_col: str,\n",
    "    k: int=3,\n",
    "    blocking: bool=False,\n",
    "    overlap: float=0.0,\n",
    "    verbose: bool=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Split a time-series PySpark DataFrame into k train/test folds with optional overlap and blocking.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): PySpark DataFrame with a timestamp column.\n",
    "        dep_utc_time_colvarname (str): Name of the timestamp column.\n",
    "        k (int): Number of folds.\n",
    "        blocking (bool): Whether to block the training set to avoid cumulative data.\n",
    "        overlap (float): Fraction of overlap between validation windows (e.g. 0.2 = 20% overlap).\n",
    "        verbose (bool): Whether to print the time splits.\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_df, val_df) tuples.\n",
    "    \"\"\"\n",
    "    # Get time boundaries\n",
    "    min_date = df.select(F.min(time_col)).first()[0]\n",
    "    max_date = df.select(F.max(time_col)).first()[0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "\n",
    "    # Adjust chunk sizing\n",
    "    total_width = k + 1 - overlap * (k - 1)\n",
    "    chunk_size = int(np.ceil(n_days / total_width))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Splitting data into {k} folds with {overlap*100:.0f}% overlap\")\n",
    "        print(f\"Min date: {min_date}, Max date: {max_date}\")\n",
    "        print(f\"{chunk_size:,} days per fold\")\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        # Offset calculation with overlap\n",
    "        train_start_offset = 0 if not blocking else int(i * (1 - overlap) * chunk_size)\n",
    "        train_end_offset = int((i + 1) * chunk_size)\n",
    "        val_start_offset = train_end_offset\n",
    "        val_end_offset = int(val_start_offset + chunk_size)\n",
    "\n",
    "        # Compute actual timestamps\n",
    "        train_start = min_date + timedelta(days=train_start_offset)\n",
    "        train_end = min_date + timedelta(days=train_end_offset)\n",
    "        val_start = min_date + timedelta(days=val_start_offset)\n",
    "        val_end = min_date + timedelta(days=val_end_offset)\n",
    "\n",
    "        if val_start >= max_date:\n",
    "            break\n",
    "        if val_end > max_date:\n",
    "            val_end = max_date + timedelta(days=1)\n",
    "\n",
    "        # Apply filters\n",
    "        train_df = df.filter((F.col(time_col) >= train_start) & (F.col(time_col) < train_end))\n",
    "        val_df = df.filter((F.col(time_col) >= val_start) & (F.col(time_col) < val_end))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold {i + 1}:\")\n",
    "            print(f\"  TRAIN: {train_start.date()} → {train_end.date()} ({train_df.count():,} rows)\")\n",
    "            print(f\"  VAL:   {val_start.date()} → {val_end.date()} ({val_df.count():,} rows)\")\n",
    "            print(\"------------------------------------------------------------\")\n",
    "\n",
    "        folds.append((train_df, val_df))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d203c88d-e948-404a-a643-8f83ddd44eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Testing Time Series CV function\n",
    "folds = time_series_cv_folds(\n",
    "    df_train,\n",
    "    time_col=\"sched_depart_utc\",\n",
    "    k=5,\n",
    "    overlap=.2,\n",
    "    blocking=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db18fbba-c48b-45ad-bbe5-d83ad6df3250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8a83f2-af2b-47c4-9ec3-351155909ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(cv_cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6461a9bb-a191-4922-8ac2-90b3bd130970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def timeSeriesSplitCV(df, \n",
    "                      pre_pipeline,\n",
    "                      hidden_layers,\n",
    "                      stepSize,\n",
    "                      maxIter,\n",
    "                      blockSize,\n",
    "                      cv_info= cv_cutoffs,\n",
    "                      sampling='down', \n",
    "                      metric='f2', \n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timeSeriesSplit k-fold cross validation. Params:\n",
    "  1) pre_pipeline: indexers, encoders, and vector assembler\n",
    "  2) cross validation info\n",
    "  3) hidden layer sizes in a list\n",
    "\n",
    "\n",
    "  note that the scaling+classification pipeline is initialized and fit in this method itself \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    print(f\"processing for fold {i}\")\n",
    "    ppl = pre_pipeline # hopefully avoid getting the recursive depth issue\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed','priororigin_mean_dep_delay']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed', 'priororigin_mean_dep_delay']})\n",
    "        \n",
    "    # Fit the first pipeline on the model to get feature encodings:\n",
    "\n",
    "    print(f\"fitting encoding pipeline for fold {i}\")\n",
    "\n",
    "    train_df_transformed_model = ppl.fit(train_df)\n",
    "\n",
    "    print(f\"encoding train set for fold {i}\")\n",
    "    train_df_transformed= train_df_transformed_model.transform(train_df)\n",
    "    \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_df_transformed = train_df_transformed_model.transform(dev_df)\n",
    "\n",
    "    # Fit the second pipeline on the model to get scaling and classification:\n",
    "\n",
    "    print(f\"getting layer sizes for fold {i}\")\n",
    "    layers = [train_df_transformed.first()['features'].size] + hidden_layers + [2]\n",
    "    #input features, hidden layers, classification head\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "    classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=maxIter,\n",
    "                                                stepSize=stepSize,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=blockSize,\n",
    "                                                seed=1234)\n",
    "    pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n",
    "    print(f\"fitting encoded train df for fold {i}\")\n",
    "    mlp_model = pipeline_mlp.fit(train_df_transformed.select('features','outcome'))\n",
    "\n",
    "    print(f\"transforming encoded dev df for fold {i}\")\n",
    "    dev_pred = mlp_model.transform(dev_df_transformed.select('features','outcome'))\n",
    "\n",
    "    if metric=='f2':\n",
    "      evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"outcome\", \n",
    "        metricName=\"fMeasureByLabel\",\n",
    "        beta=2.0,\n",
    "        metricLabel=1.0\n",
    "      )\n",
    "\n",
    "      score = evaluator.evaluate(dev_pred)\n",
    "\n",
    "    scores.append(score)\n",
    "    print(f'Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e15008-70c0-498d-b568-24be3abc62fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "seasonality_cols_cv = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [\"origin_HourlyDewPointTemperature\", \"origin_HourlyPrecipitation\", \"origin_HourlyWindGustSpeed\", \"origin_HourlyVisibility\", \"origin_HourlyPressureChange\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"train\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]\n",
    "numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c504707a-ab9a-4318-840f-c636928a98d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any, Union,Callable\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    RandomForestClassifier,\n",
    "    MultilayerPerceptronClassifier\n",
    ")\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "import mlflow\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a181460-ce05-453f-a0cf-55cd64d8ec60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "    encoders = [OneHotEncoder(\n",
    "        inputCol='{0}_index'.format(column), \n",
    "        outputCol='{0}_ohe'.format(column)\n",
    "        ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "    [encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "    [encoders[i].getHandleInvalid() for i in range(len(encoders))] #sanity check\n",
    "\n",
    "    # Fill missing values with 0 for the specified columns\n",
    "    # df_filled = df_train.fillna({c: 0 for c in numeric_cols_cv if c in df_train.columns})\n",
    "\n",
    "\n",
    "    featuresCreator = VectorAssembler(\n",
    "        inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "        outputCol='features', handleInvalid='skip')\n",
    "\n",
    "    stages = indexers + encoders\n",
    "\n",
    "    pre_pipeline = Pipeline(stages= stages + [featuresCreator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f833006-a043-4399-b7f2-06b0a19ec6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_tuner(\n",
    "    model_name: str,\n",
    "    model_params: Dict[str, Any],\n",
    "    mlflow_run_name: str = \"/Users/m.bakr@berkeley.edu/flight_delay_tuning\",\n",
    "    metric: str = \"F2\",\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Union[float, str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Universal tuning function for PySpark classification models using time-series cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of ['logreg', 'rf', 'mlp', 'xgb']\n",
    "        model_params (Dict[str, Any]): Parameters to apply to the model\n",
    "        folds (List of (train_df, val_df)): Time-aware CV folds\n",
    "        mlflow_run_name (str): Optional MLflow parent run name\n",
    "        verbose (bool): Whether to log outputs during tuning\n",
    "\n",
    "    Returns:\n",
    "        Dict with best average F2 or pr score, model name, and parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Model factory\n",
    "    model_factory = {\n",
    "        \"logreg\": LogisticRegression,\n",
    "        \"rf\": RandomForestClassifier,\n",
    "        \"mlp\": MultilayerPerceptronClassifier,\n",
    "        \"xgb\": SparkXGBClassifier\n",
    "    }\n",
    "\n",
    "    assert model_name in model_factory, f\"Unsupported model: {model_name}\"\n",
    "\n",
    "    ModelClass = model_factory[model_name]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "\n",
    "        indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "        encoders = [OneHotEncoder(\n",
    "        inputCol='{0}_index'.format(column), \n",
    "        outputCol='{0}_ohe'.format(column)\n",
    "        ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "        [encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "\n",
    "        featuresCreator = VectorAssembler(\n",
    "            inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "            outputCol='features', handleInvalid='skip')\n",
    "\n",
    "        stages = indexers + encoders\n",
    "\n",
    "        pre_pipeline = Pipeline(stages= stages + [featuresCreator])\n",
    "        scores = timeSeriesSplitCV(df=df_train,\n",
    "                                   pre_pipeline= pre_pipeline, **model_params)\n",
    "        \n",
    "\n",
    "        avg_score = float(np.mean(scores))\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_params(model_params)\n",
    "        mlflow.log_metric(\"avg_{metric}_score\", avg_score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"✅ Average {metric} Score: {avg_score:.4f} | Model: {model_name}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"params\": model_params,\n",
    "        \"avg_f2_score\": avg_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4080ca35-1cad-40e7-a9a9-798c0a6c0d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ad7f48-7fda-4751-9922-a98a404ba70f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d9675e1-3d38-4cbc-95f0-9104a92221c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_hyperopt_objective(\n",
    "    model_name: str,\n",
    "    param_space_converter: Callable[[Dict[str, Any]], Dict[str, Any]],\n",
    "    mlflow_experiment_name: str = \"Hyperopt_Universal_Tuning\",\n",
    "    verbose: bool = True\n",
    ") -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Creates a Hyperopt-compatible objective function for any PySpark classifier.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of 'logreg', 'rf', 'mlp', 'xgb'.\n",
    "        folds (List of (train_df, val_df)): Time-series CV folds.\n",
    "        param_space_converter (Callable): Converts Hyperopt sample into model params.\n",
    "        mlflow_experiment_name (str): MLflow experiment name.\n",
    "        verbose (bool): Logging toggle.\n",
    "\n",
    "    Returns:\n",
    "        Callable that can be passed as fn to hyperopt.fmin()\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(sampled_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Convert sampled param space to Spark-friendly params\n",
    "        model_params = param_space_converter(sampled_params)\n",
    "\n",
    "        result = model_tuner(\n",
    "            model_name=model_name,\n",
    "            model_params=model_params,\n",
    "            mlflow_run_name=f\"hyperopt_{model_name}\",\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": -result[\"avg_f2_score\"],  # Minimize negative F2\n",
    "            \"status\": STATUS_OK,\n",
    "            \"params\": result[\"params\"]\n",
    "        }\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1bf2bb2-1307-4c8c-a1d6-4da60e2d1afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_space = {\n",
    "    \"hidden_layers\": hp.choice(\"hidden_layers\", [[64, 32], [32, 8, 4], [128, 32]]),\n",
    "    \"stepSize\": hp.uniform(\"stepSize\", 0.01, 0.3),\n",
    "    \"maxIter\": hp.choice(\"maxIter\", [100, 200]),\n",
    "    \"blockSize\": hp.choice(\"blockSize\", [64, 128])\n",
    "}\n",
    "\n",
    "def mlp_param_mapper(sampled):\n",
    "    return {\n",
    "        \"hidden_layers\": list(sampled[\"hidden_layers\"]),\n",
    "        \"stepSize\": sampled[\"stepSize\"],\n",
    "        \"maxIter\": sampled[\"maxIter\"],\n",
    "        \"blockSize\": sampled[\"blockSize\"]\n",
    "    }\n",
    "\n",
    "mlp_obj = make_hyperopt_objective(\n",
    "    model_name=\"mlp\",\n",
    "    param_space_converter=mlp_param_mapper,\n",
    "    mlflow_experiment_name=\"MLP_Hyperopt\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_mlp = fmin(\n",
    "    fn=mlp_obj,\n",
    "    space=mlp_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Best MLP params:\", best_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e0c45f-292e-4f15-9c42-c7c948d1c9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Must show training and performance scores, including training curves by epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24e2222-d2b0-490d-9b75-bd6e26d4a47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbb6681e-8488-434e-88fe-e766ea67d549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d1e4f7-b7f9-497f-9905-e70caef1dd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a9e92c-2819-41e2-944a-18dedc098d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a2abbd-53fb-4df7-bc7b-c882fda32832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217cf192-2dd0-47af-bfba-b8ccf8c2acf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb6e12d-e718-4a8a-b029-db4fe9382a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getTSCVmods(df, \n",
    "                      pre_pipeline,\n",
    "                      cv_info, \n",
    "                      hidden_layers,\n",
    "                      sampling='down', \n",
    "                      metric='f2', \n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timeSeriesSplit k-fold cross validation. Params:\n",
    "  1) pre_pipeline: indexers, encoders, and vector assembler\n",
    "  2) cross validation info\n",
    "  3) hidden layer sizes in a list\n",
    "\n",
    "\n",
    "  note that the scaling+classification pipeline is initialized and fit in this method itself \n",
    "\n",
    "  returns scores and pipelines\n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  encoder_pipelines = []\n",
    "  classifier_pipelines = []\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    print(f\"processing for fold {i}\")\n",
    "    ppl = pre_pipeline # hopefully avoid getting the recursive depth issue\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        \n",
    "    # Fit the first pipeline on the model to get feature encodings:\n",
    "\n",
    "    print(f\"fitting encoding pipeline for fold {i}\")\n",
    "\n",
    "    train_df_transformed_model = ppl.fit(train_df)\n",
    "    encoder_pipelines.append(train_df_transformed_model)\n",
    "\n",
    "    print(f\"encoding train set for fold {i}\")\n",
    "    train_df_transformed= train_df_transformed_model.transform(train_df)\n",
    "    \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_df_transformed = train_df_transformed_model.transform(dev_df)\n",
    "\n",
    "    # Fit the second pipeline on the model to get scaling and classification:\n",
    "\n",
    "    print(f\"getting layer sizes for fold {i}\")\n",
    "    layers = [train_df_transformed.first()['features'].size] + hidden_layers + [2]\n",
    "    #input features, hidden layers, classification head\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "    classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=200,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=128,\n",
    "                                                stepSize=.0524,\n",
    "                                                seed=1234)\n",
    "    pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n",
    "    print(f\"fitting encoded train df for fold {i}\")\n",
    "    mlp_model = pipeline_mlp.fit(train_df_transformed.select('features','outcome'))\n",
    "    classifier_pipelines.append(mlp_model)\n",
    "    print(f\"transforming encoded dev df for fold {i}\")\n",
    "    dev_pred = mlp_model.transform(dev_df_transformed.select('features','outcome'))\n",
    "\n",
    "    if metric=='f2':\n",
    "      evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"outcome\", \n",
    "        metricName=\"fMeasureByLabel\",\n",
    "        beta=2.0,\n",
    "        metricLabel=1.0\n",
    "      )\n",
    "\n",
    "      score = evaluator.evaluate(dev_pred)\n",
    "\n",
    "    scores.append(score)\n",
    "    print(f'Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "\n",
    "  return scores, encoder_pipelines, classifier_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b1286e-4d80-499c-a794-f8833b68b6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "seasonality_cols_cv = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [\"origin_HourlyDewPointTemperature\", \"origin_HourlyPrecipitation\", \"origin_HourlyWindGustSpeed\", \"origin_HourlyVisibility\", \"origin_HourlyPressureChange\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]\n",
    "numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33be228-ebdf-4ba8-993c-3b7d201b1d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pre-pipeline\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(\n",
    "    inputCol='{0}_index'.format(column), \n",
    "    outputCol='{0}_ohe'.format(column)\n",
    "    ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "[encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "[encoders[i].getHandleInvalid() for i in range(len(encoders))] #sanity check\n",
    "\n",
    "# Fill missing values with 0 for the specified columns\n",
    "# df_filled = df_train.fillna({c: 0 for c in numeric_cols_cv if c in df_train.columns})\n",
    "\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "scores, encoding_pipelines, classifier_pipelines = getTSCVmods(\n",
    "    df_train,\n",
    "    vec_pipeline_full,\n",
    "    cv_cutoffs,\n",
    "    hidden_layers = [128, 32],\n",
    "    sampling='down',\n",
    "    metric='f2',\n",
    "    verbose=True,\n",
    "    dep_utc_varname='sched_depart_utc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13aa1b06-e4f6-41b2-81cb-93805be64667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e745ea-95e4-4b2d-baf5-c111efbd5248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2b3809-b647-4a9d-9a24-91da65917f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for idx, pipeline in enumerate(encoding_pipelines):\n",
    "    pipeline.write().overwrite().save(f\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/encoding_pipeline_{idx}\")\n",
    "\n",
    "for idx, pipeline in enumerate(classifier_pipelines):\n",
    "    pipeline.write().overwrite().save(f\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints//classifier_pipeline_{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867802f8-cf07-4716-8e7b-16b424d2bc23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, pipeline in enumerate(classifier_pipelines):\n",
    "    objective_history = pipeline.stages[-1].summary().objectiveHistory\n",
    "    plt.plot(objective_history, label=f'Fold {i}')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Objective History for Each Fold Pipeline')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2b5c3a-77ff-4b01-9014-0829a3895020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bd17f5-3da2-45c5-89c1-6e1510a26ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test0=encoding_pipelines[0].transform(df_test.withColumnsRenamed({'daily_full':'daily','weekly_full':'weekly','yearly_full':'yearly','holidays_full':'holidays'}))\n",
    "test0=classifier_pipelines[0].transform(test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d359a9ec-5dbf-4963-bd92-79328eeaa977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classifier_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09a6c9b-ed0f-4a73-8ba1-622429d9eb28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f0cc55-2e30-4f41-a668-5f407f9d9b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter=df_test.withColumnsRenamed({'daily_full':'daily','weekly_full':'weekly','yearly_full':'yearly','holidays_full':'holidays'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba7a755-3c55-4b66-be7d-cf25bd266c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_cols= numeric_cols_cv+seasonality_cols_cv+cat_cols+['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3cdae7-ff8a-42ca-8892-8b5005558494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bccf85-27a5-4695-8274-5ed1c4992819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01899107-1fbf-4bf0-8035-a7602bb4a96a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_test_encoded.columns[:-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478bfb16-cf32-4da6-a608-cc03226e519d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfa824e-a9e5-477e-9019-12bd46683859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d191aa47-4f99-4361-9788-982eb3715872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter=df_test.withColumnsRenamed({'daily_full':'daily','weekly_full':'weekly','yearly_full':'yearly','holidays_full':'holidays'})\n",
    "\n",
    "\n",
    "fold0_test_encoded = encoding_pipelines[0].transform(starter)\n",
    "fold0_test_transformed = classifier_pipelines[0] \\\n",
    "    .transform(fold0_test_encoded) \\\n",
    "    .withColumn(\"fold0_probs\", vector_to_array(\"probability\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e37034-6204-4677-b7a9-58cd20161104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_test_encoded.columns[:-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d356c3f-29ae-48fe-bf2c-233e0debb0b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_test_encoded = encoding_pipelines[1] \\\n",
    "    .transform(fold0_test_transformed.select(fold0_test_encoded.columns[:-25] + \n",
    "                                             ['fold0_probs']))\n",
    "\n",
    "fold1_test_transformed = classifier_pipelines[1] \\\n",
    "    .transform(fold1_test_encoded) \\\n",
    "    .withColumn(\"fold1_probs\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(starter.columns+\n",
    "            ['fold0_probs','fold1_probs']) #fold 1 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6481ea-b087-41a3-a851-c4d58e584b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fold1_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb579840-698e-498a-90e7-ab56efcd3db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold2_test_encoded = encoding_pipelines[2] \\\n",
    "    .transform(fold1_test_transformed.select(fold0_test_encoded.columns[:-25] + \n",
    "                                             ['fold0_probs', 'fold1_probs']))\n",
    "\n",
    "fold2_test_transformed = classifier_pipelines[2] \\\n",
    "    .transform(fold2_test_encoded) \\\n",
    "    .withColumn(\"fold2_probs\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(starter.columns+\n",
    "            ['fold0_probs','fold1_probs', 'fold2_probs']) #fold 2 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb7c02f-8ff8-46ba-9c1c-e2a2caf4280d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold3_test_encoded = encoding_pipelines[3] \\\n",
    "    .transform(fold2_test_transformed.select(fold0_test_encoded.columns[:-25] + \n",
    "                                             ['fold0_probs', 'fold1_probs', 'fold2_probs']))\n",
    "\n",
    "fold3_test_transformed = classifier_pipelines[3] \\\n",
    "    .transform(fold3_test_encoded) \\\n",
    "    .withColumn(\"fold3_probs\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(starter.columns+\n",
    "            ['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs']) #fold 3 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b428397-b487-4c0d-87de-c5bd61cc32bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold4_test_encoded = encoding_pipelines[4] \\\n",
    "    .transform(fold3_test_transformed.select(fold0_test_encoded.columns[:-25] + \n",
    "                                             ['fold0_probs', 'fold1_probs', 'fold2_probs', 'fold3_probs']))\n",
    "\n",
    "fold4_test_transformed = classifier_pipelines[4] \\\n",
    "    .transform(fold4_test_encoded) \\\n",
    "    .withColumn(\"fold4_probs\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(starter.columns+\n",
    "            ['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs', 'fold4_probs']) #fold 4 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "198889a1-4785-4932-a9dc-e8961de01abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fold4_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def815bd-9e2c-4bd5-99e3-118351e7818f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.5  # decay rate; adjust as needed\n",
    "num_folds = 5\n",
    "\n",
    "raw_weights = np.array([alpha ** (num_folds - 1 - i) for i in range(num_folds)])\n",
    "weights = raw_weights / raw_weights.sum()  # normalize to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c2fe34-5db1-4b62-bcf4-fa37824f61ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd8b7f7-4cf7-4245-8b59-c4385d03901d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ewa_expr = sum([weights[i] * col(f\"fold{i}_probs\") for i in range(num_folds)])\n",
    "\n",
    "final_df = fold4_test_transformed.withColumn(\"ewa_prob\", ewa_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691b24e8-17ef-4cd1-9ede-e29add63cedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df=final_df.withColumn('prediction', when(col('ewa_prob') >= 0.5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58507c0d-9498-4434-ba27-438d9fb96010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c4409b-2685-4fbe-b22e-67fd28aa7f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\",\n",
    "    predictionCol='prediction', \n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "evaluator.evaluate(final_df.withColumn(\"prediction\", col(\"prediction\").cast(DoubleType())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2bdccf-10a1-4351-a202-c9e7a1438b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df=final_df.withColumnsRenamed({'fold0_probs':'mlp_fold0',\n",
    "                             'fold1_probs':'mlp_fold1',\n",
    "                             'fold2_probs':'mlp_fold2',\n",
    "                             'fold3_probs':'mlp_fold3',\n",
    "                             'fold4_probs':'mlp_fold4',\n",
    "                             'ewa_prob':'mlp_ewa_prob',\n",
    "                             'prediction':'mlp_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccfe342-42e0-4dc1-a50c-e3fd9a90850c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/mlp_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3019f306-6709-4f83-81d9-c64fe691ed1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e185638a-418c-4a0d-ad88-f238630bfb58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ed568b-57e8-4ac3-bc78-140ab8622fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_dfx = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/mlp_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca192523-596f-45b1-8295-11f2f76685a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511cb1c9-b274-4ceb-a0ab-be014e94d2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e1d561-6347-4069-b93f-ffdb8c923d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"YEAR\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "keep_me = [\"outcome\",dep_utc_varname]\n",
    "\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *num_weather_cols, *graph_cols]\n",
    "# numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbd3559-fa1c-4c6a-8263-9034f8364f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_0\")\n",
    "\n",
    "fold1_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_1\")\n",
    "fold2_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_2\")\n",
    "fold3_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_3\")\n",
    "fold4_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_4\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f20975-ff0b-4a49-a0e0-fa63ec00039b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_cols = ['mlp_fold0','mlp_fold1','mlp_fold2','mlp_fold3','mlp_fold4','mlp_ewa_prob','mlp_prediction']\n",
    "filter_cols = [*keep_me, *numeric_cols, *categorical_cols, *mlp_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a8df30-8a3a-4ce5-9522-85a189850a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_dfx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025e5910-11db-4809-8158-b0d5950de111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_final_df=final_dfx.withColumnRenamed('train','pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20619f9b-754a-44f9-8a4e-d49ab964d514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(xgb_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3099a165-03b9-40b4-9250-9b0b86f9b005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_test=fold0_mod.transform(xgb_final_df).withColumn(\"fold0_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs']) #fold 0 preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a22214f-c8c4-4daa-a8eb-1602f0bfcdd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fold0_test=fold0_mod.transform(final_df.withColumnsRenamed({'train':'pagerank'})).withColumn(\"fold0_probs\", vector_to_array(\"probability\")[1]).select(filter_cols+['fold0_probs']) #fold 0 preds\n",
    "\n",
    "fold1_test=fold1_mod.transform(fold0_test).withColumn(\"fold1_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs','fold1_probs']) #fold 1 preds\n",
    "\n",
    "fold2_test=fold2_mod.transform(fold1_test).withColumn(\"fold2_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs','fold1_probs', 'fold2_probs']) #fold 2 preds\n",
    "\n",
    "fold3_test=fold3_mod.transform(fold2_test).withColumn(\"fold3_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs']) \n",
    "\n",
    "fold4_test=fold4_mod.transform(fold3_test).withColumn(\"fold4_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs', 'fold4_probs'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709b316c-d02d-4118-a444-a9a98ff026a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold4_test.checkpoint()\n",
    "display(fold4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdcef01-0ee8-49c1-ba34-c8053969f6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.5  # decay rate; adjust as needed\n",
    "num_folds = 5\n",
    "\n",
    "raw_weights = np.array([alpha ** (num_folds - 1 - i) for i in range(num_folds)])\n",
    "weights = raw_weights / raw_weights.sum()  # normalize to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e45794-ab90-4ef0-86dd-f3d90edf4056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ewa_expr = sum([weights[i] * col(f\"fold{i}_probs\") for i in range(num_folds)])\n",
    "\n",
    "xgb_final_df = fold4_test.withColumn(\"ewa_prob\", ewa_expr)\n",
    "\n",
    "xgb_final_df=xgb_final_df.withColumn('prediction', when(col('ewa_prob') >= 0.5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13891d4f-c408-4c21-9ebe-1a6763a94045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_final_df=xgb_final_df.withColumnsRenamed({'fold0_probs':'xgb_fold0',\n",
    "                             'fold1_probs':'xgb_fold1',\n",
    "                             'fold2_probs':'xgb_fold2',\n",
    "                             'fold3_probs':'xgb_fold3',\n",
    "                             'fold4_probs':'xgb_fold4',\n",
    "                             'ewa_prob':'xgb_ewa_prob',\n",
    "                             'prediction':'xgb_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5fa2354-2841-4410-add4-f87dcf62bd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffac50ea-de79-4329-a7bd-c079cd6f2517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_final_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/model_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93358ec0-7a77-40be-bc13-4173931b859c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98391559-d6e1-4cdc-aaae-ed118d29fd7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_final_df= spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/model_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa148bc-de4b-465e-b117-ab3da82e8c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4245a3f3-1bf2-4109-a691-7b2fb04ea470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_id = \"182e305fc13048549e73bcc23fb6af78\"\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "loaded_lr_model = mlflow.spark.load_model(model_uri)\n",
    "loaded_lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32ff22d7-8b08-4d38-8361-653d5d531798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c88e0b-bfbe-4747-ae99-0f34bb4811de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df=loaded_lr_model.transform(xgb_final_df.withColumnRenamed('train','pagerank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d02f6fe-6b8c-4900-8666-752f38dca748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6dd00c-36e9-4115-aae8-eb10ac7af7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df=lr_df.withColumn(\"lr_probs\", vector_to_array(\"probability\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8dfd6d-5904-418d-8bf8-ff06c9c5aa3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df=lr_df.withColumnRenamed('prediction','lr_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7add5b12-be84-4057-aa0d-60b87d173bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58acc3fb-9e9e-4fdb-8a7d-f395bbe87da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[c for c in lr_df.columns if 'mlp' in c or 'xgb' in c or 'lr' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e5e937-8641-4646-b2b1-03ff69be0aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/mlp_xgb_lr_results.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.17-sg-ensemble",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
