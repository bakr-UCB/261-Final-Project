{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa988ec4-230e-49bc-868f-55235d230db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modeling Pipeline (Experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165fe1c6-20c0-4156-8fcd-78eff30c68e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134d7e1c-a32a-4883-bdfb-ca3a7927701a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5e9bdd-2536-4ffb-b641-9c87ff506349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any, Union,Callable\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    RandomForestClassifier,\n",
    "    MultilayerPerceptronClassifier\n",
    ")\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "import mlflow\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40542d5e-46eb-4213-9a16-989eb8ee96a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6df1ab-7364-416b-8a35-70efe4d364ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variables and directories\n",
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/datasets_final_project_2022\"\n",
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/checkpoints\")\n",
    "period = \"\" # one of the following values (\"\", \"_3m\", \"_6m\", \"_1y\")\n",
    "k = 5 # cv folds\n",
    "overlap = 0.2 # cv overlap\n",
    "\n",
    "# Datasets\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined{period}_cleaned_engineered_timefeat.parquet\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned_pr_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320be5ab-c3e4-4b6e-b7ed-fc1c2c34f4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory Inspection\n",
    "display(dbutils.fs.ls(f\"{team_BASE_DIR}/interim/join_checkpoints/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd689be-1e15-4a47-aafd-210212a9ef0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP : Features Selection and Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a0b50a-5d60-4cb6-8fdf-f8ce5f174c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\"]\n",
    "\n",
    "# date related columns\n",
    "date_cols = [\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\"]\n",
    "\n",
    "# prior & current flight cols\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                ]\n",
    "\n",
    "bool_flight_cols = ['priorflight_isdeparted', \n",
    "                    'priorflight_isarrived_calc',\n",
    "                    'priorflight_isdelayed_calc',\n",
    "                    'priorflight_cancelled_true']\n",
    "\n",
    "# graph columns\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",\"sched_depart_utc\"]\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "categorical_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]\n",
    "\n",
    "features = numeric_cols + categorical_cols\n",
    "\n",
    "# features = numeric_cols + categorical_cols\n",
    "label = \"outcome\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62304ab2-aed9-4e91-9287-4d5e2920e612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP : Time-series CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00561fab-518a-493a-a18e-bf2ee77644db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_timeseries(df, time_col: str, test_start: str, test_stop: str=\"2100-01-01\", verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Splits a PySpark DataFrame into a train/test set based on a timestamp column.\n",
    "    The most recent `test_fraction` of the data (by time) is used as test set.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        time_col (str): Timestamp column name (must be sortable).\n",
    "        test_start (str): Minimum date for the test set.\n",
    "        verbose (bool): Print boundaries and sizes.\n",
    "\n",
    "    Returns:\n",
    "        (train_df, test_df): Tuple of train and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Filter df to before max date\n",
    "    df = df.filter(F.col(time_col) < test_stop)\n",
    "\n",
    "    # Get min and max time\n",
    "    min_time, max_time = df.selectExpr(f\"min({time_col})\", f\"max({time_col})\").first()\n",
    "    total_days = (max_time - min_time).days\n",
    "\n",
    "    train_df = df.filter(F.col(time_col) < test_start)\n",
    "    test_df = df.filter(F.col(time_col) >= test_start)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"ðŸ“… Total date range: {min_time.date()} â†’ {max_time.date()} ({total_days} days)\")\n",
    "        print(f\"âœ… Train: {min_time.date()} â†’ {test_start} ({train_df.count():,} rows)\")\n",
    "        print(f\"ðŸ§ª Test: {test_start} â†’ {max_time.date()} ({test_df.count():,} rows)\")\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5769db2-0f27-46d8-9e2c-61a2ea4f68c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(F.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(F.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(F.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(F.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f023b00f-7de8-4f31-afd2-7ccf8a70098a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_series_cv_folds(\n",
    "    df,\n",
    "    time_col: str,\n",
    "    k: int=3,\n",
    "    blocking: bool=False,\n",
    "    overlap: float=0.0,\n",
    "    keep_me: list=df.columns, # defines variables to keep\n",
    "    verbose: bool=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Split a time-series PySpark DataFrame into k train/test folds with optional overlap and blocking.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): PySpark DataFrame with a timestamp column.\n",
    "        dep_utc_time_colvarname (str): Name of the timestamp column.\n",
    "        k (int): Number of folds.\n",
    "        blocking (bool): Whether to block the training set to avoid cumulative data.\n",
    "        overlap (float): Fraction of overlap between validation windows (e.g. 0.2 = 20% overlap).\n",
    "        verbose (bool): Whether to print the time splits.\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_df, val_df) tuples.\n",
    "    \"\"\"\n",
    "    # Get time boundaries\n",
    "    min_date = df.select(F.min(time_col)).first()[0]\n",
    "    max_date = df.select(F.max(time_col)).first()[0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "\n",
    "    # Adjust chunk sizing\n",
    "    total_width = k + 1 - overlap * (k - 1)\n",
    "    chunk_size = int(np.ceil(n_days / total_width))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Splitting data into {k} folds with {overlap*100:.0f}% overlap\")\n",
    "        print(f\"Min date: {min_date}, Max date: {max_date}\")\n",
    "        print(f\"{chunk_size:,} days per fold\")\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        # define indices based on chunk size and overlap\n",
    "        if i == 0:\n",
    "            train_start_offset = 0\n",
    "            train_end_offset = chunk_size\n",
    "        else:\n",
    "            train_start_offset += np.ceil((1-overlap)*chunk_size)\n",
    "            train_end_offset += np.floor((1-overlap)*chunk_size)\n",
    "        val_start_offset = train_end_offset\n",
    "        val_end_offset = val_start_offset + chunk_size\n",
    "\n",
    "        # # Offset calculation with overlap\n",
    "        # train_start_offset = 0 if not blocking else int(i * (1 - overlap) * chunk_size)\n",
    "        # train_end_offset = int((i + 1) * chunk_size)\n",
    "        # val_start_offset = train_end_offset\n",
    "        # val_end_offset = int(val_start_offset + chunk_size)\n",
    "\n",
    "        # Compute actual timestamps\n",
    "        train_start = min_date + timedelta(days=train_start_offset)\n",
    "        train_end = min_date + timedelta(days=train_end_offset)\n",
    "        val_start = min_date + timedelta(days=val_start_offset)\n",
    "        val_end = min_date + timedelta(days=val_end_offset)\n",
    "\n",
    "        if val_start >= max_date:\n",
    "            break\n",
    "        if val_end > max_date:\n",
    "            val_end = max_date + timedelta(days=1)\n",
    "\n",
    "        # Apply filters\n",
    "        train_df = downsample( \\\n",
    "            df.filter((F.col(time_col) >= train_start) & (F.col(time_col) < train_end)))\n",
    "        val_df = df.filter((F.col(time_col) >= val_start) & (F.col(time_col) < val_end))\n",
    "\n",
    "        # handle fold-specific variables\n",
    "        train_df = train_df \\\n",
    "            .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "            .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "            .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "            .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "            .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "        train_df = train_df.fillna({col:0 for col in \\\n",
    "            ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        val_df = val_df \\\n",
    "            .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "            .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "            .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "            .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "            .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "        val_df = val_df.fillna({col:0 for col in \\\n",
    "            ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        \n",
    "        train_df = train_df.select(*keep_me)\n",
    "        val_df = val_df.select(*keep_me)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold {i + 1}:\")\n",
    "            print(f\"  TRAIN: {train_start.date()} â†’ {train_end.date()} ({train_df.count():,} rows)\")\n",
    "            print(f\"  VAL:   {val_start.date()} â†’ {val_end.date()} ({val_df.count():,} rows)\")\n",
    "            print(\"------------------------------------------------------------\")\n",
    "\n",
    "        folds.append((train_df, val_df))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5291c112-e9e5-4a0f-954d-531be74c2faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cv_eval(preds, metric):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  if metric == \"F2\":\n",
    "    return F2\n",
    "  else:\n",
    "      return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd84a9f-b5bf-4675-afbe-815c28481c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def cv_eval(predictions: DataFrame, label_col=\"outcome\", prediction_col=\"prediction\", metric:str=\"F2\"):\n",
    "#   \"\"\"\n",
    "#   Input: transformed df with prediction and label\n",
    "#   Output: desired score \n",
    "#   \"\"\"\n",
    "#   rdd_preds_m = predictions.select(['prediction', label_col]).rdd\n",
    "#   rdd_preds_b = predictions.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "#   metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "#   metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "#   if metric == \"F2\":\n",
    "#     score = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "#   elif metric == \"pr\":\n",
    "#     score = metrics_b.areaUnderPR\n",
    "#   return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41d7990-598b-4ea5-aff1-09b4c4677a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_tuner(\n",
    "    model_name: str,\n",
    "    model_params: Dict[str, Any],\n",
    "    stages,\n",
    "    folds: List[Tuple[DataFrame, DataFrame]],\n",
    "    mlflow_run_name: str = \"/Users/m.bakr@berkeley.edu/flight_delay_tuning\",\n",
    "    metric: str = \"F2\",\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Union[float, str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Universal tuning function for PySpark classification models using time-series cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of ['logreg', 'rf', 'mlp', 'xgb']\n",
    "        model_params (Dict[str, Any]): Parameters to apply to the model\n",
    "        folds (List of (train_df, val_df)): Time-aware CV folds\n",
    "        mlflow_run_name (str): Optional MLflow parent run name\n",
    "        verbose (bool): Whether to log outputs during tuning\n",
    "\n",
    "    Returns:\n",
    "        Dict with best average F2 or pr score, model name, and parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Model factory\n",
    "    model_factory = {\n",
    "        \"logreg\": LogisticRegression,\n",
    "        \"rf\": RandomForestClassifier,\n",
    "        \"mlp\": MultilayerPerceptronClassifier,\n",
    "        \"xgb\": SparkXGBClassifier\n",
    "    }\n",
    "\n",
    "    assert model_name in model_factory, f\"Unsupported model: {model_name}\"\n",
    "\n",
    "    ModelClass = model_factory[model_name]\n",
    "\n",
    "    # Apply required fields\n",
    "    model = ModelClass(\n",
    "        featuresCol=\"features_final\",\n",
    "        labelCol=label,\n",
    "        # weightCol=\"weight\",  # Handles imbalance\n",
    "        **model_params\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=stages + [model]) \n",
    "\n",
    "    scores = []\n",
    "\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "        for i, (train_df, val_df) in enumerate(folds):\n",
    "            fitted_model = pipeline.fit(train_df)\n",
    "            preds = fitted_model.transform(val_df)\n",
    "            score = cv_eval(preds, metric)\n",
    "            scores.append(score)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"[Fold {i+1}] {metric} Score: {score:.4f}\")\n",
    "\n",
    "            mlflow.log_metric(f\"{metric}_fold_{i+1}\", score)\n",
    "\n",
    "        avg_score = float(np.mean(scores))\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_params(model_params)\n",
    "        mlflow.log_metric(\"avg_{metric}_score\", avg_score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"âœ… Average {metric} Score: {avg_score:.4f} | Model: {model_name}\")\n",
    "\n",
    "    return model_name, avg_score, model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f2801c-bf0c-44a0-9bed-e2ca4eb7940b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_hyperopt_objective(\n",
    "    model_name: str,\n",
    "    folds: List[Tuple[DataFrame, DataFrame]],\n",
    "    stages: List,\n",
    "    param_space_converter: Callable[[Dict[str, Any]], Dict[str, Any]],\n",
    "    mlflow_experiment_name: str = \"Hyperopt_Universal_Tuning\",\n",
    "    verbose: bool = True\n",
    ") -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Creates a Hyperopt-compatible objective function for any PySpark classifier.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of 'logreg', 'rf', 'mlp', 'xgb'.\n",
    "        folds (List of (train_df, val_df)): Time-series CV folds.\n",
    "        param_space_converter (Callable): Converts Hyperopt sample into model params.\n",
    "        mlflow_experiment_name (str): MLflow experiment name.\n",
    "        verbose (bool): Logging toggle.\n",
    "\n",
    "    Returns:\n",
    "        Callable that can be passed as fn to hyperopt.fmin()\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(sampled_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Convert sampled param space to Spark-friendly params\n",
    "        model_params = param_space_converter(sampled_params)\n",
    "\n",
    "        result = model_tuner(\n",
    "            model_name=model_name,\n",
    "            model_params=model_params,\n",
    "            stages=stages,\n",
    "            folds=folds,\n",
    "            mlflow_run_name=f\"hyperopt_{model_name}\",\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        print(\"!!!\")\n",
    "        print(result)\n",
    "\n",
    "        return {\n",
    "            \"loss\": -result[1],  # Minimize negative F2\n",
    "            \"status\": STATUS_OK,\n",
    "            \"params\": result[2]\n",
    "        }\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42387b9d-111d-43a4-b6da-de46968fc2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8eb8bc-9cf8-44fc-8e20-8abd5d2400e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Testing Time Series CV function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49d2e95-4584-4a67-982d-7a0cc4ce32e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split_timeseries(\n",
    "    df=df,\n",
    "    time_col=\"sched_depart_utc\",\n",
    "    test_start=\"2019-01-01\",\n",
    "    test_stop=\"2020-01-01\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda14a23-ed8f-4524-9a6e-39b4159fd7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_cols = [*keep_me, *numeric_cols, *categorical_cols]\n",
    "\n",
    "# Testing Time Series CV function\n",
    "folds = time_series_cv_folds(\n",
    "    train_df,\n",
    "    time_col=\"sched_depart_utc\",\n",
    "    k=k,\n",
    "    overlap=overlap,\n",
    "    blocking=True,\n",
    "    keep_me=filter_cols,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33ca572-5259-40f9-8b86-f0fe9b896938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48385f3b-f583-4f25-9f16-cbb6a7da1b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Random Forest Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2632091f-478a-4dde-b22b-7291a7a6df5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "34627366-ee45-477f-a60e-98eca004b637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Define random search param grid\n",
    "# param_grid = []\n",
    "# for _ in range(10):  # 10 random configs\n",
    "#     param_grid.append({\n",
    "#         \"numTrees\": random.choice([50, 100, 200]),\n",
    "#         \"maxDepth\": random.choice([5, 10, 15]),\n",
    "#         \"featureSubsetStrategy\": random.choice([\"auto\", \"sqrt\", \"log2\"])\n",
    "#     })\n",
    "\n",
    "# # Define other stages\n",
    "# stages= [VectorAssembler(inputCols=features,outputCol=\"features_final\")]\n",
    "\n",
    "# # Run custom tuner\n",
    "# best_model, best_params, best_score = None, None, float('-inf')\n",
    "# for params in param_grid:\n",
    "#     model, score, _ = model_tuner(\n",
    "#         model_name=\"rf\",\n",
    "#         model_params=params,\n",
    "#         folds=folds,\n",
    "#         stages=stages,\n",
    "#         mlflow_run_name=\"/Users/m.bakr@berkeley.edu/flight_delay_tuning_EIL_sandbox\",\n",
    "#         verbose=True\n",
    "#     )\n",
    "#     if score > best_score:\n",
    "#         best_model, best_params, best_score = model, params, score\n",
    "\n",
    "# print(\"Best F2 Score:\", best_score)\n",
    "# print(\"Best Params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae2f8d9-cdb4-47e5-af0d-bbae3e69339f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# 1. Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# 4. Update feature list to include imputed columns\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "features = numeric_cols + categorical_vec_columns\n",
    "\n",
    "# 5. Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# 6. Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_final\"\n",
    ")\n",
    "\n",
    "stages += [assembler,scaler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bf8b88f-1648-46d8-9014-41bc9343bb67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8 workers as of 8:16 pm Eastern 4/17\n",
    "\n",
    "10 workers as of 8:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afb52e8-e579-420e-87b8-90008db2ebeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Hyperopt search space\n",
    "rf_space = {\n",
    "    \"numTrees\": hp.choice(\"numTrees\", [20, 40, 60]),\n",
    "    \"maxDepth\": hp.quniform(\"maxDepth\", 5, 12, 1),\n",
    "    \"featureSubsetStrategy\": hp.choice(\"featureSubsetStrategy\", [\"auto\", \"sqrt\", \"log2\"])\n",
    "}\n",
    "\n",
    "def rf_param_mapper(sampled: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"numTrees\": int(sampled[\"numTrees\"]),\n",
    "        \"maxDepth\": int(sampled[\"maxDepth\"]),\n",
    "        \"featureSubsetStrategy\": sampled[\"featureSubsetStrategy\"]\n",
    "    }\n",
    "\n",
    "objective = make_hyperopt_objective(\n",
    "    model_name=\"rf\",\n",
    "    folds=folds,\n",
    "    stages=stages,\n",
    "    param_space_converter=rf_param_mapper,\n",
    "    mlflow_experiment_name=\"RF_Hyperopt_Flight_Delay_EIL_sandbox\"\n",
    ")\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=rf_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best Hyperopt Config:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c26da4-20e5-453d-bd82-91aca04c615d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Train and evaluate chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7c4d17-bdd9-49c9-b151-9c9aef401cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_samp = downsample(train_df)\n",
    "train_df_samp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41cf7a4e-4cb4-4e98-9848-84de6382be2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_samp = train_df_samp \\\n",
    "    .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "    .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "    .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "    .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "    .withColumnRenamed(f\"test\",\"pagerank\")\n",
    "train_df_samp = train_df_samp.fillna({col:0 for col in \\\n",
    "    ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "test_df = test_df \\\n",
    "    .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "    .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "    .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "    .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "    .withColumnRenamed(f\"test\",\"pagerank\")\n",
    "test_df = test_df.fillna({col:0 for col in \\\n",
    "    ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "\n",
    "train_df_samp = train_df_samp.select(*filter_cols)\n",
    "test_df = test_df.select(*filter_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f5eeb8-df30-48ed-80d6-2bd631f1603d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "    featuresCol=\"features_final\",\n",
    "    labelCol=label,\n",
    "    featureSubsetStrategy=\"auto\",\n",
    "    maxDepth=10,\n",
    "    numTrees=20\n",
    "    )\n",
    "pipeline = Pipeline(stages=stages + [model])\n",
    "\n",
    "metric = \"F2\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"rf_tuned_model\"):\n",
    "    fitted_model = pipeline.fit(train_df_samp)\n",
    "    preds = fitted_model.transform(test_df)\n",
    "    score = cv_eval(preds, metric)\n",
    "\n",
    "    print(f\"[Held out test] {metric} Score: {score:.4f}\")\n",
    "\n",
    "    mlflow.log_metric(f\"{metric}_held_out_test\", score)\n",
    "\n",
    "    mlflow.log_param(\"model\", \"rf\")\n",
    "    mlflow.log_metric(\"{metric}_score\", score)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.spark.log_model(fitted_model, \"rf_model\")\n",
    "\n",
    "    # Extract prediction and label columns\n",
    "    prediction_and_labels = preds.select(\"prediction\", label).rdd.map(lambda r: (r[0], r[1]))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8564f23-71ce-4fb3-b872-b7c91dcbad15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dir(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828d0d9e-cc39-43f3-a108-b57f1fa937e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fitted_model.stages[-1].summary.fMeasureByLabel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc2938d-0df1-45dc-ae46-e0771172b5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fitted_model.stages[-1].summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45077b5f-4cab-49f7-95c1-cdd523abe432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fada31e0-5f7c-471b-ae24-4f1ecc5ec4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# logreg_space = {\n",
    "#     \"regParam\": hp.uniform(\"regParam\", 0.0, 0.5),\n",
    "#     \"elasticNetParam\": hp.uniform(\"elasticNetParam\", 0.0, 1.0)\n",
    "# }\n",
    "\n",
    "# def logreg_param_mapper(sampled):\n",
    "#     return {\n",
    "#         \"regParam\": sampled[\"regParam\"],\n",
    "#         \"elasticNetParam\": sampled[\"elasticNetParam\"],\n",
    "#         \"maxIter\": 100\n",
    "#     }\n",
    "\n",
    "# logreg_obj = make_hyperopt_objective(\n",
    "#     model_name=\"logreg\",\n",
    "#     folds=folds,\n",
    "#     param_space_converter=logreg_param_mapper,\n",
    "#     mlflow_experiment_name=\"LogReg_Hyperopt\",\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# best_logreg = fmin(\n",
    "#     fn=logreg_obj,\n",
    "#     space=logreg_space,\n",
    "#     algo=tpe.suggest,\n",
    "#     max_evals=20,\n",
    "#     trials=Trials()\n",
    "# )\n",
    "\n",
    "# print(\"Best Logistic Regression params:\", best_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69e3bfe3-0e29-49da-bdb2-912292cd9f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8cbb7cb-fc78-495b-abf8-75bd3fae71eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_space = {\n",
    "    \"eta\": hp.uniform(\"eta\", 0.01, 0.3),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "    \"num_round\": hp.quniform(\"num_round\", 50, 200, 10)\n",
    "}\n",
    "\n",
    "def xgb_param_mapper(sampled):\n",
    "    return {\n",
    "        \"eta\": sampled[\"eta\"],\n",
    "        \"max_depth\": int(sampled[\"max_depth\"]),\n",
    "        \"subsample\": sampled[\"subsample\"],\n",
    "        \"colsample_bytree\": sampled[\"colsample_bytree\"],\n",
    "        \"num_round\": int(sampled[\"num_round\"]),\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"num_workers\": 2,\n",
    "        \"verbosity\": 0\n",
    "    }\n",
    "xgb_obj = make_hyperopt_objective(\n",
    "    model_name=\"xgb\",\n",
    "    folds=folds,\n",
    "    stages=stages,\n",
    "    param_space_converter=xgb_param_mapper,\n",
    "    mlflow_experiment_name=\"XGBoost_Hyperopt\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=xgb_obj,\n",
    "    space=xgb_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Best XGBoost params:\", best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e730c8e-f4fe-4881-9098-282c1aeffd2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803abca7-ec4b-4ad3-abff-fd523b2a9025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_space = {\n",
    "    \"hidden_layers\": hp.choice(\"hidden_layers\", [[64, 32], [128, 64], [100, 50]]),\n",
    "    \"stepSize\": hp.uniform(\"stepSize\", 0.01, 0.3),\n",
    "    \"maxIter\": hp.choice(\"maxIter\", [100, 200]),\n",
    "    \"blockSize\": hp.choice(\"blockSize\", [64, 128])\n",
    "}\n",
    "\n",
    "def mlp_param_mapper(sampled):\n",
    "    return {\n",
    "        \"layers\": [input_dim] + sampled[\"hidden_layers\"] + [2],\n",
    "        \"stepSize\": sampled[\"stepSize\"],\n",
    "        \"maxIter\": sampled[\"maxIter\"],\n",
    "        \"blockSize\": sampled[\"blockSize\"]\n",
    "    }\n",
    "\n",
    "mlp_obj = make_hyperopt_objective(\n",
    "    model_name=\"mlp\",\n",
    "    folds=folds,\n",
    "    param_space_converter=mlp_param_mapper,\n",
    "    mlflow_experiment_name=\"MLP_Hyperopt\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_mlp = fmin(\n",
    "    fn=mlp_obj,\n",
    "    space=mlp_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Best MLP params:\", best_mlp)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(EIL Clone) 3.15-mas-modeling-pipeline-with-tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
