{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea57f82-909d-40c5-a5ef-ff6655abcf9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a6327e-1199-4273-86b3-bb53c01e3c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Configure Spark settings for better performance\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder\\\n",
    "#     .config(\"spark.executor.memory\", \"16g\")\\\n",
    "#     .config(\"spark.executor.cores\", 4)\\\n",
    "#     .appName('Final Project Training')\\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "# spark.conf.set(\"spark.default.parallelism\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d62c3-ed6a-47d0-bb21-3c0faee08bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, when\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0961268-eafb-4755-9d8c-7541236ade55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cbc4b7a-aaa3-4ba6-8df3-e98af097300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7e45df-c877-46e1-b5f9-fa843a68c574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de30d1e2-82a4-461f-a323-79e9ca866d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds and overlap\n",
    "k = 5\n",
    "overlap = 0.2\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup)\n",
    "compute_seasonality = False\n",
    "apply_seasonality = False\n",
    "\n",
    "# define train/test split date\n",
    "if period == \"3m\":\n",
    "    min_test_dt = \"2015-03-01\"\n",
    "elif period == \"1y\":\n",
    "    min_test_dt = \"2019-10-01\"\n",
    "elif period == \"\":\n",
    "    min_test_dt = \"2019-01-01\"\n",
    "print(f\"Min test set date for {period} dataset: {min_test_dt}\")\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7820b43-a23e-488f-a7f3-d9b07493cf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data and perform simple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "\n",
    "# read in joined, cleaned dataset\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather_{period}.parquet\") # !!!\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_weather_cleaned_combo.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather{period}_v1.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned.parquet\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned_pr_v2.parquet\")\n",
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour and date variables (needed for seasonality and CV splits, respectively)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname))) \\\n",
    "    .withColumn(\"dep_date_utc\", to_date(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5455f469-d289-4a5c-935a-ebe06e5f16d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Group by the year and count the number of records for each year\n",
    "# df_year_counts = df.groupBy(\"YEAR\").count()\n",
    "\n",
    "# # Display the result\n",
    "# display(df_year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "# df_train.cache()\n",
    "# print(f\"Train data: {df_train.count()} records\")\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt) \\\n",
    "    .filter(f.col(dep_utc_varname) < \"2020-01-01\")\n",
    "# df_test.cache()\n",
    "# print(f\"Test data: {df_test.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa30565b-2ab1-45f9-ac57-8832abc556d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get cross-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc820cf1-c179-4279-8fe0-505d1758f9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits_by_days_with_overlap(df, k=3, blocking=False, overlap=0, dep_utc_varname=dep_utc_varname, verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation, based on # days in dataset\n",
    "    '''\n",
    "    \n",
    "    min_date = df.select(f.min(\"dep_date_utc\")).collect()[0][0]\n",
    "    max_date = df.select(f.max(\"dep_date_utc\")).collect()[0][0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "    total_width = k+1 - overlap*(k-1)\n",
    "    chunk_size = np.ceil(n_days/total_width) # last chunk may be slightly smaller than the others\n",
    "\n",
    "    # idx = np.arange(0,)\n",
    "    # idx = np.arange(0,n_days,chunk_size)\n",
    "    # idx[-1] = n_days-1\n",
    "    # idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Splitting data into {k} folds with {overlap} overlap')\n",
    "        print(f'Min date: {min_date}, max date: {max_date}')\n",
    "        print(f'{chunk_size:,} days per fold')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define indices based on chunk size and overlap\n",
    "        if i == 0:\n",
    "            train_min_offset = 0\n",
    "            train_max_offset = chunk_size\n",
    "        else:\n",
    "            train_min_offset += np.ceil((1-overlap)*chunk_size)\n",
    "            train_max_offset += np.floor((1-overlap)*chunk_size)\n",
    "        test_min_offset = train_max_offset\n",
    "        test_max_offset = test_min_offset + chunk_size\n",
    "\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = min_date\n",
    "        else:\n",
    "            t_min_train = min_date + timedelta(days=train_min_offset)\n",
    "        # define maximum training time\n",
    "        t_max_train = min_date + timedelta(days=train_max_offset)\n",
    "        # define minimum test time\n",
    "        t_min_test = min_date + timedelta(days=test_min_offset)\n",
    "        # define maximum test_time\n",
    "        t_max_test = min_date + timedelta(days=test_max_offset)\n",
    "\n",
    "        if t_max_test > max_date + timedelta(1):\n",
    "            t_max_test = max_date + timedelta(1)\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        print(\"(Note that the max dates are non-inclusive)\")\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc02f46-6e68-4780-91f9-e82265e6a3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs = [\n",
    "    {\"train_min\": \"2014-12-31\", \"train_max\": \"2015-10-09\", \"test_min\": \"2015-10-09\", \"test_max\": \"2016-07-17\"},\n",
    "    {\"train_min\": \"2015-08-14\", \"train_max\": \"2016-05-21\",\"test_min\": \"2016-05-21\", \"test_max\": \"2017-02-27\"},\n",
    "    {\"train_min\": \"2016-03-27\", \"train_max\": \"2017-01-01\",\"test_min\": \"2017-01-01\", \"test_max\": \"2017-10-10\"},\n",
    "    {\"train_min\": \"2016-11-08\", \"train_max\": \"2017-08-14\",\"test_min\": \"2017-08-14\", \"test_max\": \"2018-05-23\"},\n",
    "    {\"train_min\": \"2017-06-22\", \"train_max\": \"2018-03-27\",\"test_min\": \"2018-03-27\", \"test_max\": \"2019-01-01\"}\n",
    "    ]\n",
    "cv_cutoffs = pd.DataFrame(cv_cutoffs)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # get cross-validation split times\n",
    "# cv_cutoffs = get_cv_time_limits_by_days_with_overlap(df_train.select(\"dep_date_utc\"), k=k, blocking=True, overlap=overlap,\n",
    "#     dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "# cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbb6681e-8488-434e-88fe-e766ea67d549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d1e4f7-b7f9-497f-9905-e70caef1dd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a9e92c-2819-41e2-944a-18dedc098d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a2abbd-53fb-4df7-bc7b-c882fda32832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217cf192-2dd0-47af-bfba-b8ccf8c2acf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb6e12d-e718-4a8a-b029-db4fe9382a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getTSCVmods(df, \n",
    "                      pre_pipeline,\n",
    "                      cv_info, \n",
    "                      hidden_layers,\n",
    "                      sampling='down', \n",
    "                      metric='f2', \n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timeSeriesSplit k-fold cross validation. Params:\n",
    "  1) pre_pipeline: indexers, encoders, and vector assembler\n",
    "  2) cross validation info\n",
    "  3) hidden layer sizes in a list\n",
    "\n",
    "\n",
    "  note that the scaling+classification pipeline is initialized and fit in this method itself \n",
    "\n",
    "  returns scores and pipelines\n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  encoder_pipelines = []\n",
    "  classifier_pipelines = []\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    print(f\"processing for fold {i}\")\n",
    "    ppl = pre_pipeline # hopefully avoid getting the recursive depth issue\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        \n",
    "    # Fit the first pipeline on the model to get feature encodings:\n",
    "\n",
    "    print(f\"fitting encoding pipeline for fold {i}\")\n",
    "\n",
    "    train_df_transformed_model = ppl.fit(train_df)\n",
    "    encoder_pipelines.append(train_df_transformed_model)\n",
    "\n",
    "    print(f\"encoding train set for fold {i}\")\n",
    "    train_df_transformed= train_df_transformed_model.transform(train_df)\n",
    "    \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_df_transformed = train_df_transformed_model.transform(dev_df)\n",
    "\n",
    "    # Fit the second pipeline on the model to get scaling and classification:\n",
    "\n",
    "    print(f\"getting layer sizes for fold {i}\")\n",
    "    layers = [train_df_transformed.first()['features'].size] + hidden_layers + [2]\n",
    "    #input features, hidden layers, classification head\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "    classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=200,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=128,\n",
    "                                                stepSize=.0524,\n",
    "                                                seed=1234)\n",
    "    pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n",
    "    print(f\"fitting encoded train df for fold {i}\")\n",
    "    mlp_model = pipeline_mlp.fit(train_df_transformed.select('features','outcome'))\n",
    "    classifier_pipelines.append(mlp_model)\n",
    "    print(f\"transforming encoded dev df for fold {i}\")\n",
    "    dev_pred = mlp_model.transform(dev_df_transformed.select('features','outcome'))\n",
    "\n",
    "    if metric=='f2':\n",
    "      evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"outcome\", \n",
    "        metricName=\"fMeasureByLabel\",\n",
    "        beta=2.0,\n",
    "        metricLabel=1.0\n",
    "      )\n",
    "\n",
    "      score = evaluator.evaluate(dev_pred)\n",
    "\n",
    "    scores.append(score)\n",
    "    print(f'Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "\n",
    "  return scores, encoder_pipelines, classifier_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b1286e-4d80-499c-a794-f8833b68b6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "seasonality_cols_cv = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [\"origin_HourlyDewPointTemperature\", \"origin_HourlyPrecipitation\", \"origin_HourlyWindGustSpeed\", \"origin_HourlyVisibility\", \"origin_HourlyPressureChange\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]\n",
    "numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33be228-ebdf-4ba8-993c-3b7d201b1d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pre-pipeline\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(\n",
    "    inputCol='{0}_index'.format(column), \n",
    "    outputCol='{0}_ohe'.format(column)\n",
    "    ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "[encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "[encoders[i].getHandleInvalid() for i in range(len(encoders))] #sanity check\n",
    "\n",
    "# Fill missing values with 0 for the specified columns\n",
    "# df_filled = df_train.fillna({c: 0 for c in numeric_cols_cv if c in df_train.columns})\n",
    "\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "scores, encoding_pipelines, classifier_pipelines = getTSCVmods(\n",
    "    df_train,\n",
    "    vec_pipeline_full,\n",
    "    cv_cutoffs,\n",
    "    hidden_layers = [128, 32],\n",
    "    sampling='down',\n",
    "    metric='f2',\n",
    "    verbose=True,\n",
    "    dep_utc_varname='sched_depart_utc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13aa1b06-e4f6-41b2-81cb-93805be64667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e745ea-95e4-4b2d-baf5-c111efbd5248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2b3809-b647-4a9d-9a24-91da65917f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for idx, pipeline in enumerate(encoding_pipelines):\n",
    "    pipeline.write().overwrite().save(f\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/encoding_pipeline_{idx}\")\n",
    "\n",
    "for idx, pipeline in enumerate(classifier_pipelines):\n",
    "    pipeline.write().overwrite().save(f\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints//classifier_pipeline_{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867802f8-cf07-4716-8e7b-16b424d2bc23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, pipeline in enumerate(classifier_pipelines):\n",
    "    objective_history = pipeline.stages[-1].summary().objectiveHistory\n",
    "    plt.plot(objective_history, label=f'Fold {i}')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Objective History for Each Fold Pipeline')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2b5c3a-77ff-4b01-9014-0829a3895020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bd17f5-3da2-45c5-89c1-6e1510a26ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test0=encoding_pipelines[0].transform(df_test.withColumnsRenamed({'daily_full':'daily','weekly_full':'weekly','yearly_full':'yearly','holidays_full':'holidays'}))\n",
    "test0=classifier_pipelines[0].transform(test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d359a9ec-5dbf-4963-bd92-79328eeaa977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classifier_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09a6c9b-ed0f-4a73-8ba1-622429d9eb28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f0cc55-2e30-4f41-a668-5f407f9d9b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter=df_test.withColumnsRenamed({'daily_full':'daily','weekly_full':'weekly','yearly_full':'yearly','holidays_full':'holidays'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba7a755-3c55-4b66-be7d-cf25bd266c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_cols= numeric_cols_cv+seasonality_cols_cv+cat_cols+['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3cdae7-ff8a-42ca-8892-8b5005558494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bccf85-27a5-4695-8274-5ed1c4992819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01899107-1fbf-4bf0-8035-a7602bb4a96a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_test_encoded.columns[:-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478bfb16-cf32-4da6-a608-cc03226e519d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfa824e-a9e5-477e-9019-12bd46683859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d191aa47-4f99-4361-9788-982eb3715872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "starter=df_test.withColumnsRenamed({'daily_full':'daily','weekly_full':'weekly','yearly_full':'yearly','holidays_full':'holidays'})\n",
    "\n",
    "\n",
    "fold0_test_encoded = encoding_pipelines[0].transform(starter)\n",
    "fold0_test_transformed = classifier_pipelines[0] \\\n",
    "    .transform(fold0_test_encoded) \\\n",
    "    .withColumn(\"fold0_probs\", vector_to_array(\"probability\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e37034-6204-4677-b7a9-58cd20161104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_test_encoded.columns[:-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d356c3f-29ae-48fe-bf2c-233e0debb0b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_test_encoded = encoding_pipelines[1] \\\n",
    "    .transform(fold0_test_transformed.select(fold0_test_encoded.columns[:-25] + \n",
    "                                             ['fold0_probs']))\n",
    "\n",
    "fold1_test_transformed = classifier_pipelines[1] \\\n",
    "    .transform(fold1_test_encoded) \\\n",
    "    .withColumn(\"fold1_probs\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(starter.columns+\n",
    "            ['fold0_probs','fold1_probs']) #fold 1 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6481ea-b087-41a3-a851-c4d58e584b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fold1_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb579840-698e-498a-90e7-ab56efcd3db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold2_test_encoded = encoding_pipelines[2] \\\n",
    "    .transform(fold1_test_transformed.select(fold0_test_encoded.columns[:-25] + \n",
    "                                             ['fold0_probs', 'fold1_probs']))\n",
    "\n",
    "fold2_test_transformed = classifier_pipelines[2] \\\n",
    "    .transform(fold2_test_encoded) \\\n",
    "    .withColumn(\"fold2_probs\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(starter.columns+\n",
    "            ['fold0_probs','fold1_probs', 'fold2_probs']) #fold 2 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb7c02f-8ff8-46ba-9c1c-e2a2caf4280d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold3_test_encoded = encoding_pipelines[3] \\\n",
    "    .transform(fold2_test_transformed.select(fold0_test_encoded.columns[:-25] + \n",
    "                                             ['fold0_probs', 'fold1_probs', 'fold2_probs']))\n",
    "\n",
    "fold3_test_transformed = classifier_pipelines[3] \\\n",
    "    .transform(fold3_test_encoded) \\\n",
    "    .withColumn(\"fold3_probs\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(starter.columns+\n",
    "            ['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs']) #fold 3 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b428397-b487-4c0d-87de-c5bd61cc32bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold4_test_encoded = encoding_pipelines[4] \\\n",
    "    .transform(fold3_test_transformed.select(fold0_test_encoded.columns[:-25] + \n",
    "                                             ['fold0_probs', 'fold1_probs', 'fold2_probs', 'fold3_probs']))\n",
    "\n",
    "fold4_test_transformed = classifier_pipelines[4] \\\n",
    "    .transform(fold4_test_encoded) \\\n",
    "    .withColumn(\"fold4_probs\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(starter.columns+\n",
    "            ['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs', 'fold4_probs']) #fold 4 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "198889a1-4785-4932-a9dc-e8961de01abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fold4_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def815bd-9e2c-4bd5-99e3-118351e7818f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.5  # decay rate; adjust as needed\n",
    "num_folds = 5\n",
    "\n",
    "raw_weights = np.array([alpha ** (num_folds - 1 - i) for i in range(num_folds)])\n",
    "weights = raw_weights / raw_weights.sum()  # normalize to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c2fe34-5db1-4b62-bcf4-fa37824f61ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd8b7f7-4cf7-4245-8b59-c4385d03901d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ewa_expr = sum([weights[i] * col(f\"fold{i}_probs\") for i in range(num_folds)])\n",
    "\n",
    "final_df = fold4_test_transformed.withColumn(\"ewa_prob\", ewa_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691b24e8-17ef-4cd1-9ede-e29add63cedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df=final_df.withColumn('prediction', when(col('ewa_prob') >= 0.5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58507c0d-9498-4434-ba27-438d9fb96010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c4409b-2685-4fbe-b22e-67fd28aa7f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\",\n",
    "    predictionCol='prediction', \n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "evaluator.evaluate(final_df.withColumn(\"prediction\", col(\"prediction\").cast(DoubleType())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2bdccf-10a1-4351-a202-c9e7a1438b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df=final_df.withColumnsRenamed({'fold0_probs':'mlp_fold0',\n",
    "                             'fold1_probs':'mlp_fold1',\n",
    "                             'fold2_probs':'mlp_fold2',\n",
    "                             'fold3_probs':'mlp_fold3',\n",
    "                             'fold4_probs':'mlp_fold4',\n",
    "                             'ewa_prob':'mlp_ewa_prob',\n",
    "                             'prediction':'mlp_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccfe342-42e0-4dc1-a50c-e3fd9a90850c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/mlp_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3019f306-6709-4f83-81d9-c64fe691ed1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a confusion matrix\n",
    "confusion_matrix = final_df.groupBy(\"outcome\", \"mlp_prediction\").count()\n",
    "\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e185638a-418c-4a0d-ad88-f238630bfb58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ed568b-57e8-4ac3-bc78-140ab8622fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_dfx = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/mlp_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca192523-596f-45b1-8295-11f2f76685a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511cb1c9-b274-4ceb-a0ab-be014e94d2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e1d561-6347-4069-b93f-ffdb8c923d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"YEAR\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "keep_me = [\"outcome\",dep_utc_varname]\n",
    "\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *num_weather_cols, *graph_cols]\n",
    "# numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbd3559-fa1c-4c6a-8263-9034f8364f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_0\")\n",
    "\n",
    "fold1_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_1\")\n",
    "fold2_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_2\")\n",
    "fold3_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_3\")\n",
    "fold4_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_4\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "050f36a3-35db-471c-990f-504d6500b4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_mod.stages[-1].get_booster().feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f20975-ff0b-4a49-a0e0-fa63ec00039b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_cols = ['mlp_fold0','mlp_fold1','mlp_fold2','mlp_fold3','mlp_fold4','mlp_ewa_prob','mlp_prediction']\n",
    "filter_cols = [*keep_me, *numeric_cols, *categorical_cols, *mlp_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a8df30-8a3a-4ce5-9522-85a189850a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_dfx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025e5910-11db-4809-8158-b0d5950de111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_final_df=final_dfx.withColumnRenamed('train','pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20619f9b-754a-44f9-8a4e-d49ab964d514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(xgb_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3099a165-03b9-40b4-9250-9b0b86f9b005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_test=fold0_mod.transform(xgb_final_df).withColumn(\"fold0_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs']) #fold 0 preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6ebe49-6ffe-4aa1-91fd-c9a53a55eac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fold0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a22214f-c8c4-4daa-a8eb-1602f0bfcdd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fold0_test=fold0_mod.transform(final_df.withColumnsRenamed({'train':'pagerank'})).withColumn(\"fold0_probs\", vector_to_array(\"probability\")[1]).select(filter_cols+['fold0_probs']) #fold 0 preds\n",
    "\n",
    "fold1_test=fold1_mod.transform(fold0_test).withColumn(\"fold1_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs','fold1_probs']) #fold 1 preds\n",
    "\n",
    "fold2_test=fold2_mod.transform(fold1_test).withColumn(\"fold2_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs','fold1_probs', 'fold2_probs']) #fold 2 preds\n",
    "\n",
    "fold3_test=fold3_mod.transform(fold2_test).withColumn(\"fold3_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs']) \n",
    "\n",
    "fold4_test=fold4_mod.transform(fold3_test).withColumn(\"fold4_probs\", vector_to_array(\"probability\")[1]).select(xgb_final_df.columns+['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs', 'fold4_probs'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709b316c-d02d-4118-a444-a9a98ff026a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold4_test.checkpoint()\n",
    "display(fold4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdcef01-0ee8-49c1-ba34-c8053969f6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.5  # decay rate; adjust as needed\n",
    "num_folds = 5\n",
    "\n",
    "raw_weights = np.array([alpha ** (num_folds - 1 - i) for i in range(num_folds)])\n",
    "weights = raw_weights / raw_weights.sum()  # normalize to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e45794-ab90-4ef0-86dd-f3d90edf4056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ewa_expr = sum([weights[i] * col(f\"fold{i}_probs\") for i in range(num_folds)])\n",
    "\n",
    "xgb_final_df = fold4_test.withColumn(\"ewa_prob\", ewa_expr)\n",
    "\n",
    "xgb_final_df=xgb_final_df.withColumn('prediction', when(col('ewa_prob') >= 0.5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13891d4f-c408-4c21-9ebe-1a6763a94045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_final_df=xgb_final_df.withColumnsRenamed({'fold0_probs':'xgb_fold0',\n",
    "                             'fold1_probs':'xgb_fold1',\n",
    "                             'fold2_probs':'xgb_fold2',\n",
    "                             'fold3_probs':'xgb_fold3',\n",
    "                             'fold4_probs':'xgb_fold4',\n",
    "                             'ewa_prob':'xgb_ewa_prob',\n",
    "                             'prediction':'xgb_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df9fb7f-ab6c-4129-866b-56f76fd2b1fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(xgb_final_df.select('outcome','mlp_prediction','xgb_prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffac50ea-de79-4329-a7bd-c079cd6f2517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_final_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgb_mlp_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b657f251-c4c1-410a-8c91-dd6e38915394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80e060b-f99a-481f-8b14-a92d500efa9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds = [fold0_mod,fold1_mod, fold2_mod, fold3_mod, fold4_mod]  # Add all fold models here\n",
    "feature_importance_dfs = []\n",
    "\n",
    "for fold_mod in folds:\n",
    "    va = fold_mod.stages[-3]\n",
    "    tree = fold_mod.stages[-1]\n",
    "    mappings = list(zip(va.getInputCols(), tree.get_feature_importances().keys()))\n",
    "    avg_gain = pd.DataFrame(list(tree.get_feature_importances('gain').items()), columns=['id', 'avg_gain'])\n",
    "    weight = pd.DataFrame(list(tree.get_feature_importances('weight').items()), columns=['id', 'weight'])\n",
    "    avg_cover = pd.DataFrame(list(tree.get_feature_importances('cover').items()), columns=['id', 'avg_cover'])\n",
    "\n",
    "    feature_importance_df = avg_gain.merge(weight, on='id').merge(avg_cover, on='id')\n",
    "    mappings_df = pd.DataFrame(mappings, columns=['name', 'id'])\n",
    "    feature_importance_df = feature_importance_df.merge(mappings_df, on='id')\n",
    "    feature_importance_dfs.append(feature_importance_df)\n",
    "\n",
    "# Combine all feature importance dataframes if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd38d665-8d3f-4589-a608-ce7885d1acd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, feature_importance_df in enumerate(feature_importance_dfs):\n",
    "    # Top 10 features by avg_gain\n",
    "    top_gain = feature_importance_df.nlargest(10, 'avg_gain')\n",
    "\n",
    "    # Top 10 features by weight\n",
    "    top_weight = feature_importance_df.nlargest(10, 'weight')\n",
    "\n",
    "    # Top 10 features by avg_cover\n",
    "    top_cover = feature_importance_df.nlargest(10, 'avg_cover')\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Plot for avg_gain\n",
    "    axes[0].barh(top_gain['name'][::-1], top_gain['avg_gain'][::-1], color='skyblue')\n",
    "    axes[0].set_title(f'Fold {i+1} - Top 10 Features by Average Gain')\n",
    "    axes[0].set_xlabel('Average Gain')\n",
    "    axes[0].set_ylabel('Features')\n",
    "\n",
    "    # Plot for weight\n",
    "    axes[1].barh(top_weight['name'][::-1], top_weight['weight'][::-1], color='lightgreen')\n",
    "    axes[1].set_title(f'Fold {i+1} - Top 10 Features by Weight')\n",
    "    axes[1].set_xlabel('Weight')\n",
    "    axes[1].set_ylabel('Features')\n",
    "\n",
    "    # Plot for avg_cover\n",
    "    axes[2].barh(top_cover['name'][::-1], top_cover['avg_cover'][::-1], color='salmon')\n",
    "    axes[2].set_title(f'Fold {i+1} - Top 10 Features by Average Cover')\n",
    "    axes[2].set_xlabel('Average Cover')\n",
    "    axes[2].set_ylabel('Features')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93358ec0-7a77-40be-bc13-4173931b859c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98391559-d6e1-4cdc-aaae-ed118d29fd7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_final_df= spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgb_mlp_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa148bc-de4b-465e-b117-ab3da82e8c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c9edf2-e473-41b6-b23c-78caa7c29d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "lr_model = 'runs:/895cf8de5c794d7fb932457e7b48b831/model'\n",
    "\n",
    "# Load model\n",
    "lr_mod = mlflow.spark.load_model(lr_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c88e0b-bfbe-4747-ae99-0f34bb4811de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df=lr_mod.transform(xgb_final_df.withColumnRenamed('train','pagerank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6dd00c-36e9-4115-aae8-eb10ac7af7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df=lr_df.withColumn(\"lr_probs\", vector_to_array(\"probability\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8dfd6d-5904-418d-8bf8-ff06c9c5aa3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lr_df=lr_df.withColumnRenamed('prediction','lr_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7add5b12-be84-4057-aa0d-60b87d173bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20344470-4f32-4adc-b8bd-40a8764e5917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df = lr_df.select(xgb_final_df.columns + ['lr_prediction','lr_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f908ee-7bf6-4e0e-a636-7352ce50e3dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_df=lr_df.withColumn(\"lr_probs\", vector_to_array(\"probability\")[1])\n",
    "lr_df=lr_df.withColumnRenamed('prediction','lr_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5cf01ec-c2c0-4b0d-9613-dc3d5eb470c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c9deea8-5ebf-4918-998a-7d78f47ec3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "rf_model = 'runs:/d62a6556ba1b42f5a9285c462a11ac33/rf_model'\n",
    "\n",
    "# Load model\n",
    "rf_mod = mlflow.spark.load_model(rf_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7cfa5b-c484-4933-bc00-c13eb30417bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(rf_mod.stages[-3].getInputCols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09972d97-cffe-4e1f-94c2-b4c38225b200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_df=rf_mod.transform(lr_df.withColumnRenamed('train','pagerank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8b4c4b3-ffe9-4cf1-8be0-97ec59d7d940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_df=rf_df.withColumn(\"rf_probs\", vector_to_array(\"probability\")[1])\n",
    "rf_df=rf_df.withColumnRenamed('prediction','rf_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e66a1d-00eb-401f-a316-cacc4eb02d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cdccaab-0ce4-4a77-a218-75dc16e3e81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58acc3fb-9e9e-4fdb-8a7d-f395bbe87da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Assuming lr_df has the columns 'mlp_prediction', 'xgb_prediction', and 'lr_prediction'\n",
    "avg_prediction_df = rf_df.withColumn(\n",
    "    \"avg_prediction\",\n",
    "    round((col(\"mlp_prediction\") + col(\"xgb_prediction\") + col(\"lr_prediction\") + col('rf_prediction')) / 4)\n",
    ")\n",
    "\n",
    "display(avg_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80df4602-b951-47f7-8ac8-f96b8d42841d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\",\n",
    "    predictionCol='avg_prediction', \n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "evaluator.evaluate(avg_prediction_df.withColumn(\"avg_prediction\", col(\"avg_prediction\").cast(DoubleType())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd4f206-c5e6-476f-ae00-ada7b1d7d76a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19a7e42-9b0d-42a8-9d61-231b4d5ab6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Assuming lr_df has the columns 'mlp_prediction', 'xgb_prediction', and 'lr_prediction'\n",
    "avg_prediction_df = rf_df.withColumn(\n",
    "    \"avg_prediction\",\n",
    "    f.when(round((col(\"mlp_ewa_prob\") + col(\"xgb_ewa_prob\") + col(\"lr_probs\") + col('rf_probs')) / 4) >= .5, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "display(avg_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6085e1f0-d5ab-4649-8b3f-ed0df357c3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\",\n",
    "    predictionCol='avg_prediction', \n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "evaluator.evaluate(avg_prediction_df.withColumn(\"avg_prediction\", col(\"avg_prediction\").cast(DoubleType())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e5e937-8641-4646-b2b1-03ff69be0aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/mlp_xgb_lr_rf_results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83fd4def-a2a1-440b-a687-a2b65d9414a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ensemble CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0f1367-55db-47b4-8333-fb28030bfc2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_val_preds_MLP(df, encoding_pipelines, classifier_pipelines,\n",
    "                      cv_info,\n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  For the test portion of each CV fold, transform it with the already trained encoder/classifier to get the predictions\n",
    "  return the df with predictions which will then be used to get the other models' preds\n",
    "\n",
    "  input: train_df\n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "\n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_df_transformed = encoding_pipelines[i].transform(dev_df)\n",
    "\n",
    "    print(f\"transforming encoded dev df for fold {i}\")\n",
    "    dev_pred = classifier_pipelines[i].transform(dev_df_transformed)\n",
    "    dev_pred = dev_pred.withColumnsRenamed({\"daily\": f\"daily_{i}\",\n",
    "                                            \"weekly\": f\"weekly_{i}\",\n",
    "                                            \"yearly\": f\"yearly_{i}\",\n",
    "                                            \"holidays\": f\"holidays_{i}\",\n",
    "                                            \"pagerank\": f\"train_{i}\"})\n",
    "\n",
    "    if i == 0:\n",
    "        all_dev_preds = dev_pred\n",
    "    else:\n",
    "        all_dev_preds = all_dev_preds.union(dev_pred)\n",
    "  return all_dev_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1bac83-3f12-44b6-85db-ca3de1fa11a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_df = get_val_preds_MLP(df_train,\n",
    "                  encoding_pipelines,\n",
    "                  classifier_pipelines,\n",
    "                  cv_cutoffs)\n",
    "dev_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35b60f6-ff5e-4927-8e35-b3141f1132fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01e236c-26f1-42c8-a046-cbe50b7ee4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classifier_pipelines[0].stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203fb7c1-f6b9-40c2-9a65-46488f1294f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\",\n",
    "    predictionCol='prediction',\n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0)\n",
    "\n",
    "evaluator.evaluate(dev_df.withColumn(\"prediction\", col(\"prediction\").cast(DoubleType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464e2025-ac86-4466-9507-654a8e702d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_df.checkpoint() #since i have to do this a bunch more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d1d745-957d-424d-9b8b-5e9be38cbdad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_df_next= dev_df.select(df_train.columns + ['prediction']).withColumnRenamed('prediction','mlp_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a861cbe-c91b-4d1c-a048-9fde93ff4321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_val_preds_XGB(df, \n",
    "                     mod_pipelines,\n",
    "                      cv_info = cv_cutoffs,\n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  For the test portion of each CV fold, transform it with the already trained encoder/classifier to get the predictions\n",
    "  return the df with predictions which will then be used to get the other models' preds\n",
    "\n",
    "  input: train_df\n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "\n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_pred = mod_pipelines[i].transform(dev_df)\n",
    "\n",
    "    dev_pred = dev_pred.withColumnsRenamed({\"daily\": f\"daily_{i}\",\n",
    "                                            \"weekly\": f\"weekly_{i}\",\n",
    "                                            \"yearly\": f\"yearly_{i}\",\n",
    "                                            \"holidays\": f\"holidays_{i}\",\n",
    "                                            \"pagerank\": f\"train_{i}\",\n",
    "                                            \"prediction\": \"xgb_prediction\",\n",
    "                                            \"probability\": \"xgb_probability\"})\n",
    "\n",
    "    if i == 0:\n",
    "        all_dev_preds = dev_pred\n",
    "    else:\n",
    "        all_dev_preds = all_dev_preds.union(dev_pred)\n",
    "  return all_dev_preds.select(df.columns + [\"xgb_prediction\", \"xgb_probability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9928ecb5-1191-4586-868b-46b7119f25ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgb_pipelines= [fold0_mod, fold1_mod, fold2_mod, fold3_mod, fold4_mod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac60725d-c313-44d3-b4c5-c0725393d142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_df_xgb = get_val_preds_XGB(dev_df_next, xgb_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7ea21e-49ac-4014-ad74-f84109198fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_df_xgb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095a6a5d-de82-43af-a10d-caf835f68810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_df_xgb.filter(f.col('sched_depart_utc') > cv_cutoffs.loc[0]['test_min']).filter(f.col('sched_depart_utc') <= cv_cutoffs.loc[0]['test_max']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022ff37e-eee0-4085-b011-0761b8c30f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e84a7f41-b459-44b3-baa8-23def3264387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eaa627d-3ad4-437b-b8f3-8d8a320f69f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b08e7f-ebb3-4142-adb5-cd19736c8769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_prediction_df.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f101b52b-abe0-4e52-b053-87bf7704eee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_prediction_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/ensemble.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4803d3a-3edb-433e-b011-e43abcaba255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len('_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1262b49b-9a63-4bec-b43d-0f738de29ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'mlp_prediction'[:-11].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce809a25-a5e9-498b-960d-4ac33a6d7772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to plot confusion matrix in percentage/proportion\n",
    "def plot_confusion_matrix(predictions, label_col, prediction_col):\n",
    "    # Cast prediction column to DoubleType\n",
    "    predictions = predictions.withColumn(prediction_col, col(prediction_col).cast(\"double\"))\n",
    "    \n",
    "    # Select only the columns of interest\n",
    "    prediction_and_labels = predictions.select(prediction_col, label_col).rdd.map(tuple)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    metrics = MulticlassMetrics(prediction_and_labels)\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    \n",
    "    # Normalize the confusion matrix to get proportions\n",
    "    confusion_matrix = confusion_matrix / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Get unique labels from the DataFrame\n",
    "    labels = predictions.select(label_col).distinct().orderBy(label_col).rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Convert to pandas dataframe for easier plotting\n",
    "    cm_df = pd.DataFrame(confusion_matrix, index=labels, columns=labels)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt=\".2%\", cmap=\"Blues\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix: {prediction_col[:-11].upper()}')\n",
    "    plt.show()\n",
    "\n",
    "# List of prediction columns\n",
    "prediction_columns = [col for col in avg_prediction_df.columns if col.endswith('_prediction')]\n",
    "\n",
    "# Plot confusion matrix for each prediction column\n",
    "for prediction_col in prediction_columns:\n",
    "    plot_confusion_matrix(avg_prediction_df, \"outcome\", prediction_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfc51c8-afcf-47e7-9b45-10de613da057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_prediction_df.filter(col('avg_prediction')==0).filter(col(\"outcome\")==1).groupBy('CANCELLED').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f77cb39-dfcb-412e-a549-3d46cf548d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_prediction_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361128b6-c516-4627-b6b0-0653a02ff1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_prediction_df.filter(col('avg_prediction')==0).filter(col(\"outcome\")==1).groupBy('QUARTER').count().orderBy(f.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66601b9-c0f6-449e-9199-d767fd32f65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filter false negatives\n",
    "false_negatives_df = avg_prediction_df.filter((col('avg_prediction') == 0) & (col('outcome') == 1))\n",
    "\n",
    "# Filter false positives\n",
    "false_positives_df = avg_prediction_df.filter((col('avg_prediction') == 1) & (col('outcome') == 0))\n",
    "\n",
    "# Filter true negatives\n",
    "true_negatives_df = avg_prediction_df.filter((col('avg_prediction') == 0) & (col('outcome') == 0))\n",
    "\n",
    "# Filter true positives\n",
    "true_positives_df = avg_prediction_df.filter((col('avg_prediction') == 1) & (col('outcome') == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d33cb69-23b8-4c7e-8743-cc7c73cea723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group by QUARTER and count\n",
    "false_negatives_quarter_df = false_negatives_df.groupBy('QUARTER').count().withColumnRenamed('count', 'false_negatives')\n",
    "false_positives_quarter_df = false_positives_df.groupBy('QUARTER').count().withColumnRenamed('count', 'false_positives')\n",
    "true_negatives_quarter_df = true_negatives_df.groupBy('QUARTER').count().withColumnRenamed('count', 'true_negatives')\n",
    "true_positives_quarter_df = true_positives_df.groupBy('QUARTER').count().withColumnRenamed('count', 'true_positives')\n",
    "\n",
    "# Join dataframes on QUARTER\n",
    "time_series_df = false_negatives_quarter_df.join(false_positives_quarter_df, 'QUARTER', 'outer') \\\n",
    "                                           .join(true_negatives_quarter_df, 'QUARTER', 'outer') \\\n",
    "                                           .join(true_positives_quarter_df, 'QUARTER', 'outer') \\\n",
    "                                           .orderBy('QUARTER')\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "time_series_pd = time_series_df.toPandas()\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(time_series_pd['QUARTER'], time_series_pd['false_negatives'], label='False Negatives', marker='o')\n",
    "plt.plot(time_series_pd['QUARTER'], time_series_pd['false_positives'], label='False Positives', marker='o')\n",
    "plt.plot(time_series_pd['QUARTER'], time_series_pd['true_negatives'], label='True Negatives', marker='o')\n",
    "plt.plot(time_series_pd['QUARTER'], time_series_pd['true_positives'], label='True Positives', marker='o')\n",
    "plt.title('Time Series of Predictions')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22cab2c7-beff-4584-8997-01cf094ad4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "false_negatives_day_df = false_negatives_df.withColumn('DAY', col('FL_DATE').cast('date')).groupBy('DAY').count().withColumnRenamed('count', 'false_negatives')\n",
    "false_positives_day_df = false_positives_df.withColumn('DAY', col('FL_DATE').cast('date')).groupBy('DAY').count().withColumnRenamed('count', 'false_positives')\n",
    "true_negatives_day_df = true_negatives_df.withColumn('DAY', col('FL_DATE').cast('date')).groupBy('DAY').count().withColumnRenamed('count', 'true_negatives')\n",
    "true_positives_day_df = true_positives_df.withColumn('DAY', col('FL_DATE').cast('date')).groupBy('DAY').count().withColumnRenamed('count', 'true_positives')\n",
    "\n",
    "# Join dataframes on DAY\n",
    "time_series_day_df = false_negatives_day_df.join(false_positives_day_df, 'DAY', 'outer') \\\n",
    "                                           .join(true_negatives_day_df, 'DAY', 'outer') \\\n",
    "                                           .join(true_positives_day_df, 'DAY', 'outer') \\\n",
    "                                           .orderBy('DAY')\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "time_series_day_pd = time_series_day_df.toPandas()\n",
    "\n",
    "\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(time_series_day_pd['DAY'], time_series_day_pd['false_negatives'], label='False Negatives', color='darkblue')\n",
    "plt.plot(time_series_day_pd['DAY'], time_series_day_pd['false_positives'], label='False Positives', color='darkred')\n",
    "# plt.plot(time_series_day_pd['DAY'], time_series_day_pd['true_negatives'], label='True Negatives')\n",
    "# plt.plot(time_series_day_pd['DAY'], time_series_day_pd['true_positives'], label='True Positives')\n",
    "\n",
    "# Formatting\n",
    "plt.title('Incorrect Predictions by Day and Type')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Count of Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis major ticks to every month\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=15))\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%b-%Y'))  # Format: 01-Jan-2025\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for readability\n",
    "plt.tight_layout()       # Avoid label cutoff\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f91856-259a-41b9-aaab-452bfa991ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "correct_mlp_df = avg_prediction_df.filter((col('mlp_prediction') == col('outcome')) &\n",
    "                                       (col('lr_prediction') != col('outcome')) &\n",
    "                                       (col('rf_prediction') != col('outcome')) &\n",
    "                                       (col('xgb_prediction') != col('outcome')))\n",
    "\n",
    "display(correct_mlp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2595496d-d11d-473a-95e7-6a1f56625b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(avg_prediction_df.groupBy('FL_DATE').count().orderBy(f.col('COUNT').desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "049930e6-6d2d-4540-b2fc-4707b9182f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_prediction_df.filter(col('avg_prediction')==0).filter(col(\"outcome\")==1).groupBy('YEAR').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab7b9be3-ee30-4575-a523-7aa40f1ca8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.18-sg-ensemble",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
