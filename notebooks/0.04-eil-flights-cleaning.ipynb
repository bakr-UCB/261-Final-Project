{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "242b11e6-dc13-4cb9-878a-09819a738b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Flights data cleaning\n",
    "Erica Landreth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99f26655-f249-48f3-a3e3-cf69842e276f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtering to relevant rows/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224029e0-3bde-4598-8895-8463b38c5e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airline Data (1 year)\n",
    "df_flights = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_1y/\")\n",
    "shape_orig = (df_flights.count(), len(df_flights.dtypes))\n",
    "display(df_flights)\n",
    "print(f\"Original shape: {shape_orig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d74631-9a9d-4443-b020-576c94723719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## define columns to drop\n",
    "# columns related to diversion: not enough data to use the diversion info\n",
    "div_cols = [col for col in df_flights.columns if col.startswith('DIV') and col != \"DIVERTED\"]\n",
    "# redundant carrier ID's (EDA indicated that OP_UNIQUE_CARRIER is sufficient)\n",
    "xtra_carrier_cols = [\"OP_CARRIER_AIRLINE_ID\",\"OP_CARRIER\"]\n",
    "# redundant airport ID's (EDA indicated that ORIGIN/DEST and *_AIRPORT_SEQ_ID are sufficient)\n",
    "xtra_airport_cols = [ \\\n",
    "  \"ORIGIN_AIRPORT_ID\",\"ORIGIN_CITY_MARKET_ID\",\"ORIGIN_STATE_ABR\",\"ORIGIN_STATE_FIPS\",\"ORIGIN_STATE_NM\",\"ORIGIN_WAC\", \\\n",
    "  \"DEST_AIRPORT_ID\",\"DEST_CITY_MARKET_ID\",\"DEST_STATE_ABR\",\"DEST_STATE_FIPS\",\"DEST_STATE_NM\",\"DEST_WAC\"]\n",
    "# redundant flight info (could be recreated if need be)\n",
    "xtra_flight_cols = [\"WHEELS_OFF\",\"WHEELS_ON\",\"FLIGHTS\",\"ACTUAL_ELAPSED_TIME\",\"DISTANCE_GROUP\"]\n",
    "# redundant delay status info (could be recreated if need be)\n",
    "xtra_time_cols = [\"DEP_TIME\",\"DEP_DELAY_NEW\",\"DEP_DEL15\",\"DEP_DELAY_GROUP\",\"ARR_TIME\",\"ARR_DELAY_NEW\",\"ARR_DEL15\",\"ARR_DELAY_GROUP\"]\n",
    "\n",
    "## fields to keep\n",
    "# core features: useful for ML features and/or feature engineering\n",
    "core_feats = [\"FL_DATE\",\"OP_UNIQUE_CARRIER\",\"TAIL_NUM\",\"OP_CARRIER_FL_NUM\",\"ORIGIN\",\"DEST\",\"CRS_DEP_TIME\",\"DEP_DELAY\",\"CRS_ARR_TIME\",\"ARR_DELAY\",\"CANCELLED\",\"DIVERTED\",\"CRS_ELAPSED_TIME\",\"AIR_TIME\",\"DISTANCE\"]\n",
    "# we may or may not end up using these, but they can't easily be recreated later, so we'll keep them to be cautious\n",
    "on_the_fence = [\"ORIGIN_AIRPORT_SEQ_ID\",\"DEST_AIRPORT_SEQ_ID\",\"TAXI_OUT\",\"TAXI_IN\"]\n",
    "# useful for time series analysis\n",
    "time_series = [\"QUARTER\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"DEP_TIME_BLK\",\"ARR_TIME_BLK\",\"YEAR\"]\n",
    "# useful to sanity check that joins are successful\n",
    "sanity_check = [\"ORIGIN_CITY_NAME\",\"DEST_CITY_NAME\"]\n",
    "# provides reasoning for cancellations, delays, and returns to gate\n",
    "delay_info = [col for col in df_flights.columns if col.endswith(\"_DELAY\") and col not in core_feats] + [\"CANCELLATION_CODE\"] + [\"FIRST_DEP_TIME\",\"LONGEST_ADD_GTIME\",\"TOTAL_ADD_GTIME\"]\n",
    "    # Note: cancellation codes are: \"A\" for carrier-caused, \"B\" for weather, \"C\" for National Aviation System, and \"D\" for security\n",
    "\n",
    "all_cols = div_cols+xtra_carrier_cols+xtra_airport_cols+xtra_flight_cols+xtra_time_cols+core_feats+on_the_fence+time_series+sanity_check+delay_info\n",
    "\n",
    "missing = [col for col in df_flights.columns if col not in all_cols]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e308a2-fe19-429e-9b23-4c8f8a4a8143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define columns to keep\n",
    "keep_me = core_feats + on_the_fence + time_series + sanity_check + delay_info\n",
    "keep_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99af8fa5-07f6-45e6-a61f-b7ce5fde6800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter to columns of interest, and de-dupe\n",
    "df_flights = df_flights.select(keep_me).distinct()\n",
    "\n",
    "shape_filt = (df_flights.count(), len(df_flights.dtypes))\n",
    "display(df_flights)\n",
    "print(f\"Filtered shape: {shape_filt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f2c94c-10f2-4433-9938-2fa2d3347639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sanity check: we expect half the records after de-dupe\n",
    "shape_orig[0]/shape_filt[0] == 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183da413-8777-4f63-befa-15782664a382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Converting flight data to UTC time zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c1dd8dd-5afe-40ae-b954-fd1fe335c41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create time zone reference file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2395a85-0408-4bb5-98b4-a19272e85cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The below code was used to look up the time zone for each airport. The resulting time zone info was saved out to parquet, so from this point on, just load the time zone parquet (see below for path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba380239-d0bb-42ff-b393-d56461f65dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install timezonefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873c32a9-8b38-468f-b472-298ec8e25e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # imports\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "# from timezonefinder import TimezoneFinder\n",
    "# import pytz\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # load stations data\n",
    "# df_stations = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/stations_with_neighbors.parquet/\")\n",
    "\n",
    "# # get unique airport info from stations table\n",
    "# df_locs = df_stations.select('neighbor_call','neighbor_lat','neighbor_lon').distinct()\n",
    "# display(df_locs)\n",
    "\n",
    "# # define function to look up time zones\n",
    "# def find_timezone(lat, lng):\n",
    "#     tf = TimezoneFinder()\n",
    "#     timezone_str = tf.timezone_at(lat=lat, lng=lng)\n",
    "#     return timezone_str if timezone_str else \"Unknown\"\n",
    "\n",
    "# # define udf for time zone lookup\n",
    "# find_timezone_udf = udf(find_timezone, StringType())\n",
    "\n",
    "# # add time zone column\n",
    "# df_locs = df_locs.withColumn(\"timezone\", find_timezone_udf(col(\"neighbor_lat\"), col(\"neighbor_lon\")))\n",
    "\n",
    "# # save df_time zone info as a parquet file\n",
    "# folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "# df_locs.write.parquet(f\"{folder_path}/external/tz_lookup.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bc8069e-2444-4bc6-b328-ded42b824aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "\n",
    "# df_tz = spark.read.parquet(f\"dbfs:/student-groups/Group_4_1/external/tz_lookup.parquet\")\n",
    "\n",
    "# # manually fill in missing time zone info\n",
    "# # note: neighbor_call is ICAO\n",
    "# BBG = Row(neighbor_call=\"BBG\", neighbor_lat=36.53856729627892, neighbor_lon=-93.19908127077512, timezone=\"America/Chicago\")\n",
    "# KOGS = Row(neighbor_call=\"KOGS\", neighbor_lat=44.6820707679313, neighbor_lon=-75.47692203483886, timezone=\"America/New_York\")\n",
    "# NSTU = Row(neighbor_call=\"NSTU\", neighbor_lat=-14.329024376251269, neighbor_lon=-170.71329690482548, timezone=\"Pacific/Pago_Pago\")\n",
    "# PGSN = Row(neighbor_call=\"PGSN\", neighbor_lat=15.11974288544001, neighbor_lon=145.7282788950688, timezone=\"Pacific/Saipan\")\n",
    "# PGUM = Row(neighbor_call=\"PGUM\", neighbor_lat=13.48562402083883, neighbor_lon=144.8001485238768, timezone=\"Pacific/Guam\")\n",
    "# TJPS = Row(neighbor_call=\"TJPS\", neighbor_lat=18.01055087987774, neighbor_lon=-66.56323216254391, timezone=\"America/Puerto_Rico\")\n",
    "# TJSJ = Row(neighbor_call=\"TJSJ\", neighbor_lat=18.457160454103658, neighbor_lon=-66.0974759565605, timezone=\"America/Puerto_Rico\")\n",
    "# US_0571 = Row(neighbor_call=\"US-0571\", neighbor_lat=48.25780621107438, neighbor_lon=-103.74169879360201, timezone=\"America/Chicago\")\n",
    "\n",
    "# man_df = spark.createDataFrame([BBG,KOGS,NSTU,PGSN,PGUM,TJPS,TJSJ,US_0571])\n",
    "# df_tz = df_tz.union(man_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e4ef2e2-6308-4388-a368-99bdaa9ffea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# folder_path = \"dbfs:/student-groups/Group_4_1\"\n",
    "# df_tz.write.mode('overwrite').parquet(f\"{folder_path}/external/tz_lookup_manually_adjusted.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930030f1-0075-49df-b697-87e108dd6658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apply time zones to flight times for a small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700af18d-de94-4414-8b60-a12e79a79b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df_airports = spark.read.option(\"header\",\"true\").csv(f\"dbfs:/mnt/mids-w261/airport-codes_csv.csv\")\n",
    "df_flights = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_3m/\")\n",
    "df_tz = spark.read.parquet(f\"dbfs:/student-groups/Group_4_1/external/tz_lookup_manually_adjusted.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c129ec8-6c38-4909-9cb0-d914fbb5aab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# start with a very, very small flight data sample\n",
    "tmp_flights = df_flights.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd8da30-602e-43be-bf0b-b9386b28f1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, we start with some joining, to get the relevant airport codes, and to introduce the time zone info, for both origin and destination airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e56294b-1ea1-4974-915d-1373b98ff1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create temporary views\n",
    "tmp_flights.createOrReplaceTempView(\"tmp_flights\")\n",
    "df_airports.createOrReplaceTempView(\"df_airports\")\n",
    "df_tz.createOrReplaceTempView(\"df_tz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0485e4b9-5c98-4caf-9c85-ca15ca56fcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "WITH origin AS(\n",
    "SELECT  tf.FL_DATE as date,\n",
    "        tf.CRS_DEP_TIME as dep_time,\n",
    "        tf.CRS_ARR_TIME as arr_time,\n",
    "        tf.ORIGIN as origin_iata,\n",
    "        tf.DEST as dest_iata,\n",
    "        a.ident as origin_icao\n",
    "FROM tmp_flights as tf\n",
    "LEFT JOIN df_airports as a on tf.ORIGIN = a.iata_code),\n",
    "\n",
    "origin_dest AS(\n",
    "SELECT  origin.date,\n",
    "        origin.dep_time,\n",
    "        origin.arr_time,\n",
    "        origin.origin_iata,\n",
    "        origin.dest_iata,\n",
    "        origin.origin_icao,\n",
    "        a.ident as dest_icao\n",
    "FROM origin\n",
    "LEFT JOIN df_airports as a on origin.dest_iata = a.iata_code),\n",
    "\n",
    "origin_dest_tz1 AS(\n",
    "SELECT  od.date,\n",
    "        od.dep_time,\n",
    "        od.arr_time,\n",
    "        od.origin_iata,\n",
    "        tz.timezone as origin_tz,\n",
    "        od.dest_iata,\n",
    "        od.origin_icao,\n",
    "        od.dest_icao\n",
    "FROM origin_dest as od\n",
    "LEFT JOIN df_tz as tz on od.origin_icao = tz.neighbor_call\n",
    "),\n",
    "\n",
    "origin_dest_tz2 AS(\n",
    "SELECT  od.date,\n",
    "        od.dep_time,\n",
    "        od.arr_time,\n",
    "        od.origin_iata,\n",
    "        od.origin_tz,\n",
    "        od.dest_iata,\n",
    "        tz.timezone as dest_tz,\n",
    "        od.origin_icao,\n",
    "        od.dest_icao\n",
    "FROM origin_dest_tz1 as od\n",
    "LEFT JOIN df_tz as tz on od.dest_icao = tz.neighbor_call\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM origin_dest_tz2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "out = spark.sql(query)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b128dec-4aff-40c9-ae14-90ee778befec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we apply the time zones to the departure and arrival times, and convert to UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0162f8b-a6a4-4168-bf40-ab760b4c43e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def create_datetime(yyyymmdd, hhmm, tz):\n",
    "    \"\"\"\n",
    "    Create UTC timestamp from flights table columns\n",
    "    yyyymmdd = FL_DATE\n",
    "    hhmm = CRS_DEP_TIME or CRS_ARR_TIME\n",
    "    tz = time zone from time zone table\n",
    "\n",
    "    Returns UTC time stamp, (cast to string)\n",
    "    \"\"\"\n",
    "\n",
    "    yyyy,MM,dd = yyyymmdd.split('-')\n",
    "    yyyy = int(yyyy) # get year\n",
    "    MM = int(MM) # get month\n",
    "    dd = int(dd) # get day\n",
    "    hh = hhmm//100 # get hour\n",
    "    mm = hhmm%100 # get minute\n",
    "\n",
    "    # create datetime variable\n",
    "    dt = datetime(yyyy,MM,dd,hh,mm)\n",
    "    # apply local time zone\n",
    "    dt_local = pytz.timezone(tz).localize(dt)\n",
    "    # convert to UTC\n",
    "    dt_utc = dt_local.astimezone(pytz.utc)\n",
    "\n",
    "    # return UTC datetime, cast to string\n",
    "    return str(dt_utc)\n",
    "\n",
    "# create_datetime('2015-02-27',901,'America/Chicago')\n",
    "\n",
    "dt_udf = udf(create_datetime)\n",
    "out = out.withColumn(\"dep_datetime\", dt_udf(col(\"date\"), col(\"dep_time\"), col(\"origin_tz\"))) \\\n",
    "    .withColumn(\"arr_datetime\", dt_udf(col(\"date\"), col(\"arr_time\"), col(\"dest_tz\")))\n",
    "display(out)\n",
    "\n",
    "# TODO: handle flights crossing midnight !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feaab843-45ca-428c-a572-5ef537dab2b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply time zones to create full cleaned flights table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99852409-c3a2-471b-8d3b-590f454cb677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airports = spark.read.option(\"header\",\"true\").csv(f\"dbfs:/mnt/mids-w261/airport-codes_csv.csv\")\n",
    "df_tz = spark.read.parquet(f\"dbfs:/student-groups/Group_4_1/external/tz_lookup_manually_adjusted.parquet\")\n",
    "\n",
    "# start with a very, very small flight data sample\n",
    "tmp_flights = df_flights.limit(10)\n",
    "\n",
    "# create temporary views\n",
    "df_flights.createOrReplaceTempView(\"df_flights\")\n",
    "df_airports.createOrReplaceTempView(\"df_airports\")\n",
    "df_tz.createOrReplaceTempView(\"df_tz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528a1ef3-e063-4c56-a7c9-12319baae167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define all columns of the flights table\n",
    "flights_cols = \", \".join(df_flights.columns)\n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "WITH origin AS(\n",
    "SELECT  {flights_cols},\n",
    "        flights.FL_DATE as date,\n",
    "        flights.CRS_DEP_TIME as dep_time,\n",
    "        flights.CRS_ARR_TIME as arr_time,\n",
    "        flights.ORIGIN as origin_iata,\n",
    "        flights.DEST as dest_iata,\n",
    "        a.ident as origin_icao\n",
    "FROM df_flights as flights\n",
    "LEFT JOIN df_airports as a on flights.ORIGIN = a.iata_code),\n",
    "\n",
    "origin_dest AS(\n",
    "SELECT  {flights_cols},\n",
    "        origin.date,\n",
    "        origin.dep_time,\n",
    "        origin.arr_time,\n",
    "        origin.origin_iata,\n",
    "        origin.dest_iata,\n",
    "        origin.origin_icao,\n",
    "        a.ident as dest_icao\n",
    "FROM origin\n",
    "LEFT JOIN df_airports as a on origin.dest_iata = a.iata_code),\n",
    "\n",
    "origin_dest_tz1 AS(\n",
    "SELECT  {flights_cols},\n",
    "        od.date,\n",
    "        od.dep_time,\n",
    "        od.arr_time,\n",
    "        od.origin_iata,\n",
    "        tz.timezone as origin_tz,\n",
    "        od.dest_iata,\n",
    "        od.origin_icao,\n",
    "        od.dest_icao\n",
    "FROM origin_dest as od\n",
    "LEFT JOIN df_tz as tz on od.origin_icao = tz.neighbor_call\n",
    "),\n",
    "\n",
    "origin_dest_tz2 AS(\n",
    "SELECT  {flights_cols},\n",
    "        od.date,\n",
    "        od.dep_time,\n",
    "        od.arr_time,\n",
    "        od.origin_iata,\n",
    "        od.origin_tz,\n",
    "        od.dest_iata,\n",
    "        tz.timezone as dest_tz,\n",
    "        od.origin_icao,\n",
    "        od.dest_icao\n",
    "FROM origin_dest_tz1 as od\n",
    "LEFT JOIN df_tz as tz on od.dest_icao = tz.neighbor_call\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM origin_dest_tz2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "out = spark.sql(query)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9854d41-791c-497a-a545-ed5b4af402de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp = out.filter( (col(\"origin_tz\").isNull()) | (col(\"dest_tz\").isNull()) )\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d7ed7c-bbda-4583-85d4-d725c3d3c1d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "# import pytz\n",
    "# from datetime import datetime, timedelta\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# def create_datetime(yyyymmdd, hhmm, tz):\n",
    "#     \"\"\"\n",
    "#     Create UTC timestamp from flights table columns\n",
    "#     yyyymmdd = FL_DATE\n",
    "#     hhmm = CRS_DEP_TIME or CRS_ARR_TIME\n",
    "#     tz = time zone from time zone table\n",
    "\n",
    "#     Returns UTC time stamp, (cast to string)\n",
    "#     \"\"\"\n",
    "\n",
    "#     yyyy,MM,dd = yyyymmdd.split('-')\n",
    "#     yyyy = int(yyyy) # get year\n",
    "#     MM = int(MM) # get month\n",
    "#     dd = int(dd) # get day\n",
    "#     hh = hhmm//100 # get hour\n",
    "#     mm = hhmm%100 # get minute\n",
    "\n",
    "#     if hh == 24:\n",
    "#         hh = 0\n",
    "#         do_shift = True\n",
    "#     else:\n",
    "#         do_shift = False\n",
    "\n",
    "#     # create datetime variable\n",
    "#     dt = datetime(yyyy,MM,dd,hh,mm)\n",
    "#     if do_shift:\n",
    "#         dt += timedelta(days=1)\n",
    "#     # apply local time zone\n",
    "#     dt_local = pytz.timezone(tz).localize(dt)\n",
    "#     # convert to UTC\n",
    "#     dt_utc = dt_local.astimezone(pytz.utc)\n",
    "\n",
    "#     # return UTC datetime, cast to string\n",
    "#     return str(dt_utc)\n",
    "\n",
    "# dt_udf = udf(create_datetime)\n",
    "\n",
    "# def create_arr_datetime(dep_yyyymmdd, dep_hhmm, dep_tz, flight_dur):\n",
    "#     \"\"\"\n",
    "#     Create UTC timestamp from flights table columns\n",
    "#     yyyymmdd = FL_DATE\n",
    "#     hhmm = CRS_DEP_TIME or CRS_ARR_TIME\n",
    "#     tz = time zone from time zone table\n",
    "\n",
    "#     Returns UTC time stamp, (cast to string)\n",
    "#     \"\"\"\n",
    "\n",
    "#     if flight_dur is None:\n",
    "#         return None\n",
    "    \n",
    "\n",
    "#     yyyy,MM,dd = dep_yyyymmdd.split('-')\n",
    "#     yyyy = int(yyyy) # get year\n",
    "#     MM = int(MM) # get month\n",
    "#     dd = int(dd) # get day\n",
    "#     hh = dep_hhmm//100 # get hour\n",
    "#     mm = dep_hhmm%100 # get minute\n",
    "\n",
    "#     if hh == 24:\n",
    "#         hh = 0\n",
    "#         do_shift = True\n",
    "#     else:\n",
    "#         do_shift = False\n",
    "\n",
    "#     # create datetime variable\n",
    "#     dt = datetime(yyyy,MM,dd,hh,mm)\n",
    "#     if do_shift:\n",
    "#         dt += timedelta(days=1)\n",
    "#     # apply local time zone\n",
    "#     dt_local = pytz.timezone(dep_tz).localize(dt)\n",
    "#     # convert to UTC\n",
    "#     dt_utc = dt_local.astimezone(pytz.utc)\n",
    "#     dt_arr_utc = dt_utc + timedelta(minutes=flight_dur)\n",
    "\n",
    "#     # return UTC datetime, cast to string\n",
    "#     return str(dt_arr_utc)\n",
    "    \n",
    "# dt_arr_udf = udf(create_arr_datetime)\n",
    "\n",
    "# def create_datetime_v2(yyyymmdd, dep_hhmm, arr_hhmm, tz):\n",
    "#     \"\"\"\n",
    "#     Create UTC timestamp from flights table columns\n",
    "#     yyyymmdd = FL_DATE\n",
    "#     hhmm = CRS_DEP_TIME or CRS_ARR_TIME\n",
    "#     tz = time zone from time zone table\n",
    "\n",
    "#     Returns UTC time stamp, (cast to string)\n",
    "#     \"\"\"\n",
    "\n",
    "#     yyyy,MM,dd = yyyymmdd.split('-')\n",
    "#     yyyy = int(yyyy) # get year\n",
    "#     MM = int(MM) # get month\n",
    "#     dd = int(dd) # get day\n",
    "#     hh = arr_hhmm//100 # get hour\n",
    "#     mm = arr_hhmm%100 # get minute\n",
    "\n",
    "#     if hh == 24:\n",
    "#         hh = 0\n",
    "#         do_shift = True\n",
    "#     else:\n",
    "#         do_shift = False\n",
    "\n",
    "#     # create datetime variable\n",
    "#     dt = datetime(yyyy,MM,dd,hh,mm)\n",
    "#     if do_shift:\n",
    "#         dt += timedelta(days=1)\n",
    "#     if dep_hhmm > arr_hhmm:\n",
    "#         dt += timedelta(days=1)\n",
    "#     # apply local time zone\n",
    "#     dt_local = pytz.timezone(tz).localize(dt)\n",
    "#     # convert to UTC\n",
    "#     dt_utc = dt_local.astimezone(pytz.utc)\n",
    "\n",
    "#     # return UTC datetime, cast to string\n",
    "#     return str(dt_utc)\n",
    "\n",
    "# dt2_udf = udf(create_datetime_v2)\n",
    "\n",
    "# out = out.withColumn(\"dep_datetime\", dt_udf(col(\"date\"), col(\"dep_time\"), col(\"origin_tz\"))) \\\n",
    "#     .withColumn(\"arr_datetime_SANITYCHECK\", dt2_udf(col(\"date\"), col(\"dep_time\"), col(\"arr_time\"), col(\"dest_tz\"))) \\\n",
    "#     .withColumn(\"arr_datetime\", dt_arr_udf(col(\"date\"), col(\"dep_time\"), col(\"origin_tz\"), col(\"CRS_ELAPSED_TIME\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3aa9d8-6f42-479a-93ae-587d56a4ad57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def to_utc(yyyymmdd, dep_hhmm, arr_hhmm, dep_tz, arr_tz, flight_dur):\n",
    "    \"\"\"\n",
    "    Create UTC timestamp from flights table columns\n",
    "    yyyymmdd = FL_DATE\n",
    "    hhmm = CRS_DEP_TIME or CRS_ARR_TIME\n",
    "    tz = time zone from time zone table\n",
    "\n",
    "    Returns UTC time stamp, (cast to string)\n",
    "    \"\"\"\n",
    "\n",
    "    yyyy,MM,dd = yyyymmdd.split('-')\n",
    "    yyyy = int(yyyy) # get year\n",
    "    MM = int(MM) # get month\n",
    "    dd = int(dd) # get day\n",
    "\n",
    "    dep_hh = dep_hhmm//100 # get hour\n",
    "    dep_mm = dep_hhmm%100 # get minute\n",
    "    if dep_hh == 24:\n",
    "        dep_hh = 0\n",
    "        dep_shift = True\n",
    "    else:\n",
    "        dep_shift = False\n",
    "\n",
    "    arr_hh = arr_hhmm//100 # get hour\n",
    "    arr_mm = arr_hhmm%100\n",
    "    if arr_hh == 24:\n",
    "        arr_hh = 0\n",
    "        arr_shift = True\n",
    "    else:\n",
    "        arr_shift = False\n",
    "\n",
    "    # create datetime variable for departure\n",
    "    dt_dep = datetime(yyyy,MM,dd,dep_hh,dep_mm)\n",
    "    if dep_shift:\n",
    "        dt_dep += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    dep_local = pytz.timezone(dep_tz).localize(dt_dep)\n",
    "    # convert to UTC\n",
    "    dep_utc = dep_local.astimezone(pytz.utc)\n",
    "\n",
    "    # create datetime variable for arrival\n",
    "    dt_arr = datetime(yyyy,MM,dd,arr_hh,arr_mm)\n",
    "    if arr_shift:\n",
    "        dt_arr += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    arr_local = pytz.timezone(arr_tz).localize(dt_arr)\n",
    "    # convert to UTC\n",
    "    arr_utc = arr_local.astimezone(pytz.utc)\n",
    "\n",
    "    if dep_utc > arr_utc:\n",
    "        arr_utc += timedelta(days=1)\n",
    "\n",
    "    # sanity check\n",
    "    arr_utc_SC = dep_utc + timedelta(minutes=flight_dur)\n",
    "\n",
    "    # return UTC datetime, cast to string\n",
    "    return (str(dep_utc), str(arr_utc), str(arr_utc_SC))\n",
    "\n",
    "# to_utc('2015-02-27',901,1031,'America/Chicago','America/Los_Angeles',180)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"dep_datetime\", StringType(), True),\n",
    "    StructField(\"arr_datetime\", StringType(), True),\n",
    "    StructField(\"arr_datetime_SANITYCHECK\", StringType(), True)\n",
    "])\n",
    "\n",
    "dt_udf = udf(to_utc, schema)\n",
    "\n",
    "out = out.withColumn('processed', dt_udf(col(\"date\"), col(\"dep_time\"), col(\"arr_time\"), col(\"origin_tz\"), col(\"dest_tz\"), col(\"CRS_ELAPSED_TIME\")))\n",
    "\n",
    "# out = out.withColumn(\"dep_datetime\", dt_udf(col(\"date\"), col(\"dep_time\"), col(\"origin_tz\"))) \\\n",
    "#     .withColumn(\"arr_datetime_SANITYCHECK\", dt2_udf(col(\"date\"), col(\"dep_time\"), col(\"arr_time\"), col(\"dest_tz\"))) \\\n",
    "#     .withColumn(\"arr_datetime\", dt_arr_udf(col(\"date\"), col(\"dep_time\"), col(\"origin_tz\"), col(\"CRS_ELAPSED_TIME\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a50072a4-c45e-481d-b6d8-b6fb2fd1ac33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = [c for c in out.columns if c != \"processed\"]\n",
    "cols += [\"processed.dep_datetime\",\"processed.arr_datetime\",\"processed.arr_datetime_SANITYCHECK\"]\n",
    "out = out.select(cols)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "795a8f34-f5de-46a2-8aac-52916fea63fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0.04-eil-flights-cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
