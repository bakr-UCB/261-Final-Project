{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7242c0b-e377-4108-94d5-687826addb3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Baseline Modeling on Joined Data\n",
    "\n",
    "Erica Landreth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea57f82-909d-40c5-a5ef-ff6655abcf9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e4de20-2fac-4c65-aa47-724b68203cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d62c3-ed6a-47d0-bb21-3c0faee08bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7e45df-c877-46e1-b5f9-fa843a68c574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de30d1e2-82a4-461f-a323-79e9ca866d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"1y\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds and overlap\n",
    "k = 5\n",
    "overlap = 0.2\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup)\n",
    "compute_seasonality = True\n",
    "\n",
    "# define train/test split date\n",
    "min_test_dt = \"2019-10-01\"\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7820b43-a23e-488f-a7f3-d9b07493cf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data and perform simple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/modeling_checkpoints\")\n",
    "\n",
    "# read in joined, cleaned dataset\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_weather_cleaned_combo.parquet\")\n",
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour variable (needed for seasonality)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt)\n",
    "df_train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d1e079e4-266b-4e77-9ba8-154a1f0addfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "# def get_cv_time_limits(df, k=3, blocking=False, dep_utc_varname=\"dep_datetime\", verbose=True):\n",
    "#     '''\n",
    "#     Get time bins for time-series cross validation\n",
    "#     '''\n",
    "#     n = df.count()\n",
    "#     df = df.withColumn(\"row_id\", f.row_number()\n",
    "#             .over(Window.partitionBy().orderBy(dep_utc_varname)))\n",
    "#     chunk_size = np.floor(n/(k+1))\n",
    "\n",
    "#     idx = np.arange(0,)\n",
    "#     idx = np.arange(0,n,chunk_size)\n",
    "#     idx[-1] = n-1\n",
    "#     idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "#     if verbose:\n",
    "#         print('')\n",
    "#         print(f'Number of validation datapoints for each fold is {chunk_size:,}')\n",
    "#         print(\"************************************************************\")\n",
    "\n",
    "#     bin_edges = df.filter(f.col(\"row_id\").isin(idx)).select(\"row_id\",dep_utc_varname).toPandas()\n",
    "\n",
    "#     out = []\n",
    "#     for i in range(k):\n",
    "#         # define minimum training time based on cross-validation style\n",
    "#         if not blocking:\n",
    "#             t_min_train = bin_edges[dep_utc_varname][0]\n",
    "#         else:\n",
    "#             t_min_train = bin_edges[dep_utc_varname][i]\n",
    "#         # define maximum training time\n",
    "#         t_max_train = bin_edges[dep_utc_varname][i+1]\n",
    "#         # define minimum test time\n",
    "#         t_min_test = bin_edges[dep_utc_varname][i+1]\n",
    "#         # define maximum test_time\n",
    "#         t_max_test = bin_edges[dep_utc_varname][i+2]\n",
    "\n",
    "#         out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "#                     \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "#     out = pd.DataFrame(out)\n",
    "        \n",
    "#     if verbose:\n",
    "#         for i in range(k):\n",
    "#             print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "#             print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        \n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc820cf1-c179-4279-8fe0-505d1758f9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits_with_overlap(df, k=3, blocking=False, overlap=0, dep_utc_varname=\"dep_datetime\", verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation\n",
    "    '''\n",
    "    n = df.count()\n",
    "    df = df.withColumn(\"row_id\", f.row_number()\n",
    "            .over(Window.partitionBy().orderBy(dep_utc_varname)))\n",
    "    total_width = k+1 - overlap*(k-1)\n",
    "    chunk_size = np.floor(n/total_width)\n",
    "\n",
    "    idx = np.arange(0,)\n",
    "    idx = np.arange(0,n,chunk_size)\n",
    "    idx[-1] = n-1\n",
    "    idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print('')\n",
    "        print(f'Number of validation datapoints for each fold is {chunk_size:,}')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    df.cache()\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define indices\n",
    "        if i == 0:\n",
    "            train_min_idx = 1\n",
    "            train_max_idx = chunk_size\n",
    "        else:\n",
    "            train_min_idx += np.floor((1-overlap)*chunk_size)\n",
    "            train_max_idx += np.floor((1-overlap)*chunk_size)\n",
    "        test_min_idx = train_min_idx + chunk_size\n",
    "        test_max_idx = train_max_idx + chunk_size\n",
    "\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = df.filter(f.col(\"row_id\") == 1).select(dep_utc_varname).toPandas().values[0][0]\n",
    "        else:\n",
    "            t_min_train = df.filter(f.col(\"row_id\") == train_min_idx).select(dep_utc_varname).toPandas().values[0][0]\n",
    "        # define maximum training time\n",
    "        t_max_train = df.filter(f.col(\"row_id\") == train_max_idx).select(dep_utc_varname).toPandas().values[0][0]\n",
    "        # define minimum test time\n",
    "        t_min_test = df.filter(f.col(\"row_id\") == test_min_idx).select(dep_utc_varname).toPandas().values[0][0]\n",
    "        # define maximum test_time\n",
    "        t_max_test = df.filter(f.col(\"row_id\") == test_max_idx).select(dep_utc_varname).toPandas().values[0][0]\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e0ef6902-1ce0-4e3c-8e5d-fbbf029b662f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # get cross-validation split times\n",
    "# df_sandbox = df.limit(100).cache()\n",
    "# cv_cutoffs = get_cv_time_limits_with_overlap(df_sandbox.select(dep_utc_varname), k=3, blocking=True, overlap=0.2,\n",
    "#     dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "# cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get cross-validation split times\n",
    "# cv_cutoffs = get_cv_time_limits(df_train.select(dep_utc_varname), k=k, blocking=True, \n",
    "#     dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "cv_cutoffs = get_cv_time_limits_with_overlap(df_train.select(dep_utc_varname), k=k, blocking=True, overlap=overlap,\n",
    "    dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaba55bd-601f-4def-a082-b259656574c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train seasonality models for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831adc2c-61e3-4984-a797-764958c5d2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# informed by: https://www.databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html\n",
    "\n",
    "def forecast_delay(history_pd: pd.DataFrame) -> pd.DataFrame: \n",
    "    \n",
    "    # define Prophet model\n",
    "    model = Prophet(\n",
    "        interval_width=0.9,\n",
    "        growth='linear',\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        # holidays=us_holidays,\n",
    "        # seasonality_mode='multiplicative'\n",
    "    )\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(history_pd)\n",
    "    \n",
    "    # configure predictions\n",
    "    future_pd = model.make_future_dataframe(\n",
    "        periods=24*7, \n",
    "        freq='h',\n",
    "        include_history=False\n",
    "    )\n",
    "    \n",
    "    # make predictions\n",
    "    results_pd = model.predict(future_pd)\n",
    "\n",
    "    # ref date and dow\n",
    "    ref_date = history_pd.ds.iloc[0].date()\n",
    "    ref_dow = history_pd.DAY_OF_WEEK[0]\n",
    "\n",
    "    # helper function: get day of the week,\n",
    "    # using reference date and dow\n",
    "    def get_dow(x,ref_date,dow):\n",
    "        d_days = (x.date() - ref_date).days + dow\n",
    "        d_days = d_days%7\n",
    "        if d_days == 0:\n",
    "            d_days = 7\n",
    "        return d_days\n",
    "\n",
    "    # get dow for forecasted points\n",
    "    results_pd['dow'] = results_pd.ds.apply(lambda x: get_dow(x,ref_date,ref_dow))\n",
    "\n",
    "    # get hour for forecasted points\n",
    "    results_pd['hour'] = results_pd.ds.apply(lambda x: x.hour)\n",
    "\n",
    "    # store origin\n",
    "    results_pd['ORIGIN'] = history_pd.ORIGIN.iloc[0]\n",
    "        \n",
    "    # return components\n",
    "    return results_pd[['dow','hour','weekly','daily','ORIGIN']]\n",
    "\n",
    "schema = StructType([StructField('dow', LongType(), True),\n",
    "                     StructField('hour', LongType(), True),\n",
    "                     StructField('weekly', DoubleType(), True),\n",
    "                     StructField('daily', DoubleType(), True),\n",
    "                     StructField('ORIGIN', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3583341c-b65e-4213-bd4e-005b21d1ff44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality( df, t_min, t_max, \n",
    "    dep_utc_varname=dep_utc_varname, delay_varname=\"DEP_DELAY\", \n",
    "    forecast_fn=forecast_delay, schema=schema ):\n",
    "\n",
    "    return (\n",
    "        df.filter((df[dep_utc_varname] >= t_min) & \\\n",
    "            (df[dep_utc_varname] < t_max))\n",
    "        .withColumnRenamed(delay_varname, \"y\")\n",
    "        .withColumnRenamed(dep_utc_varname, \"ds\")\n",
    "        .groupBy('ORIGIN')\n",
    "            .applyInPandas(forecast_fn, schema=schema)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53913682-beee-47d2-9f66-20151ae8913b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if compute_seasonality:\n",
    "    # train seasonality model for each cross validation split\n",
    "    for i in range(k):\n",
    "        # train seasonality model for this cross validation split\n",
    "        model = get_seasonality(df_train, cv_cutoffs[\"train_min\"][i], cv_cutoffs[\"train_max\"][i])\n",
    "        # write out\n",
    "        fn_out = f\"seasonality_model_{period}_cv{i}of{k}_overlap{overlap}.parquet\"\n",
    "        model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")\n",
    "\n",
    "    # train seasonality model for full training data\n",
    "    model = get_seasonality(df_train, datetime(1970,1,1), min_test_dt)\n",
    "    # write out\n",
    "    fn_out = f\"seasonality_model_{period}_train.parquet\"\n",
    "    model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d45b72-fd5c-4852-b707-e5d1e57fc097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{team_BASE_DIR}/interim/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5bdccd-7230-44de-9d2e-c2fba8819855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ca65bd-089d-4899-8384-4f7bc2025ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define columns to be used in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c4ef7d-8964-4aca-a618-768a5ef36855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\"]\n",
    "\n",
    "# date related columns\n",
    "# TODO: encode as ordinal categorical rather than numeric ???\n",
    "date_cols = [\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",\"DAY_OF_WEEK\",\"ORIGIN\",\"dep_hour_utc\",dep_utc_varname]\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *date_cols]\n",
    "categorical_cols = flight_metadata_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b9676c-7da2-427a-b2e7-f88b13d5f28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3cd118-b967-4e04-a17e-e45022a0b13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# Index and encode each categorical column\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\",handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_vec\",handleInvalid=\"keep\")\n",
    "    stages += [indexer, encoder]\n",
    "# define encoded categorical feature names\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "# assemble features\n",
    "features = numeric_cols + categorical_vec_columns\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid='keep')\n",
    "\n",
    "# scale features\n",
    "scaler = MinMaxScaler(inputCol=\"features\", \\\n",
    "    outputCol=\"features_scaled\")\n",
    "\n",
    "# logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features_scaled', \\\n",
    "    labelCol='outcome',maxIter=50)\n",
    "\n",
    "# construct pipeline object from all components\n",
    "pipeline = Pipeline(stages=stages+[assembler,scaler,lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b582b393-c135-43e6-a3c4-21177449b2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2d4b17-f16a-44c5-919b-a49112ff8fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality_data(df, fold, k):\n",
    "  \"\"\"\n",
    "  Look up seasonlaity features from saved seasonality model.\n",
    "  \"\"\"\n",
    "  if fold == 'full':\n",
    "      fn_model = f\"seasonality_model_{period}_train.parquet\"\n",
    "  else:\n",
    "      fn_model = f\"seasonality_model_{period}_cv{fold}of{k}_overlap{overlap}.parquet\"\n",
    "  model = spark.read.parquet(f\"{team_BASE_DIR}/interim/{fn_model}\")\n",
    "\n",
    "  joined_df = df.join(model, \n",
    "                    (df[\"ORIGIN\"] == model[\"ORIGIN\"]) & \n",
    "                    (df[\"DAY_OF_WEEK\"] == model[\"dow\"]) & \n",
    "                    (df[\"dep_hour_utc\"] == model[\"hour\"]),\n",
    "                    how=\"left\").drop(model[\"ORIGIN\"])\n",
    "  \n",
    "  return joined_df\n",
    "\n",
    "\n",
    "\n",
    "# CODE BELOW DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def upsample(train_df,verbose=False):\n",
    "  '''Upsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = non_delay_count / delay_count\n",
    "\n",
    "  train_delay = train_df.filter(f.col('outcome') == 0)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 1).sample(withReplacement=True, fraction=keep_percent,seed=42)\n",
    "  train_upsampled = train_delay.union(train_non_delay)\n",
    "  return train_upsampled\n",
    "\n",
    "\n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n",
    "\n",
    "def cv_eval(preds):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  return F2, pr\n",
    "\n",
    "def timeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timSeriesSplit k-fold cross validation \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    # elif sampling=='weights':\n",
    "    #   train_df = add_class_weights(train_df).cache()\n",
    "      \n",
    "    #print info on train and dev set for this fold\n",
    "    if verbose:\n",
    "      print('    TRAIN set for fold {} goes from {} to {}, count is {:,} flights ({})'.format((i+1), \n",
    "                                                                                      train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      train_df.count(),\n",
    "                                                                                      sampling + '-sampled' if sampling else 'no sampling'))\n",
    "      print('    DEV set for fold {} goes from {} to {}, count is {:,} flights'.format((i+1), \n",
    "                                                                                      dev_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      dev_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      dev_df.count()))\n",
    "      \n",
    "    # TODO: remove once feat engineering applied outside\n",
    "    train_df = get_seasonality_data(train_df, i, k)\n",
    "    train_df = train_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "    dev_df = get_seasonality_data(dev_df, i, k)\n",
    "    dev_df = dev_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "\n",
    "    # print(train_df.dtypes)\n",
    "    # print(dev_df.dtypes)\n",
    "        \n",
    "    # Fit params on the model\n",
    "    model = pipeline.fit(train_df)\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "  # # Train on full df\n",
    "  # print('Training on full train dataset, and validating on dev dataset with best parameters from CV:')\n",
    "  # print(best_parameters)\n",
    "    \n",
    "  # if verbose:\n",
    "  #   print('    TRAIN set for best parameter fitted model goes from {} to {}, count is {:,} flights ({})'.format(train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.count(),\n",
    "  #                                                                                                    sampling + '-sampled' if sampling else 'no sampling'))\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4243676d-12cf-46f5-94d0-c235a3741a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train, cross validate, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd38f66-1f12-49fe-a619-227a69cecd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7e5aaf-398f-43c4-9b80-a4c7858bf91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "df_train = downsample(df_train).cache()\n",
    "df_train = get_seasonality_data(df_train, 'full', k).cache()\n",
    "df_test = get_seasonality_data(df_test, 'full', k).cache()\n",
    "model = pipeline.fit(df_train)\n",
    "dev_pred = model.transform(df_test)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.11-eil-joined-modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
