{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa988ec4-230e-49bc-868f-55235d230db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modeling Pipeline (Experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165fe1c6-20c0-4156-8fcd-78eff30c68e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134d7e1c-a32a-4883-bdfb-ca3a7927701a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5e9bdd-2536-4ffb-b641-9c87ff506349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any, Union,Callable\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    RandomForestClassifier,\n",
    "    MultilayerPerceptronClassifier\n",
    ")\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "import mlflow\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, Trials\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40542d5e-46eb-4213-9a16-989eb8ee96a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6df1ab-7364-416b-8a35-70efe4d364ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variables and directories\n",
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/datasets_final_project_2022\"\n",
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/checkpoints\")\n",
    "period = \"\" # one of the following values (\"\", \"_3m\", \"_6m\", \"_1y\")\n",
    "k = 5 # cv folds\n",
    "overlap = 0.2 # cv overlap\n",
    "\n",
    "# Datasets\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined{period}_cleaned_engineered_timefeat.parquet\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned_pr_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320be5ab-c3e4-4b6e-b7ed-fc1c2c34f4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Directory Inspection\n",
    "display(dbutils.fs.ls(f\"{team_BASE_DIR}/interim/join_checkpoints/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd689be-1e15-4a47-aafd-210212a9ef0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP : Features Selection and Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a0b50a-5d60-4cb6-8fdf-f8ce5f174c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\"]\n",
    "\n",
    "# date related columns\n",
    "date_cols = [\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\"]\n",
    "\n",
    "# prior & current flight cols\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                ]\n",
    "\n",
    "bool_flight_cols = ['priorflight_isdeparted', \n",
    "                    'priorflight_isarrived_calc',\n",
    "                    'priorflight_isdelayed_calc',\n",
    "                    'priorflight_cancelled_true']\n",
    "\n",
    "# graph columns\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",\"sched_depart_utc\"]\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "categorical_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]\n",
    "\n",
    "features = numeric_cols + categorical_cols\n",
    "\n",
    "# features = numeric_cols + categorical_cols\n",
    "label = \"outcome\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62304ab2-aed9-4e91-9287-4d5e2920e612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## STEP : Time-series CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00561fab-518a-493a-a18e-bf2ee77644db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_timeseries(df, time_col: str, test_start: str, test_stop: str=\"2100-01-01\", verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Splits a PySpark DataFrame into a train/test set based on a timestamp column.\n",
    "    The most recent `test_fraction` of the data (by time) is used as test set.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        time_col (str): Timestamp column name (must be sortable).\n",
    "        test_start (str): Minimum date for the test set.\n",
    "        verbose (bool): Print boundaries and sizes.\n",
    "\n",
    "    Returns:\n",
    "        (train_df, test_df): Tuple of train and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Filter df to before max date\n",
    "    df = df.filter(F.col(time_col) < test_stop)\n",
    "\n",
    "    # Get min and max time\n",
    "    min_time, max_time = df.selectExpr(f\"min({time_col})\", f\"max({time_col})\").first()\n",
    "    total_days = (max_time - min_time).days\n",
    "\n",
    "    train_df = df.filter(F.col(time_col) < test_start)\n",
    "    test_df = df.filter(F.col(time_col) >= test_start)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"ðŸ“… Total date range: {min_time.date()} â†’ {max_time.date()} ({total_days} days)\")\n",
    "        print(f\"âœ… Train: {min_time.date()} â†’ {test_start} ({train_df.count():,} rows)\")\n",
    "        print(f\"ðŸ§ª Test: {test_start} â†’ {max_time.date()} ({test_df.count():,} rows)\")\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5769db2-0f27-46d8-9e2c-61a2ea4f68c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(F.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(F.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(F.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(F.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f023b00f-7de8-4f31-afd2-7ccf8a70098a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_series_cv_folds(\n",
    "    df,\n",
    "    time_col: str,\n",
    "    k: int=3,\n",
    "    blocking: bool=False,\n",
    "    overlap: float=0.0,\n",
    "    keep_me: list=df.columns, # defines variables to keep\n",
    "    verbose: bool=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Split a time-series PySpark DataFrame into k train/test folds with optional overlap and blocking.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): PySpark DataFrame with a timestamp column.\n",
    "        dep_utc_time_colvarname (str): Name of the timestamp column.\n",
    "        k (int): Number of folds.\n",
    "        blocking (bool): Whether to block the training set to avoid cumulative data.\n",
    "        overlap (float): Fraction of overlap between validation windows (e.g. 0.2 = 20% overlap).\n",
    "        verbose (bool): Whether to print the time splits.\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_df, val_df) tuples.\n",
    "    \"\"\"\n",
    "    # Get time boundaries\n",
    "    min_date = df.select(F.min(time_col)).first()[0]\n",
    "    max_date = df.select(F.max(time_col)).first()[0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "\n",
    "    # Adjust chunk sizing\n",
    "    total_width = k + 1 - overlap * (k - 1)\n",
    "    chunk_size = int(np.ceil(n_days / total_width))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Splitting data into {k} folds with {overlap*100:.0f}% overlap\")\n",
    "        print(f\"Min date: {min_date}, Max date: {max_date}\")\n",
    "        print(f\"{chunk_size:,} days per fold\")\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        # define indices based on chunk size and overlap\n",
    "        if i == 0:\n",
    "            train_start_offset = 0\n",
    "            train_end_offset = chunk_size\n",
    "        else:\n",
    "            train_start_offset += np.ceil((1-overlap)*chunk_size)\n",
    "            train_end_offset += np.floor((1-overlap)*chunk_size)\n",
    "        val_start_offset = train_end_offset\n",
    "        val_end_offset = val_start_offset + chunk_size\n",
    "\n",
    "        # # Offset calculation with overlap\n",
    "        # train_start_offset = 0 if not blocking else int(i * (1 - overlap) * chunk_size)\n",
    "        # train_end_offset = int((i + 1) * chunk_size)\n",
    "        # val_start_offset = train_end_offset\n",
    "        # val_end_offset = int(val_start_offset + chunk_size)\n",
    "\n",
    "        # Compute actual timestamps\n",
    "        train_start = min_date + timedelta(days=train_start_offset)\n",
    "        train_end = min_date + timedelta(days=train_end_offset)\n",
    "        val_start = min_date + timedelta(days=val_start_offset)\n",
    "        val_end = min_date + timedelta(days=val_end_offset)\n",
    "\n",
    "        if val_start >= max_date:\n",
    "            break\n",
    "        if val_end > max_date:\n",
    "            val_end = max_date + timedelta(days=1)\n",
    "\n",
    "        # Apply filters\n",
    "        train_df = downsample( \\\n",
    "            df.filter((F.col(time_col) >= train_start) & (F.col(time_col) < train_end)))\n",
    "        val_df = df.filter((F.col(time_col) >= val_start) & (F.col(time_col) < val_end))\n",
    "\n",
    "        # handle fold-specific variables\n",
    "        train_df = train_df \\\n",
    "            .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "            .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "            .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "            .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "            .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "        train_df = train_df.fillna({col:0 for col in \\\n",
    "            ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        val_df = val_df \\\n",
    "            .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "            .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "            .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "            .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "            .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "        val_df = val_df.fillna({col:0 for col in \\\n",
    "            ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        \n",
    "        train_df = train_df.select(*keep_me)\n",
    "        val_df = val_df.select(*keep_me)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold {i + 1}:\")\n",
    "            print(f\"  TRAIN: {train_start.date()} â†’ {train_end.date()} ({train_df.count():,} rows)\")\n",
    "            print(f\"  VAL:   {val_start.date()} â†’ {val_end.date()} ({val_df.count():,} rows)\")\n",
    "            print(\"------------------------------------------------------------\")\n",
    "\n",
    "        folds.append((train_df, val_df))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5291c112-e9e5-4a0f-954d-531be74c2faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cv_eval(preds, metric):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  if metric == \"F2\":\n",
    "    return F2\n",
    "  else:\n",
    "      return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41d7990-598b-4ea5-aff1-09b4c4677a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_tuner(\n",
    "    model,\n",
    "    stages,\n",
    "    folds: List[Tuple[DataFrame, DataFrame]],\n",
    "    mlflow_run_name: str = \"/Users/m.bakr@berkeley.edu/flight_delay_tuning\",\n",
    "    metric: str = \"F2\",\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Union[float, str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Universal tuning function for PySpark classification models using time-series cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of ['logreg', 'rf', 'mlp', 'xgb']\n",
    "        model_params (Dict[str, Any]): Parameters to apply to the model\n",
    "        folds (List of (train_df, val_df)): Time-aware CV folds\n",
    "        mlflow_run_name (str): Optional MLflow parent run name\n",
    "        verbose (bool): Whether to log outputs during tuning\n",
    "\n",
    "    Returns:\n",
    "        Dict with best average F2 or pr score, model name, and parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # # Model factory\n",
    "    # model_factory = {\n",
    "    #     \"logreg\": LogisticRegression,\n",
    "    #     \"rf\": RandomForestClassifier,\n",
    "    #     \"mlp\": MultilayerPerceptronClassifier,\n",
    "    #     \"xgb\": SparkXGBClassifier\n",
    "    # }\n",
    "\n",
    "    # assert model_name in model_factory, f\"Unsupported model: {model_name}\"\n",
    "\n",
    "    # ModelClass = model_factory[model_name]\n",
    "\n",
    "    # # Apply required fields\n",
    "    # model = ModelClass(\n",
    "    #     featuresCol=\"features_final\",\n",
    "    #     labelCol=label,\n",
    "    #     # weightCol=\"weight\",  # Handles imbalance\n",
    "    #     **model_params\n",
    "    # )\n",
    "\n",
    "    pipeline = Pipeline(stages=stages + [model]) \n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i, (train_df, val_df) in enumerate(folds):\n",
    "        fitted_model = pipeline.fit(train_df)\n",
    "        preds = fitted_model.transform(val_df)\n",
    "        score = cv_eval(preds, metric)\n",
    "        scores.append(score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Fold {i+1}] {metric} Score: {score:.4f}\")\n",
    "\n",
    "        mlflow.log_metric(f\"{metric}_fold_{i+1}\", score)\n",
    "\n",
    "    avg_score = float(np.mean(scores))\n",
    "    # mlflow.log_param(\"model\", model_name)\n",
    "    # mlflow.log_params(model_params)\n",
    "    mlflow.log_metric(\"avg_{metric}_score\", avg_score)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"âœ… Average {metric} Score: {avg_score:.4f}\")\n",
    "\n",
    "    return \"\", avg_score, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f2801c-bf0c-44a0-9bed-e2ca4eb7940b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_hyperopt_objective(\n",
    "    model_name: str,\n",
    "    folds: List[Tuple[DataFrame, DataFrame]],\n",
    "    stages: List,\n",
    "    param_space_converter: Callable[[Dict[str, Any]], Dict[str, Any]],\n",
    "    mlflow_experiment_name: str = \"Hyperopt_Universal_Tuning\",\n",
    "    verbose: bool = True\n",
    ") -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Creates a Hyperopt-compatible objective function for any PySpark classifier.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of 'logreg', 'rf', 'mlp', 'xgb'.\n",
    "        folds (List of (train_df, val_df)): Time-series CV folds.\n",
    "        param_space_converter (Callable): Converts Hyperopt sample into model params.\n",
    "        mlflow_experiment_name (str): MLflow experiment name.\n",
    "        verbose (bool): Logging toggle.\n",
    "\n",
    "    Returns:\n",
    "        Callable that can be passed as fn to hyperopt.fmin()\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(sampled_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Convert sampled param space to Spark-friendly params\n",
    "        model_params = param_space_converter(sampled_params)\n",
    "\n",
    "        result = model_tuner(\n",
    "            model_name=model_name,\n",
    "            model_params=model_params,\n",
    "            stages=stages,\n",
    "            folds=folds,\n",
    "            mlflow_run_name=f\"hyperopt_{model_name}\",\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        print(\"!!!\")\n",
    "        print(result)\n",
    "\n",
    "        return {\n",
    "            \"loss\": -result[1],  # Minimize negative F2\n",
    "            \"status\": STATUS_OK,\n",
    "            \"params\": result[2]\n",
    "        }\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42387b9d-111d-43a4-b6da-de46968fc2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8eb8bc-9cf8-44fc-8e20-8abd5d2400e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get Train/Test and Data for Each CV Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49d2e95-4584-4a67-982d-7a0cc4ce32e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split_timeseries(\n",
    "    df=df,\n",
    "    time_col=\"sched_depart_utc\",\n",
    "    test_start=\"2019-01-01\",\n",
    "    test_stop=\"2020-01-01\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda14a23-ed8f-4524-9a6e-39b4159fd7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_cols = [*keep_me, *numeric_cols, *categorical_cols]\n",
    "\n",
    "# Testing Time Series CV function\n",
    "folds = time_series_cv_folds(\n",
    "    train_df,\n",
    "    time_col=\"sched_depart_utc\",\n",
    "    k=k,\n",
    "    overlap=overlap,\n",
    "    blocking=True,\n",
    "    keep_me=filter_cols,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7c4d17-bdd9-49c9-b151-9c9aef401cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_samp = downsample(train_df)\n",
    "train_df_samp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41cf7a4e-4cb4-4e98-9848-84de6382be2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_samp = train_df_samp \\\n",
    "    .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "    .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "    .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "    .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "    .withColumnRenamed(f\"test\",\"pagerank\")\n",
    "train_df_samp = train_df_samp.fillna({col:0 for col in \\\n",
    "    ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "test_df = test_df \\\n",
    "    .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "    .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "    .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "    .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "    .withColumnRenamed(f\"test\",\"pagerank\")\n",
    "test_df = test_df.fillna({col:0 for col in \\\n",
    "    ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "\n",
    "train_df_samp = train_df_samp.select(*filter_cols).cache()\n",
    "test_df = test_df.select(*filter_cols).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33ca572-5259-40f9-8b86-f0fe9b896938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48385f3b-f583-4f25-9f16-cbb6a7da1b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Modeling Pipeline Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae2f8d9-cdb4-47e5-af0d-bbae3e69339f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_stages(categorical_cols, numeric_cols):\n",
    "\n",
    "    # List to hold the stages of the pipeline\n",
    "    stages = []\n",
    "\n",
    "    # 1. Index and encode categorical columns\n",
    "    for column in categorical_cols:\n",
    "        indexer = StringIndexer(\n",
    "            inputCol=column, \n",
    "            outputCol=column + \"_index\", \n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        encoder = OneHotEncoder(\n",
    "            inputCol=column + \"_index\", \n",
    "            outputCol=column + \"_vec\", \n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        stages += [indexer, encoder]\n",
    "\n",
    "    # 4. Update feature list to include imputed columns\n",
    "    categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "    features = numeric_cols + categorical_vec_columns\n",
    "\n",
    "    # 5. Assemble features\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=features, \n",
    "        outputCol=\"features\", \n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "\n",
    "    # 6. Scale features\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_final\"\n",
    "    )\n",
    "\n",
    "    stages += [assembler,scaler]\n",
    "\n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f5eeb8-df30-48ed-80d6-2bd631f1603d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_feature_experiment(categorical_cols,numeric_cols,run_name):\n",
    "\n",
    "    print(\"Defining model\")\n",
    "    # define RF model with tuned parameters\n",
    "    model = RandomForestClassifier(\n",
    "        featuresCol=\"features_final\",\n",
    "        labelCol=label,\n",
    "        featureSubsetStrategy=\"auto\",\n",
    "        maxDepth=10,\n",
    "        numTrees=20\n",
    "        )\n",
    "\n",
    "    print(\"Defining pipeline stages\")\n",
    "    stages = make_stages(categorical_cols,numeric_cols)\n",
    "    pipeline = Pipeline(stages=stages + [model])\n",
    "    metric = \"F2\"\n",
    "\n",
    "    print(\"FITTING ON CV FOLDS\")\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{run_name}_cv\"):\n",
    "        _ , score, _ = model_tuner(\n",
    "            model=model,\n",
    "            folds=folds,\n",
    "            stages=stages,\n",
    "            mlflow_run_name=\"/Users/m.bakr@berkeley.edu/flight_delay_tuning_EIL_sandbox\",\n",
    "            verbose=True\n",
    "        )\n",
    "        print(\"F2 Score:\", score)\n",
    "\n",
    "    print(\"FITTING ON FULL TRAINING DATA\")\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{run_name}_full\"):\n",
    "        fitted_model = pipeline.fit(train_df_samp)\n",
    "        preds = fitted_model.transform(test_df)\n",
    "        score = cv_eval(preds, metric)\n",
    "\n",
    "        print(f\"[Held out test] {metric} Score: {score:.4f}\")\n",
    "\n",
    "        mlflow.log_metric(f\"{metric}_held_out_test\", score)\n",
    "\n",
    "        mlflow.log_param(\"model\", \"rf\")\n",
    "        mlflow.log_metric(\"{metric}_score\", score)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.spark.log_model(fitted_model, \"rf_model\")\n",
    "\n",
    "        # Extract prediction and label columns\n",
    "        prediction_and_labels = preds.select(\"prediction\", label).rdd.map(lambda r: (r[0], r[1]))\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        metrics = MulticlassMetrics(prediction_and_labels)\n",
    "        confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "        \n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix)\n",
    "\n",
    "        return fitted_model, score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9beec1b0-df5a-4956-a261-6a7248962f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ff6185-1bb5-40e8-bb52-7b99b0be5381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # NORMAL CASE\n",
    "# num_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "# cat_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c709675c-0d49-4f66-8140-dbaf4e53f8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment: No weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0fe216-e44e-45a6-97d7-37ea240153b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "cat_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]\n",
    "\n",
    "run_feature_experiment(cat_cols,num_cols,\"rf_no_weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d87ac8-a628-48fe-b1a8-0a6f656add43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment: No seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b30cee-006a-4183-943b-bf6c46ae9dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [*num_weather_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "cat_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]\n",
    "\n",
    "run_feature_experiment(cat_cols,num_cols,\"rf_no_seas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32cbb591-7e33-4844-b673-db82dd241fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment: No time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5aec374-53c9-4ebb-bab7-8a0d0b619e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [*num_weather_cols, *seasonality_cols, *num_flight_cols, *graph_cols]\n",
    "cat_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]\n",
    "\n",
    "run_feature_experiment(cat_cols,num_cols,\"rf_no_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "808f637b-1bb7-43c4-988b-c945e4f26e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment: No priorflight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9d6063-38ae-4bcd-b462-48d045e56e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *graph_cols]\n",
    "cat_cols = [*date_cols, *flight_metadata_cols]\n",
    "\n",
    "run_feature_experiment(cat_cols,num_cols,\"rf_no_priorflight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b143dc-c48a-4e37-beb6-b5b6e00f0803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment: No graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513f9ff8-ba16-42e3-b454-d8b5f6b6cf78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols]\n",
    "cat_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]\n",
    "\n",
    "run_feature_experiment(cat_cols,num_cols,\"rf_no_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc8b32f-502c-4c35-87e6-7e298ed1bfbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment: No date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f695ec06-de82-4f5b-a40f-6a5c9b8df0b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "cat_cols = [ *flight_metadata_cols, *bool_flight_cols]\n",
    "\n",
    "run_feature_experiment(cat_cols,num_cols,\"rf_no_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe93ca4c-7278-46e3-8363-0d32f023e286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiment: No metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221ed6f5-1ef7-4dbb-b60a-ea278937c7b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "cat_cols = [*date_cols, *bool_flight_cols]\n",
    "\n",
    "run_feature_experiment(cat_cols,num_cols,\"rf_no_seas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "263a7831-64e8-4ab8-81e4-1fd51b750a33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de0fee5-6672-471f-8fb3-eb8fba9f1c08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "cat_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]\n",
    "\n",
    "model = SparkXGBClassifier(\n",
    "    features_col=\"features_final\",\n",
    "    label_col=label,\n",
    "    max_depth=10,\n",
    "    num_workers=5\n",
    ")\n",
    "\n",
    "stages = make_stages(categorical_cols,numeric_cols)\n",
    "pipeline = Pipeline(stages=stages + [model])\n",
    "metric = \"F2\"\n",
    "\n",
    "fitted_model = pipeline.fit(train_df_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a45135-da48-4a30-80ec-bb7541fa6d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "va = fitted_model.stages[-3]\n",
    "tree = fitted_model.stages[-1]\n",
    "mappings = list(zip(va.getInputCols(), tree.get_feature_importances().keys()))\n",
    "avg_gain = pd.DataFrame(list(tree.get_feature_importances('gain').items()), columns=['id', 'avg_gain'])\n",
    "weight = pd.DataFrame(list(tree.get_feature_importances('weight').items()), columns=['id', 'weight'])\n",
    "avg_cover = pd.DataFrame(list(tree.get_feature_importances('cover').items()), columns=['id', 'avg_cover'])\n",
    "\n",
    "feature_importance_df = avg_gain.merge(weight, on='id').merge(avg_cover, on='id')\n",
    "mappings_df = pd.DataFrame(mappings, columns=['name', 'id'])\n",
    "feature_importance_df = feature_importance_df.merge(mappings_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db361e2b-38f8-4613-a724-ca216419b3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd46e6f1-0fa5-4443-9647-78514c080b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_importance_df['idx'] = feature_importance_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60138b19-333d-44f5-a89c-2a8980ea0802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104de4f9-1b13-48d1-b6bd-02f8bf81f701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_family(index):\n",
    "    if 0 <= index <= 8:\n",
    "        return 'weather'\n",
    "    elif 9 <= index <= 12:\n",
    "        return 'seasonality'\n",
    "    elif 13 <= index <= 14:\n",
    "        return 'time'\n",
    "    elif index in [15, 16, 17, 18, 27, 28, 29, 30]:\n",
    "        return 'curr_prior_flight'\n",
    "    elif index == 19:\n",
    "        return 'graph'\n",
    "    elif 20 <= index <= 23:\n",
    "        return 'date'\n",
    "    elif 24 <= index <= 26:\n",
    "        return 'metadata'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "feature_importance_df['family'] = feature_importance_df['idx'].apply(get_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496f00cf-a8aa-4b9b-a6c3-4721da1d17a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d54940-d237-405c-b711-8765591ca07d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATA = feature_importance_df.groupby('family').agg({\n",
    "    'weight': 'sum',\n",
    "    'avg_gain': 'sum',\n",
    "    'avg_cover': 'sum'\n",
    "}).reset_index()\n",
    "DATA.set_index('family', inplace=True)\n",
    "DATA = DATA.div(DATA.sum(axis=0), axis=1)\n",
    "DATA = DATA.sort_values(by='weight', ascending=False)\n",
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e05d335c-119f-47a0-a2e6-2c204d1efe53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = DATA.weight\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "for idx,v in enumerate(data.index):\n",
    "    if idx == 0:\n",
    "        ax.barh([''],data[v],edgecolor='black',label=v.replace(\"_\",\" \"))\n",
    "    else:\n",
    "        ax.barh([''],data[v],edgecolor='black',label=v.replace(\"_\",\" \"),left=np.sum(data[data.index[:idx]]))\n",
    "ax.set_xlabel('Proportion')\n",
    "ax.set_title(f'Feature Importance by Feature Family: Weight')\n",
    "ax.set_xlim((0,1))\n",
    "ax.legend(loc='upper center',bbox_to_anchor=(0.5,-0.3),ncol=len(data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "071daf88-4379-44ea-9c67-29fbfa19a877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = DATA.avg_gain\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "for idx,v in enumerate(data.index):\n",
    "    if idx == 0:\n",
    "        ax.barh([''],data[v],edgecolor='black',label=v.replace(\"_\",\" \"))\n",
    "    else:\n",
    "        ax.barh([''],data[v],edgecolor='black',label=v.replace(\"_\",\" \"),left=np.sum(data[data.index[:idx]]))\n",
    "ax.set_xlabel('Proportion')\n",
    "ax.set_title(f'Feature Importance by Feature Family: Average Gain')\n",
    "ax.set_xlim((0,1))\n",
    "ax.legend(loc='upper center',bbox_to_anchor=(0.5,-0.3),ncol=len(data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e33356-a37f-477f-91b8-6e4ba50f73c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = DATA.avg_cover\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "for idx,v in enumerate(data.index):\n",
    "    if idx == 0:\n",
    "        ax.barh([''],data[v],edgecolor='black',label=v.replace(\"_\",\" \"))\n",
    "    else:\n",
    "        ax.barh([''],data[v],edgecolor='black',label=v.replace(\"_\",\" \"),left=np.sum(data[data.index[:idx]]))\n",
    "ax.set_xlabel('Proportion')\n",
    "ax.set_title(f'Feature Importance by Feature Family: Average Cover')\n",
    "ax.set_xlim((0,1))\n",
    "ax.legend(loc='upper center',bbox_to_anchor=(0.5,-0.3),ncol=len(data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aca450e5-0ca5-4195-9e00-749b60532074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Feature Experiments Clone) (EIL Clone) 3.15-mas-modeling-pipeline-with-tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
