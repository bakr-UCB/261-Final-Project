{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e8645b-25d8-4cf6-bb0d-d8f59f94d09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "278f3592-5af6-4642-8be6-882f9eb0854b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f276bbf-63b3-4024-8900-58901e5ed521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col,isnan, when, count, concat_ws, countDistinct, collect_set, rank, window, avg, hour, udf, isnan, pandas_udf, to_timestamp, lit, PandasUDFType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "import numpy as np\n",
    "from pyspark.sql import types\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f166b4a-d400-411b-b1f8-8f890188f865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install python-geohash\n",
    "import geohash\n",
    "from geohash import bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696cb845-b95b-46d3-aed8-c389f351d6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6624c7e4-283e-49b5-810b-f98463e4e576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/interim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6551cf-94c7-423b-b4dd-0cb2046ec479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Weather Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e135163f-0b6d-4598-a58f-39c68cdc83e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_interpolate = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        \"origin_HourlyWindSpeed\",\n",
    "        F.when(\n",
    "            F.col(\"origin_HourlyWindSpeed\").isNull(),\n",
    "            # Extract sustained wind speed from METAR groups\n",
    "            F.regexp_extract(\n",
    "                F.col(\"origin_REM\"),\n",
    "                r'\\b(\\d{3})(\\d{2,3})(?:G(\\d{2,3}))?KT\\b',  # Regex pattern\n",
    "                2  # Capture group for sustained wind speed\n",
    "            ).cast(\"int\")\n",
    "        ).otherwise(F.col(\"origin_HourlyWindSpeed\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"origin_HourlyWindGustSpeed\",\n",
    "        F.when(\n",
    "            F.col(\"origin_HourlyWindGustSpeed\").isNull(),\n",
    "            F.greatest(\n",
    "                # Regular wind gust (G group)\n",
    "                F.regexp_extract(\n",
    "                    F.col(\"origin_REM\"),\n",
    "                    r'\\b(\\d{3})(\\d{2,3})(?:G(\\d{2,3}))?KT\\b',\n",
    "                    3\n",
    "                ).cast(\"int\"),\n",
    "                # Peak wind gust (PK WND group)\n",
    "                F.regexp_extract(\n",
    "                    F.col(\"origin_REM\"),\n",
    "                    r'PK WND (\\d{3})(\\d{2,3})/(\\d{4})',  # PK WND pattern\n",
    "                    2  # Capture group for peak wind speed\n",
    "                ).cast(\"int\")\n",
    "            )\n",
    "        ).otherwise(F.col(\"origin_HourlyWindGustSpeed\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "df_interpolate = (df_interpolate \\\n",
    "    .withColumn(\n",
    "        'origin_HourlyPrecipitation',\n",
    "        F.when(\n",
    "            (F.col(\"origin_HourlyPrecipitation\").isNull()) | (F.col(\"origin_HourlyPrecipitation\") == '*'),\n",
    "            (F.regexp_extract(F.col(\"origin_REM\"), r\" P(\\d+)\", 1).cast(\"int\") * 0.01) # hundredths of inch kept in \"remarks\" section\n",
    "        ).otherwise(F.col(\"origin_HourlyPrecipitation\"))\n",
    "    ) \\\n",
    "    .withColumn('origin_HourlyPrecipitation', F.regexp_replace('origin_HourlyPrecipitation', 'T', '0.01')) \\\n",
    "    .withColumn(\n",
    "        'origin_HourlyPrecipitation',\n",
    "        F.regexp_extract('origin_HourlyPrecipitation', r\"[0-9]+(\\.[0-9]+)?\", 0) # Match digits\n",
    "    ) \\\n",
    "    .withColumn('origin_HourlyPrecipitation', F.col('origin_HourlyPrecipitation').cast(DoubleType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25eb5bed-21f4-4631-aa1d-ea6103d52908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode_geohash(precision: int):\n",
    "    @pandas_udf(\"string\")\n",
    "    def encode(latitudes: pd.Series, longitudes: pd.Series) -> pd.Series:\n",
    "        def safe_encode(lat, lon):\n",
    "            try:\n",
    "                return geohash.encode(lat, lon, precision)\n",
    "            except Exception:\n",
    "                return None\n",
    "        return latitudes.combine(longitudes, safe_encode)\n",
    "    return encode\n",
    "\n",
    "geohash_udf = encode_geohash(precision=2)\n",
    "df_interpolate = df_interpolate.withColumn('geohash', geohash_udf(F.col('origin_LATITUDE'), F.col('origin_LONGITUDE')))\n",
    "display(df_interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420e520c-ffe9-46bd-a43e-8b0c48789a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def coalesce_within_geohash(\n",
    "    df, \n",
    "    target_col, \n",
    "    geohash_col=\"geohash\", \n",
    "    dt_col=\"sched_depart_utc\", \n",
    "    window_size=6\n",
    "):\n",
    "    \"\"\"Fill nulls in `target_col` using the latest non-null value from the same geohash.\"\"\"\n",
    "    \n",
    "    window_spec = (\n",
    "        Window.partitionBy(geohash_col)\n",
    "              .orderBy(F.col(dt_col).cast(\"long\"))\n",
    "              .rowsBetween(-window_size, 0)\n",
    "    )\n",
    "    \n",
    "    return df.withColumn(\n",
    "        target_col,\n",
    "        F.last(target_col, ignorenulls=True).over(window_spec)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1971f931-6b90-411b-a199-dac2c52f4d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_interpolated = df_interpolate.withColumns(\n",
    "    {\"origin_HourlyPrecipitation\": df_interpolate[\"origin_HourlyPrecipitation\"].cast(\"float\"),\n",
    "     \"origin_HourlyWindGustSpeed\": df_interpolate[\"origin_HourlyWindGustSpeed\"].cast(\"float\"),\n",
    "     \"origin_HourlyWindSpeed\": df_interpolate[\"origin_HourlyWindSpeed\"].cast(\"float\"),\n",
    "     \"origin_HourlyDewPointTemperature\": df_interpolate[\"origin_HourlyDewPointTemperature\"].cast(\"float\"),\n",
    "     \"origin_HourlyDryBulbTemperature\": df_interpolate[\"origin_HourlyDryBulbTemperature\"].cast(\"float\"),\n",
    "     \"origin_HourlyPressureChange\": df_interpolate[\"origin_HourlyPressureChange\"].cast(\"float\"),\n",
    "     \"origin_HourlyRelativeHumidity\": df_interpolate[\"origin_HourlyRelativeHumidity\"].cast(\"float\"),\n",
    "     \"origin_HourlyWetBulbTemperature\": df_interpolate[\"origin_HourlyWetBulbTemperature\"].cast(\"float\"),\n",
    "     \"origin_HourlyVisibility\": df_interpolate[\"origin_HourlyVisibility\"].cast(\"float\")\n",
    "     \n",
    "     \n",
    "     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6743852d-845e-402d-aeb6-1231d0f16007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_to_fill = ['origin_HourlyVisibility','origin_HourlyWindSpeed','origin_HourlyDewPointTemperature','origin_HourlyDryBulbTemperature','origin_HourlyPressureChange','origin_HourlyRelativeHumidity','origin_HourlyWetBulbTemperature','origin_HourlyPrecipitation','origin_HourlyWindGustSpeed']\n",
    "\n",
    "for col in columns_to_fill:\n",
    "    df_interpolated = coalesce_within_geohash(df_interpolated, col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4714cd8-df63-46ae-abc1-109a5aa0a86d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "null_counts = df_interpolated.filter(F.col('origin_LATITUDE').isNotNull()).select(\n",
    "    [F.count(F.when(F.col(c).isNull() | F.isnan(c), c)).alias(c) for c in columns_to_fill]\n",
    ")\n",
    "\n",
    "null_counts_orig = df_interpolate.filter(F.col('origin_LATITUDE').isNotNull()).select(\n",
    "    [F.count(F.when(F.col(c).isNull() | F.isnan(c), c)).alias(c) for c in columns_to_fill]\n",
    ")\n",
    "display(null_counts_orig.unionByName(null_counts)) #null counts where we have non null loc (obviously, null loc is impossible to fill in by geohash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f05c78b-b27e-4317-b76a-12e8f75339f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def exponential_smoothing_pandas(values: pd.Series) -> pd.Series:\n",
    "    \"\"\"Vectorized UDF for exponential smoothing.\"\"\"\n",
    "    if values.empty or not pd.api.types.is_numeric_dtype(values):\n",
    "        return pd.Series([0.0] * len(values))  # Handle edge cases\n",
    "    return values.ewm(alpha=0.5, ignore_na=True).mean()\n",
    "\n",
    "def smooth_column_optimized(\n",
    "    df, \n",
    "    col_name, \n",
    "    station_col=\"origin_STATION\", \n",
    "    dt_col=\"sched_depart_date_time\", \n",
    "    window_size=6\n",
    "):\n",
    "    \"\"\"Applies exponential smoothing to remaining nulls\"\"\"\n",
    "    \n",
    "    # 1. Cast to numeric type and filter nulls\n",
    "    df = (\n",
    "        df.withColumn(col_name, F.col(col_name).cast(DoubleType()))\n",
    "    )\n",
    "    \n",
    "    # 2. Define window to collect non-null values\n",
    "    window_spec = (\n",
    "        Window.partitionBy(station_col)\n",
    "              .orderBy(F.col(dt_col).cast(\"long\"))\n",
    "              .rowsBetween(-window_size, 0)\n",
    "    )\n",
    "    \n",
    "    # 3. Collect ONLY non-null values within the window\n",
    "    df = df.withColumn(\n",
    "        \"non_null_values\",\n",
    "        F.collect_list(col_name).over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # 4. Apply vectorized UDF and fill nulls\n",
    "    return (\n",
    "        df.withColumn(\"smoothed\", exponential_smoothing_pandas(\"non_null_values\"))\n",
    "          .withColumn(\n",
    "              col_name, \n",
    "              F.coalesce(F.col(col_name), F.col(\"smoothed\"))\n",
    "          )\n",
    "          .drop(\"non_null_values\", \"smoothed\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2a6cfe-502c-428e-a60f-3c04e12b3274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_to_fill = ['origin_HourlyWindSpeed','origin_HourlyDewPointTemperature','origin_HourlyDryBulbTemperature','origin_HourlyPressureChange','origin_HourlyRelativeHumidity','origin_HourlyWetBulbTemperature','origin_HourlyPrecipitation','origin_HourlyWindGustSpeed','origin_HourlyVisibility']\n",
    "\n",
    "for col in columns_to_fill:\n",
    "    df_interpolated = smooth_column_optimized(df_interpolated, col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca0e726-9bde-42ad-b61e-d4fed9561927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filled_cols = ['origin_HourlyVisibility','origin_HourlyWindSpeed','origin_HourlyDewPointTemperature','origin_HourlyDryBulbTemperature','origin_HourlyPressureChange','origin_HourlyRelativeHumidity','origin_HourlyWetBulbTemperature','origin_HourlyPrecipitation','origin_HourlyWindGustSpeed']\n",
    "\n",
    "null_counts_ema = df_interpolated.filter(F.col('origin_LATITUDE').isNotNull()).select(\n",
    "    [F.count(F.when(F.col(c).isNull() | F.isnan(c), c)).alias(c) for c in filled_cols]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57fca9e5-8dad-4ba4-a63f-5854d6eda385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(null_counts_orig.unionByName(null_counts).unionByName(null_counts_ema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8827c78-f067-45f2-b2fe-63f17a3f7402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_interpolated.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2391d1b4-f4f2-40ad-a13d-5b9f086c4072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73356b16-3d16-4969-ad62-c9797ff3175c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Lags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "215d7768-4c2c-457b-aa86-a1fcbffb869b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74f95e9f-2804-40ec-b914-1d2f0248022c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c152b3-1d80-4949-8304-996bd8f5c163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2887cea-17ff-4f99-99e1-ed6a38ad9b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_small = df.filter(F.col('TAIL_NUM').isNotNull()).filter(F.col('sched_depart_utc')>'2019-04-01').orderBy('TAIL_NUM','sched_depart_utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6007540-17f4-46c1-bd47-da163e5039dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_small.select('TAIL_NUM','ORIGIN','DEST','sched_depart_utc','CANCELLED','two_hours_prior_depart_UTC','priorflight_origin','priorflight_dest', 'priorflight_deptime_calc','priorflight_deptime_calc','priorflight_cancelled_true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb9f0e1-ecd7-4e77-9c61-a1b39571c745",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_5a04ea56\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_65ea48cf\",\"enabled\":true,\"columnId\":\"ORIGIN\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1744383186521}],\"syncTimestamp\":1744383186522}",
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(F.col('TAIL_NUM').isNotNull()).select('TAIL_NUM','ORIGIN','DEST','sched_depart_utc','CANCELLED','two_hours_prior_depart_UTC','priorflight_origin','priorflight_dest', 'priorflight_deptime_calc','priorflight_deptime_calc','priorflight_cancelled_true').orderBy('TAIL_NUM','sched_depart_utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d16df5-59f1-4893-936b-a609f69d0e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(F.col('TAIL_NUM')=='N102UW').select('ORIGIN','DEST','sched_depart_utc','CANCELLED','two_hours_prior_depart_UTC','priorflight_origin','priorflight_dest', 'priorflight_deptime_calc','priorflight_deptime_calc','priorflight_cancelled_true').orderBy('sched_depart_utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49615498-ab7e-416b-b2a3-ba027b3d13d3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1744383449480}",
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter((F.col('CANCELLED') == 1) & (F.col('priorflight_cancelled_true') == 1)).filter(F.col('TAIL_NUM').isNotNull()).select('TAIL_NUM','ORIGIN','DEST','sched_depart_utc','CANCELLED','two_hours_prior_depart_UTC','priorflight_origin','priorflight_dest', 'priorflight_deptime_calc','priorflight_deptime_calc','priorflight_cancelled_true').orderBy('TAIL_NUM','sched_depart_utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4a4b2e-79af-42e1-8d5f-949daeb539b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"2019-10-27T19:46:00.000+00:00\".tzinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68465338-63ed-43d8-a997-29d5a9c4f1a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_utc(yyyymmdd, dep_hhmm, arr_hhmm, dep_tz, arr_tz, flight_dur):\n",
    "    \"\"\"\n",
    "    Create UTC timestamp from flights table columns\n",
    "    yyyymmdd = FL_DATE\n",
    "    dep_hhmm = CRS_DEP_TIME\n",
    "    arr_hhmm = CRS_ARR_TIME\n",
    "    dep_tz = origin_timezone\n",
    "    arr_tz = dest_timezone\n",
    "    flight_dur = CRS_ELAPSED_TIME (for sanity check of arrival time)\n",
    "\n",
    "    Returns UTC time stamp, (cast to string)\n",
    "    \"\"\"\n",
    "\n",
    "    dep_hhmm = int(dep_hhmm)\n",
    "    arr_hhmm = int(arr_hhmm)\n",
    "\n",
    "    yyyy,MM,dd = yyyymmdd.split('-')\n",
    "    yyyy = int(yyyy) # get year\n",
    "    MM = int(MM) # get month\n",
    "    dd = int(dd) # get day\n",
    "\n",
    "    dep_hh = dep_hhmm//100 # get hour\n",
    "    dep_mm = dep_hhmm%100 # get minute\n",
    "    if dep_hh == 24:\n",
    "        dep_hh = 0\n",
    "        dep_shift = True\n",
    "    else:\n",
    "        dep_shift = False\n",
    "\n",
    "    arr_hh = arr_hhmm//100 # get hour\n",
    "    arr_mm = arr_hhmm%100\n",
    "    if arr_hh == 24:\n",
    "        arr_hh = 0\n",
    "        arr_shift = True\n",
    "    else:\n",
    "        arr_shift = False\n",
    "\n",
    "    # create datetime variable for departure\n",
    "    dt_dep = datetime(yyyy,MM,dd,dep_hh,dep_mm)\n",
    "    if dep_shift:\n",
    "        dt_dep += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    dep_local = pytz.timezone(dep_tz).localize(dt_dep)\n",
    "    # convert to UTC\n",
    "    dep_utc = dep_local.astimezone(pytz.utc)\n",
    "\n",
    "    # create datetime variable for arrival\n",
    "    dt_arr = datetime(yyyy,MM,dd,arr_hh,arr_mm)\n",
    "    if arr_shift:\n",
    "        dt_arr += timedelta(days=1)\n",
    "    # apply local time zone\n",
    "    arr_local = pytz.timezone(arr_tz).localize(dt_arr)\n",
    "    # convert to UTC\n",
    "    arr_utc = arr_local.astimezone(pytz.utc)\n",
    "\n",
    "    if dep_utc > arr_utc:\n",
    "        arr_utc += timedelta(days=1)\n",
    "\n",
    "    # # sanity check\n",
    "    # arr_utc_SC = dep_utc + timedelta(minutes=flight_dur)\n",
    "\n",
    "    dt_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "    # return UTC datetime, cast to string\n",
    "    # return (dep_utc.strftime(dt_format), arr_utc.strftime(dt_format), arr_utc_SC.strftime(dt_format))\n",
    "    return (dep_utc.strftime(dt_format), arr_utc.strftime(dt_format))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"dep_datetime\", StringType(), False),\n",
    "    StructField(\"arr_datetime\", StringType(), False),\n",
    "])\n",
    "\n",
    "dt_udf = udf(to_utc, schema)\n",
    "\n",
    "out = df.withColumn('processed', \n",
    "                                 dt_udf(F.col(\"FL_DATE\"), \n",
    "                                        F.col(\"CRS_DEP_TIME\"), \n",
    "                                        F.col(\"CRS_ARR_TIME\"), \n",
    "                                        F.col(\"origin_timezone\"), \n",
    "                                        F.col(\"dest_timezone\"), \n",
    "                                        F.col(\"CRS_ELAPSED_TIME\"))\n",
    "                                 ).cache()\n",
    "\n",
    "cols = [c for c in out.columns if c != \"processed\"]\n",
    "cols += [\"processed.dep_datetime\",\"processed.arr_datetime\"]\n",
    "out = out.select(cols)\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b76b63-8fad-4607-8f3c-affd61a16c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_dated = (df\n",
    ".withColumn(\"FL_YMD\", F.col(\"FL_DATE\").substr(0,10))\n",
    ".withColumn(\n",
    "    \"base_date\", F.to_date(F.col(\"FL_YMD\"), \"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"arr_time_adj\",\n",
    "    F.when(F.col(\"CRS_ARR_TIME\") == 2400, F.lit(\"0000\")).otherwise(F.col(\"CRS_ARR_TIME\"))\n",
    ").withColumn(\n",
    "    \"arr_time_str\",\n",
    "    F.lpad(\"arr_time_adj\", 4, \"0\")\n",
    ").withColumn(\n",
    "    \"arr_timestamp\",\n",
    "    F.concat(\n",
    "        F.col(\"FL_YMD\"),\n",
    "        F.lit(\"T\"),\n",
    "        F.substring(\"arr_time_str\", 1, 2),\n",
    "        F.lit(\":\"),\n",
    "        F.substring(\"arr_time_str\", 3, 2),\n",
    "        F.lit(\":00\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"local_datetime\",\n",
    "    F.to_timestamp(\"arr_timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ").withColumn(\n",
    "    \"arr_utc\",\n",
    "    F.when(\n",
    "        F.col(\"CRS_ARR_TIME\") == 2400,\n",
    "        F.to_utc_timestamp(F.date_add(\"local_datetime\", 1), F.col(\"dest_timezone\"))\n",
    "    ).otherwise(\n",
    "        F.to_utc_timestamp(\"local_datetime\", F.col(\"dest_timezone\"))\n",
    "    )\n",
    ").withColumn(\n",
    "    \"sched_arr_utc\",\n",
    "    F.when(\n",
    "        F.col(\"sched_depart_utc\") > F.col(\"arr_utc\"),\n",
    "        F.date_add(\"arr_utc\", 1)\n",
    "    ).otherwise(F.col(\"arr_utc\"))\n",
    ").withColumn(\n",
    "    \"actual_depart_utc\",\n",
    "    F.to_utc_timestamp(F.col(\"sched_depart_utc\"), F.col(\"origin_timezone\"))\n",
    "\n",
    ").select(\n",
    "    *[c for c in df.columns],\n",
    "    \"sched_arr_utc\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e27562-be20-430b-bcce-d286d2578875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dated = (df_dated\n",
    ".withColumn(\"FL_YMD\", F.col(\"FL_DATE\").substr(0,10))\n",
    ".withColumn(\n",
    "    \"base_date\", F.to_date(F.col(\"FL_YMD\"), \"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"arr_time_adj\",\n",
    "    F.when(F.col(\"ARR_TIME\") == 2400, F.lit(\"0000\")).otherwise(F.col(\"ARR_TIME\"))\n",
    ").withColumn(\n",
    "    \"arr_time_str\",\n",
    "    F.lpad(\"arr_time_adj\", 4, \"0\")\n",
    ").withColumn(\n",
    "    \"arr_timestamp\",\n",
    "    F.concat(\n",
    "        F.col(\"FL_YMD\"),\n",
    "        F.lit(\"T\"),\n",
    "        F.substring(\"arr_time_str\", 1, 2),\n",
    "        F.lit(\":\"),\n",
    "        F.substring(\"arr_time_str\", 3, 2),\n",
    "        F.lit(\":00\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"local_datetime\",\n",
    "    F.to_timestamp(\"arr_timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ").withColumn(\n",
    "    \"arr_utc\",\n",
    "    F.when(\n",
    "        F.col(\"ARR_TIME\") == 2400,\n",
    "        F.to_utc_timestamp(F.date_add(\"local_datetime\", 1), F.col(\"dest_timezone\"))\n",
    "    ).otherwise(\n",
    "        F.to_utc_timestamp(\"local_datetime\", F.col(\"dest_timezone\"))\n",
    "    )\n",
    ").withColumn(\n",
    "    \"actual_arr_utc\",\n",
    "    F.when(\n",
    "        F.col(\"actual_depart_utc\") > F.col(\"arr_utc\"),\n",
    "        F.date_add(\"arr_utc\", 1)\n",
    "    ).otherwise(F.col(\"arr_utc\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65cff5d9-c818-4731-9401-8076bbb3fc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dated = df_dated.drop('arr_time_adj').drop('arr_time_str').drop('arr_timestamp').drop('arr_utc').drop('local_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79d2f1d-ccf6-4995-ac89-3110286f5236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(F.col('origin_LATITUDE').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7085b87e-d7e6-455f-ae0b-9026c3f31a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_lags_optimized(df):\n",
    "    # Define windows once\n",
    "    aircraft_window = Window.partitionBy(\"TAIL_NUM\").orderBy('sched_depart_utc')\n",
    "\n",
    "    # route_window = Window.partitionBy(\"ORIGIN\", \"DEST\").orderBy(\"sched_depart_utc\").rowsBetween(-10, -1)\n",
    "\n",
    "    WhenConditions = (\n",
    "        (F.col(\"ORIGIN\") == F.col(\"priorflight_dest\")) & \n",
    "        (F.col(\"priorflight_sched_deptime\") >= F.col(\"twentysix_hours_prior_depart_UTC\"))\n",
    "    )\n",
    "\n",
    "    # Precompute all lagged columns in single pass\n",
    "    lagged_cols = [\n",
    "        F.lag(\"CANCELLED\").over(aircraft_window).alias(\"priorflight_cancelled_true\"),\n",
    "        F.lag(\"ORIGIN\").over(aircraft_window).alias(\"priorflight_origin\"),\n",
    "        F.lag(\"DEST\").over(aircraft_window).alias(\"priorflight_dest\"),\n",
    "        F.lag(\"sched_depart_utc\").over(aircraft_window).alias(\"priorflight_sched_deptime\"),\n",
    "        F.lag(\"actual_depart_utc\").over(aircraft_window).alias(\"priorflight_true_deptime\"),\n",
    "        F.lag(\"CRS_ELAPSED_TIME\").over(aircraft_window).alias(\"priorflight_sched_elapsed\"),\n",
    "        F.lag(\"ACTUAL_ELAPSED_TIME\").over(aircraft_window).alias(\"priorflight_true_elapsed\"),\n",
    "        F.lag(\"DEP_DELAY\").over(aircraft_window).alias(\"priorflight_true_depdelay\"),\n",
    "        F.lag(\"sched_arr_utc\").over(aircraft_window).alias(\"priorflight_sched_arrtime\"),\n",
    "        F.lag(\"actual_arr_UTC\").over(aircraft_window).alias(\"priorflight_true_arrtime\")\n",
    "    ]\n",
    "\n",
    "    valid_prior = WhenConditions & (F.col(\"priorflight_cancelled_true\") == 0)\n",
    "\n",
    "\n",
    "    # Base transformations\n",
    "    base_df = (df\n",
    "        .withColumn(\"twentysix_hours_prior_depart_UTC\", \n",
    "                   (F.col(\"two_hours_prior_depart_UTC\") - F.expr(\"INTERVAL 24 HOURS\")).cast(\"timestamp\"))\n",
    "        .select(\"*\", *lagged_cols)\n",
    "    )    \n",
    "\n",
    "    # Core calculations\n",
    "    result_df = (base_df\n",
    "        .withColumn(\"priorflight_sched_elapsed\",\n",
    "            F.when(valid_prior,\n",
    "                F.expr(\"INTERVAL 1 MINUTE\") * F.col(\"priorflight_sched_elapsed\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        .withColumn(\"priorflight_true_elapsed\",\n",
    "                F.when(valid_prior,\n",
    "                    F.expr(\"INTERVAL 1 MINUTE\")* F.col(\"priorflight_true_elapsed\")\n",
    "                    ) \n",
    "                    \n",
    "                )\n",
    "\n",
    "        \n",
    "        .withColumn(\"priorflight_depdelay_calc\",\n",
    "            F.when(valid_prior, F.col(\"priorflight_true_depdelay\")).otherwise(F.lit(None))\n",
    "        )\n",
    "\n",
    "        .withColumn(\"priorflight_isdeparted\",\n",
    "            F.when(\n",
    "                (F.col(\"priorflight_true_deptime\") <= F.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                valid_prior, 1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "\n",
    "        .withColumn(\"priorflight_depdelay_calc\",\n",
    "            F.when(\n",
    "                (F.col(\"priorflight_true_deptime\") <= F.col(\"two_hours_prior_depart_UTC\")) & valid_prior,\n",
    "                F.col(\"priorflight_true_depdelay\")\n",
    "            ).when(\n",
    "                (F.col(\"priorflight_sched_deptime\") <= F.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                (F.col(\"priorflight_true_deptime\") > F.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                valid_prior,\n",
    "                (F.col(\"two_hours_prior_depart_UTC\").cast('long') - \n",
    "                 F.col(\"priorflight_sched_deptime\").cast('long')) / 60\n",
    "            ).otherwise(F.lit(0.0)) #if not enough info, assume all is well - in line with other logic; edge cases handled later\n",
    "        )\n",
    "        .withColumn(\"priorflight_deptime_calc\",  \n",
    "            F.col(\"priorflight_sched_deptime\") + \n",
    "            (F.expr(\"INTERVAL 1 MINUTE\") * F.col(\"priorflight_depdelay_calc\"))\n",
    "        ) #non-valid prior depdelay calcs will get rewritten over in edge case handling\n",
    "        \n",
    "        .withColumn(\"priorflight_isdelayed_calc\",\n",
    "            F.when(\n",
    "                (F.col(\"priorflight_depdelay_calc\") >= 15) | \n",
    "                (F.col('priorflight_cancelled_true') == 1), 1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "\n",
    "        .withColumn(\"priorflight_isarrived_calc\",\n",
    "            F.when(\n",
    "                (F.col(\"priorflight_true_arrtime\") <= F.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                valid_prior, 1\n",
    "            ).otherwise(0)\n",
    "        )\n",
    "        .withColumn(\"priorflight_arr_time_calc\",\n",
    "            F.when(\n",
    "                F.col(\"priorflight_isarrived_calc\") == 1,\n",
    "                F.col(\"priorflight_true_arrtime\")\n",
    "            ).when(\n",
    "                (F.col(\"priorflight_isarrived_calc\") == 0) &\n",
    "                (F.col(\"priorflight_true_deptime\") <= F.col(\"two_hours_prior_depart_UTC\")), \n",
    "                F.col(\"priorflight_true_deptime\") + F.col(\"priorflight_sched_elapsed\")\n",
    "            ).otherwise(\n",
    "                F.col(\"priorflight_deptime_calc\") + F.col(\"priorflight_sched_elapsed\")\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"turnaround_time_calc\",\n",
    "            F.when(valid_prior,\n",
    "                ((F.col(\"sched_depart_utc\").cast(\"long\") - \n",
    "                  F.col(\"priorflight_arr_time_calc\").cast(\"long\")) / 60).cast(\"double\")\n",
    "            ).otherwise(F.lit(None))\n",
    "        )\n",
    "        \n",
    "    ).cache()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# # Execute pipeline\n",
    "# result = add_lags_optimized(out)\n",
    "# display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba767df-be45-45df-9943-55809862bf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9505525-a0c3-4c8f-b99b-fe6be095ff70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aircraft_window = Window.partitionBy(\"TAIL_NUM\").orderBy('sched_depart_utc')\n",
    "\n",
    "    # route_window = Window.partitionBy(\"ORIGIN\", \"DEST\").orderBy(\"sched_depart_utc\").rowsBetween(-10, -1)\n",
    "\n",
    "WhenConditions = (\n",
    "        (F.col(\"ORIGIN\") == F.col(\"priorflight_dest\")) & \n",
    "        (F.col(\"priorflight_sched_deptime\") >= F.col(\"twentysix_hours_prior_depart_UTC\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dffc4858-2c77-434a-9cbd-0d652bbb7442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_= df.withColumn(\"priorflight_carrier\", F.lag(\"OP_UNIQUE_CARRIER\").over(aircraft_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a674e431-81dc-432f-8478-558015abca21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_= df_.withColumn(\"priororigin_mean_dep_delay\", F.lag(\"mean_dep_delay\").over(aircraft_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28bd7763-675e-494e-bb29-a79b260a7605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_= df_.withColumn(\"priororigin_type\", F.lag(\"origin_type\").over(aircraft_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31e928c-c575-486f-a154-db71dfbfb5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_.select('priororigin_mean_dep_delay').limit(100).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5645fb0a-e509-4557-898c-0dc889e5c6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select('route_risk').limit(100).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d64f0a48-6e40-41d4-bbd2-ae157b2633ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d42ba0f4-8bde-48b1-be29-d50fcb7510bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f1a637-6e44-4b4b-9e3c-92a7ee3cf870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Don't run: imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bbcb4da-5268-4cb2-88c4-354139038115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "040e0992-b4da-48a3-850a-fd7217e93798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "WhenConditions = (\n",
    "        (F.col(\"ORIGIN\") == F.col(\"priorflight_dest\")) & \n",
    "        (F.col(\"priorflight_sched_deptime\") >= F.col(\"twentysix_hours_prior_depart_UTC\"))\n",
    "    )\n",
    "valid_prior = WhenConditions & (F.col(\"priorflight_cancelled_true\") == 0)\n",
    "\n",
    "# partition by route (ORIGIN->DEST)\n",
    "hours = lambda i: i * 3600\n",
    "window_spec = Window.partitionBy(F.col(\"ORIGIN\"),F.col(\"DEST\"), F.col(\"FL_DATE\")) \\\n",
    "    .orderBy(F.col(\"sched_depart_utc\").cast(\"long\")\n",
    "             ) \\\n",
    "        .rangeBetween(-hours(48),0)\n",
    "# we will eventually get just -4 to -2 hours, but using 0 in the window allows us to\n",
    "# grab the utc-2 for the 0 hour offset case\n",
    "\n",
    "df_routes = df_lags.repartition(\"ORIGIN\", \"DEST\", \"FL_DATE\")\n",
    "\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def mean_turnarounds_udf(turnarounds: pd.Series, \n",
    "                       act_dep_times: pd.Series, \n",
    "                       sched_dep_utc2: pd.Series) -> float:\n",
    "    d = turnarounds[(act_dep_times < np.max(sched_dep_utc2))].astype(np.float)\n",
    "    return np.nanmean(d)\n",
    "\n",
    "# Apply the UDF over the window\n",
    "df_lags_imputed = df_routes \\\n",
    "    .withColumn(\"mean_turnaround_calc\", \n",
    "        F.when(~valid_prior,\n",
    "        mean_turnarounds_udf(\n",
    "                F.col(\"turnaround_time_calc\"),\n",
    "                F.col(\"actual_depart_utc\"),\n",
    "                F.col(\"two_hours_prior_depart_UTC\")\n",
    "            ).over(window_spec)).otherwise(F.col(\"turnaround_time_calc\"))\n",
    "        \n",
    "        )\n",
    "\n",
    "df_lags_imputed.cache()\n",
    "display(df_lags_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08e4237-e641-4ab2-b756-0df16d3580dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_lags_imputed.filter(F.col('turnaround_time_calc').isNull()).filter(F.col('mean_turnaround_calc').isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4da84e-c9fa-4818-af38-0b14043d6314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_lags.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31a159b2-e4b9-4c58-812a-3e8992dcfeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_lags_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "500f86ea-442a-4121-a854-d80e509222a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current = df_routes.alias(\"current\")\n",
    "prior = df_routes.alias(\"prior\")\n",
    "\n",
    "join_cond = (\n",
    "    (F.col(\"current.ORIGIN\") == F.col(\"prior.ORIGIN\")) &\n",
    "    (F.col(\"current.DEST\") == F.col(\"prior.DEST\")) &\n",
    "    (F.col(\"current.FL_DATE\") == F.col(\"prior.FL_DATE\")) &\n",
    "    (F.col(\"prior.actual_depart_utc\") <= F.col(\"current.two_hours_prior_depart_UTC\"))\n",
    ")\n",
    "\n",
    "result = (\n",
    "    current.join(prior, join_cond, \"left\")\n",
    "    .groupBy(\"current.*\")\n",
    "    .agg(F.avg(\"prior.turnaround_time_calc\").alias(\"mean_turnaround_calc\"))\n",
    "    .withColumn(\n",
    "        \"turnaround_time_calc\",\n",
    "        F.when(\n",
    "            ~valid_prior,  # Use your existing valid_prior condition\n",
    "            F.col(\"mean_turnaround_calc\")\n",
    "        ).otherwise(F.col(\"turnaround_time_calc\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cbbeaa5e-ba85-4ed7-be48-b8a19883e43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "valid_prior = (\n",
    "    (F.col(\"ORIGIN\") == F.col(\"priorflight_dest\")) &\n",
    "    (F.col(\"priorflight_sched_deptime\") >= F.col(\"twentysix_hours_prior_depart_UTC\")) &\n",
    "    (F.col(\"priorflight_cancelled_true\") == 0)\n",
    ")\n",
    "\n",
    "hours = lambda i: i * 3600\n",
    "\n",
    "# window to include prior 72 of flights on same route\n",
    "window_spec = Window.partitionBy(\"ORIGIN\", \"DEST\") \\\n",
    "                   .orderBy(F.col(\"sched_depart_utc\").cast(\"long\")) \\\n",
    "                   .rangeBetween(-hours(72),0)\n",
    "\n",
    "# calculate conditional average\n",
    "df_optimized = df_lags \\\n",
    "    .withColumn(\"valid_turnaround\",\n",
    "        F.when(\n",
    "            valid_prior,\n",
    "            F.col(\"turnaround_time_calc\")\n",
    "        ).otherwise(F.lit(None)) #only consider valid prior flights' turnaround times\n",
    "    ) \\\n",
    "    .withColumn(\"mean_turnaround\",\n",
    "        F.avg(\"valid_turnaround\").over(window_spec) #take mean over valid turnaround times\n",
    "    ) \\\n",
    "    .withColumn(\"turnaround_time_calc\",\n",
    "        F.when(\n",
    "            (~valid_prior | F.col('turnaround_time_calc').isNull()), \n",
    "               F.col(\"mean_turnaround\")\n",
    "               ) #replace flight turnarounds that have invalid priors to mean over past 72 hours of that route being flown\n",
    "          .otherwise(F.col(\"turnaround_time_calc\"))\n",
    "    ).cache()\n",
    "\n",
    "display(df_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ce62398b-3ed3-4671-beda-6b2cf826291c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "valid_prior = (\n",
    "    (F.col(\"ORIGIN\") == F.col(\"priorflight_dest\")) &\n",
    "    (F.col(\"priorflight_sched_deptime\") >= F.col(\"twentysix_hours_prior_depart_UTC\")) &\n",
    "    (F.col(\"priorflight_cancelled_true\") == 0)\n",
    ")\n",
    "\n",
    "hours = lambda i: i * 3600\n",
    "\n",
    "# window to include prior 72 of flights on same route\n",
    "window_spec = Window.partitionBy(\"ORIGIN\", \"DEST\") \\\n",
    "                   .orderBy(F.col(\"sched_depart_utc\").cast(\"long\")) \\\n",
    "                   .rangeBetween(-hours(72),0)\n",
    "\n",
    "# calculate conditional average\n",
    "df_optimized = df_lags \\\n",
    "    .withColumn(\"valid_turnaround\",\n",
    "        F.when(\n",
    "            valid_prior,\n",
    "            F.col(\"turnaround_time_calc\")\n",
    "        ).otherwise(F.lit(None)) #only consider valid prior flights' turnaround times\n",
    "    ) \\\n",
    "    .withColumn(\"mean_turnaround\",\n",
    "        F.avg(\"valid_turnaround\").over(window_spec) #take mean over valid turnaround times\n",
    "    ) \\\n",
    "    .withColumn(\"turnaround_time_calc\",\n",
    "        F.when(\n",
    "            (~valid_prior | F.col('turnaround_time_calc').isNull()), \n",
    "               F.col(\"mean_turnaround\")\n",
    "               ) #replace flight turnarounds that have invalid priors to mean over past 72 hours of that route being flown\n",
    "          .otherwise(F.col(\"turnaround_time_calc\"))\n",
    "    ).cache()\n",
    "\n",
    "display(df_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79a76fcf-3716-4d8c-80f9-36a6069910f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "        # # Edge case handling - use Erica's code for: \n",
    "        # #1) revise a bit for turnaround time calc - avg over route prior flights\n",
    "        # #2) use original for priorflight depdelay calc - lag mean delay (@prev origin)\n",
    "        # #3) then update priorflight_isdelayed_calc\n",
    "        # .withColumn(\"turnaround_time_calc\",\n",
    "        #     F.when(\n",
    "        #         (~valid_prior),\n",
    "        #         F.last(\"turnaround_time_calc\", ignorenulls=True).over(route_window)\n",
    "        #     ).otherwise(F.col(\"turnaround_time_calc\"))\n",
    "        # )\n",
    "        # .withColumn(\"priorflight_depdelay_calc\",\n",
    "        #     F.when(\n",
    "        #         (~valid_prior),\n",
    "        #         F.last(\"priorflight_depdelay_calc\", ignorenulls=True).over(route_window)\n",
    "        #     ).otherwise(F.col(\"priorflight_depdelay_calc\"))\n",
    "        # )\n",
    "\n",
    "        # .withColumn(\"priorflight_isdelayed_calc\",\n",
    "        #     F.when(\n",
    "        #         (F.col(\"priorflight_depdelay_calc\") >= 15) | \n",
    "        #         (F.col('priorflight_cancelled_true') == 1), 1\n",
    "        #     ).otherwise(0)\n",
    "        # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f0f068-f261-476d-9e7c-d8fdf308341d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "080f1908-f526-47e3-97be-0ea60061480c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e081e1a5-b517-414b-9317-d2c85e815dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Brainstorming:\n",
    "\n",
    "Pagerank\n",
    "-  makes sense to do it within windows; airports most popular in the winter won't be the most popular in the summer, plus reasonable assumption for scheduling vs leakage \n",
    "- personalized pr - should be teleportation factor preference for possible dest wrt origin\n",
    "  - convert to graphx first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4275adcd-207f-4824-a5e5-811d1d8910c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e77ac7-7550-4a19-abab-b06d632575d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "v = df_lags.select(\n",
    "    F.col(\"ORIGIN\").alias(\"id\")\n",
    ").union(\n",
    "    df_lags.select(F.col(\"DEST\").alias(\"id\"))\n",
    ").distinct()\n",
    "\n",
    "\n",
    "e = df_lags.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\")\n",
    ")\n",
    "\n",
    "g = GraphFrame(v, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc6c230-48c3-40d0-b3fb-b5fa057a71ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define fold date ranges based on erica's cv\n",
    "folds = [\n",
    "    {\"fold\": 'train_0', \"date_min\": \"2014-12-31\", \"date_max\": \"2015-10-09\"},\n",
    "    {\"fold\": 'test_0', \"date_min\": \"2015-10-09\", \"date_max\": \"2016-07-17\"},\n",
    "    {\"fold\": 'train_1', \"date_min\": \"2015-08-14\", \"date_max\": \"2016-05-21\"},\n",
    "    {\"fold\": 'test_1', \"date_min\": \"2016-05-21\", \"date_max\": \"2017-02-27\"},\n",
    "    {\"fold\": 'train_2', \"date_min\": \"2016-03-27\", \"date_max\": \"2017-01-01\"},\n",
    "    {\"fold\": 'test_2', \"date_min\": \"2017-01-01\", \"date_max\": \"2017-10-10\"},\n",
    "    {\"fold\": 'train_3', \"date_min\": \"2016-11-08\", \"date_max\": \"2017-08-14\"},\n",
    "    {\"fold\": 'test_3', \"date_min\": \"2017-08-14\", \"date_max\": \"2018-05-23\"},\n",
    "    {\"fold\": 'train_4', \"date_min\": \"2017-06-22\", \"date_max\": \"2018-03-27\"},\n",
    "    {\"fold\": 'test_4', \"date_min\": \"2018-03-27\", \"date_max\": \"2018-01-01\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba02f795-8ca7-4ef1-a31b-025828b284ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_pagerank_on_folds(df, date_column='sched_depart_utc', folds=folds):\n",
    "    results = []\n",
    "    \n",
    "    for fold in folds:\n",
    "        print(f\"Processing fold {fold['fold']}\")\n",
    "        \n",
    "        # Filter vertices for training period\n",
    "        fold_df = df.filter(\n",
    "            (F.col(date_column) >= fold['date_min']) & \n",
    "            (F.col(date_column) < fold['date_max'])\n",
    "        )\n",
    "        \n",
    "        vertices_df = fold_df.select(F.col(\"ORIGIN\").alias(\"id\")\n",
    "                 ).union(\n",
    "                     df_lags.select(F.col(\"DEST\").alias(\"id\"))\n",
    "                     ).distinct()\n",
    "        \n",
    "        edges_df = fold_df.select(\n",
    "                F.col(\"ORIGIN\").alias(\"src\"),\n",
    "                F.col(\"DEST\").alias(\"dst\")\n",
    "            )\n",
    "        \n",
    "        # Create GraphFrame for this fold\n",
    "        g = GraphFrame(vertices_df, edges_df)\n",
    "        \n",
    "        # Run PageRank\n",
    "        pagerank_results = g.pageRank(resetProbability=.15,\n",
    "                                      maxIter= 10)\n",
    "        \n",
    "        # Add fold information\n",
    "        pagerank_results_with_fold = pagerank_results.vertices.withColumn(\"fold\", F.lit(fold['fold']))\n",
    "        \n",
    "        results.append(pagerank_results_with_fold)\n",
    "\n",
    "    # Combine all results\n",
    "    all_results = results[0]\n",
    "    for i in range(1, len(results)):\n",
    "        all_results = all_results.union(results[i])\n",
    "        \n",
    "    return all_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c88bcce2-e7ad-4b1d-aa50-7392220574b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_df= run_pagerank_on_folds(df_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "88511076-94ba-4d91-89e0-70121521b992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268e18c6-0be1-4cac-97eb-bc0b325fbb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = g.pageRank(resetProbability=0.15, maxIter = 10)\n",
    "\n",
    "\n",
    "results.vertices.select(\"id\", \"pagerank\").show()\n",
    "results.edges.select(\"src\", \"dst\", \"weight\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8932c795-57c9-46bb-91fe-3d406bdee109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.edges.select('src','dst','weight').distinct().orderBy(F.col('weight').desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b464e3-8733-418a-b398-7d224dd730cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.vertices.select(\"id\", \"pagerank\").orderBy(F.col('pagerank').desc()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50a86dfb-44c9-43d1-92f7-0bfec1d12aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# paths = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(a)\")#need this to acocunt for time/immediate t+1 step though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17d17c6-2a4d-4822-a3ed-baa5a9bdd574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/pagerank.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e131d0f-fc2b-4a0e-8ecf-d4b4c0ac37dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dd63199-85ea-4e5f-ad08-e6d691240c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b319a880-94ab-477b-94c9-3b05f8e7552d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d327628a-6d65-453c-bfa5-0ab8e0a36b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/pagerank.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca61568-524e-4c91-bdbd-279a22afcd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define fold date ranges based on erica's cv\n",
    "folds = [\n",
    "    {\"fold\": 'train_0', \"date_min\": \"2014-12-31\", \"date_max\": \"2015-10-09\"},\n",
    "    {\"fold\": 'test_0', \"date_min\": \"2015-10-09\", \"date_max\": \"2016-07-17\"},\n",
    "    {\"fold\": 'train_1', \"date_min\": \"2015-08-14\", \"date_max\": \"2016-05-21\"},\n",
    "    {\"fold\": 'test_1', \"date_min\": \"2016-05-21\", \"date_max\": \"2017-02-27\"},\n",
    "    {\"fold\": 'train_2', \"date_min\": \"2016-03-27\", \"date_max\": \"2017-01-01\"},\n",
    "    {\"fold\": 'test_2', \"date_min\": \"2017-01-01\", \"date_max\": \"2017-10-10\"},\n",
    "    {\"fold\": 'train_3', \"date_min\": \"2016-11-08\", \"date_max\": \"2017-08-14\"},\n",
    "    {\"fold\": 'test_3', \"date_min\": \"2017-08-14\", \"date_max\": \"2018-05-23\"},\n",
    "    {\"fold\": 'train_4', \"date_min\": \"2017-06-22\", \"date_max\": \"2018-03-27\"},\n",
    "    {\"fold\": 'test_4', \"date_min\": \"2018-03-27\", \"date_max\": \"2019-01-01\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6ebe05-1f78-4492-9dee-25ddad906e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr.filter(F.col('id')=='ATW').filter(F.col('fold')==0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586361c4-ff0a-4463-9e85-881bf960fff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('fold').orderBy('id')\n",
    "\n",
    "pr_folds = pr.withColumn('row_num', F.row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7d2d4c-2cdd-4c74-b451-69f9e3b6767b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('fold', 'id').orderBy('id')\n",
    "\n",
    "pr_folds = pr.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "pr_folds = pr_folds.withColumn('row_num', F.when(F.col('row_num') > 1, 2).otherwise(F.col('row_num')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d76d403-8620-4267-90af-0cddecabf24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(F.col('sched_depart_utc') > folds[0]['date_min']) \\\n",
    "  .filter(F.col('sched_depart_utc') <= folds[0]['date_max']) \\\n",
    "  .select('ORIGIN', 'DEST') \\\n",
    "  .select(F.explode(F.array('ORIGIN', 'DEST')).alias('airport')) \\\n",
    "  .distinct() \\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cde4ff-46cc-48f6-93b9-5fdcb736420f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(F.col('sched_depart_utc') > folds[1]['date_min']) \\\n",
    "  .filter(F.col('sched_depart_utc') <= folds[1]['date_max']) \\\n",
    "  .select('ORIGIN', 'DEST') \\\n",
    "  .select(F.explode(F.array('ORIGIN', 'DEST')).alias('airport')) \\\n",
    "  .distinct() \\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "952a4837-de59-415a-a733-55eb0f597cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds_df = spark.createDataFrame(folds)\n",
    "\n",
    "df_with_folds = df.join(\n",
    "    folds_df,\n",
    "    (df['sched_depart_utc'] > folds_df['date_min']) & (df['sched_depart_utc'] <= folds_df['date_max']),\n",
    "    'inner'\n",
    ").select(df['*'], folds_df['fold'])\n",
    "\n",
    "display(df_with_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d217a0-4ad0-4e58-bd5d-6e9e494f3ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_folds.filter(F.col('fold').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aef63e-5bbe-4b16-85ff-cb6abc9e5942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_folds = pr_folds.withColumn(\n",
    "    'fold_label',\n",
    "    when((F.col('fold') == 0) & (F.col('row_num') == 1), 'train_0')\n",
    "    .when((F.col('fold') == 0) & (F.col('row_num') == 2), 'test_0')\n",
    "    .when((F.col('fold') == 1) & (F.col('row_num') == 1), 'train_1')\n",
    "    .when((F.col('fold') == 1) & (F.col('row_num') == 2), 'test_1')\n",
    "    .when((F.col('fold') == 2) & (F.col('row_num') == 1), 'train_2')\n",
    "    .when((F.col('fold') == 2) & (F.col('row_num') == 2), 'test_2')\n",
    "    .when((F.col('fold') == 3) & (F.col('row_num') == 1), 'train_3')\n",
    "    .when((F.col('fold') == 3) & (F.col('row_num') == 2), 'test_3')\n",
    "    .when((F.col('fold') == 4) & (F.col('row_num') == 1), 'train_4')\n",
    "    .when((F.col('fold') == 4) & (F.col('row_num') == 2), 'test_4')\n",
    ")\n",
    "\n",
    "display(pr_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84faa3e-9b41-4620-81f9-6a856c1bbc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr = df_with_folds.join(pr_folds.select('id','pagerank','fold_label'), \n",
    "                                (df_with_folds['fold'] == pr_folds['fold_label']) & \n",
    "                                (df_with_folds['ORIGIN'] == pr_folds['id']), \n",
    "                                'left')\n",
    "df_with_pr.filter(F.col('fold').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37e16e4-01e9-408b-a0de-e141aa91db41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr.groupBy('YEAR').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad9c8da-06fc-4a54-8d1e-906c847c5847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr.filter(F.col('TAIL_NUM').isNotNull()).filter(F.col('pagerank').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cbae4e6-bc6f-4928-8862-21e03e09ad5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr.write.mode('overwrite').parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37bc14aa-be82-4480-b7b9-c43d109e897c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_with_pr.filter(F.col('TAIL_NUM').isNotNull()).filter(F.col('pagerank').isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13683992-dd33-4476-a502-5701d8122afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_with_pr.filter(F.col('TAIL_NUM').isNotNull()).filter(F.col('pagerank').isNull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f31bb7a8-933b-41ec-b7f4-9941496aa040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## last folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07182adf-0238-489a-9974-b3747b25a5fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3de03b-c2b1-4da6-9c38-e88380ad0700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09cced24-0e75-40ea-bc51-d42d31693185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47975f57-8317-41d9-b489-11c084282b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('fold', 'id').orderBy('id')\n",
    "\n",
    "pr_folds = pr.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "pr_folds = pr_folds.withColumn('row_num', F.when(F.col('row_num') > 1, 2).otherwise(F.col('row_num')))\n",
    "\n",
    "pr_folds = pr_folds.withColumn(\n",
    "    'fold_label',\n",
    "    when((F.col('fold') == 0) & (F.col('row_num') == 1), 'train_0')\n",
    "    .when((F.col('fold') == 0) & (F.col('row_num') == 2), 'test_0')\n",
    "    .when((F.col('fold') == 1) & (F.col('row_num') == 1), 'train_1')\n",
    "    .when((F.col('fold') == 1) & (F.col('row_num') == 2), 'test_1')\n",
    "    .when((F.col('fold') == 2) & (F.col('row_num') == 1), 'train_2')\n",
    "    .when((F.col('fold') == 2) & (F.col('row_num') == 2), 'test_2')\n",
    "    .when((F.col('fold') == 3) & (F.col('row_num') == 1), 'train_3')\n",
    "    .when((F.col('fold') == 3) & (F.col('row_num') == 2), 'test_3')\n",
    "    .when((F.col('fold') == 4) & (F.col('row_num') == 1), 'train_4')\n",
    "    .when((F.col('fold') == 4) & (F.col('row_num') == 2), 'test_4')\n",
    ")\n",
    "\n",
    "display(pr_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f792dace-9296-4299-93b8-f41f8b6e8157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_folds = pr_folds.filter(F.col('fold_label') != 'test_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d4beeb-8d9c-4e1a-85ea-1bf5879d66ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pr_folds.filter(F.col('fold_label')=='train_4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4f4862-ec78-457f-8a59-d831b1c7bdd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/pagerank.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6231c8c-8642-4fe4-81ee-3b58198d5bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_pagerank_on_folds(df, date_column='sched_depart_utc', folds=folds):\n",
    "    results = []\n",
    "    \n",
    "    for fold in folds:\n",
    "        print(f\"Processing fold {fold['fold']}\")\n",
    "        \n",
    "        # Filter vertices for training period\n",
    "        fold_df = df.filter(\n",
    "            (F.col(date_column) >= fold['date_min']) & \n",
    "            (F.col(date_column) < fold['date_max'])\n",
    "        )\n",
    "        \n",
    "        vertices_df = fold_df.select(F.col(\"ORIGIN\").alias(\"id\")\n",
    "                 ).union(\n",
    "                     fold_df.select(F.col(\"DEST\").alias(\"id\"))\n",
    "                     ).distinct()\n",
    "        \n",
    "        edges_df = fold_df.select(\n",
    "                F.col(\"ORIGIN\").alias(\"src\"),\n",
    "                F.col(\"DEST\").alias(\"dst\")\n",
    "            )\n",
    "        \n",
    "        # Create GraphFrame for this fold\n",
    "        g = GraphFrame(vertices_df, edges_df)\n",
    "        \n",
    "        # Run PageRank\n",
    "        pagerank_results = g.pageRank(resetProbability=.15,\n",
    "                                      maxIter= 10)\n",
    "        \n",
    "        # Add fold information\n",
    "        pagerank_results_with_fold = pagerank_results.vertices.withColumn(\"fold\", F.lit(fold['fold']))\n",
    "        \n",
    "        results.append(pagerank_results_with_fold)\n",
    "\n",
    "    # Combine all results\n",
    "    all_results = results[0]\n",
    "    for i in range(1, len(results)):\n",
    "        all_results = all_results.union(results[i])\n",
    "        \n",
    "    return all_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdbc550-fcb1-47a6-952c-7f032758f6e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_final = run_pagerank_on_folds(df, folds=blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa44bbab-fbf7-40af-88b3-2148fc5416f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pr_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e813def-4c3c-4498-adaf-ca7fa1c8cbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_pr_folds = pr_folds.drop('row_num').drop('fold').withColumnRenamed('fold_label','fold').unionByName(pr_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d46aeaa-ea68-4a26-8993-19cf80a67ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(all_pr_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad247e8-2e29-447e-862c-6f642c7d56ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pr_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "954eaed4-23af-4678-8535-a5028390cfd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds_df = spark.createDataFrame(blocks)\n",
    "\n",
    "df_with_folds = df.join(\n",
    "    folds_df,\n",
    "    (df['sched_depart_utc'] > folds_df['date_min']) & (df['sched_depart_utc'] <= folds_df['date_max']),\n",
    "    'inner'\n",
    ").select(df['*'], folds_df['fold'])\n",
    "\n",
    "display(df_with_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80710509-cbda-4d40-bd5e-fece4743618f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "245234b8-e7f4-4d89-af6c-f4e3af123f35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_folds_final = df_with_folds.filter(F.col('fold').contains('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1ab2ba-2ce6-42ee-ad0d-b9f424236717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf7464d-b7f9-4807-a8a8-0bc3f692d4b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_with_pr_final = df_with_folds_final.join(\n",
    "    pr_final.select('id', 'pagerank', 'fold'),\n",
    "    (df_with_folds_final['fold'] == pr_final['fold']) & \n",
    "    (df_with_folds_final['ORIGIN'] == pr_final['id']),\n",
    "    'left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6906e592-cba3-47c0-a1ba-2274da9aa69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_df_full  = (df.join(all_pr_folds.filter(F.col('fold')=='train_0').withColumnRenamed('pagerank','train_0').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "        .join(all_pr_folds.filter(F.col('fold')=='test_0').withColumnRenamed('pagerank','test_0').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "        .join(all_pr_folds.filter(F.col('fold')=='train_1').withColumnRenamed('pagerank','train_1').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "        .join(all_pr_folds.filter(F.col('fold')=='test_1').withColumnRenamed('pagerank','test_1').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "        .join(all_pr_folds.filter(F.col('fold')=='train_2').withColumnRenamed('pagerank','train_2').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "        .join(all_pr_folds.filter(F.col('fold')=='test_2').withColumnRenamed('pagerank','test_2').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "        .join(all_pr_folds.filter(F.col('fold')=='train_3').withColumnRenamed('pagerank','train_3').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "        .join(dftest3.filter(F.col('fold') == 'test_3').withColumnRenamed('pagerank','test_3').drop('fold'), df['ORIGIN']==dftest3['id']).drop('id')\n",
    "        .join(dftrain4.filter(F.col('fold')=='train_4').withColumnRenamed('pagerank','train_4').drop('fold'), df['ORIGIN']==dftrain4['id']).drop('id')\n",
    "        .join(all_pr_folds.filter(F.col('fold')=='test_4').withColumnRenamed('pagerank','test_4').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "        .join(all_pr_folds.filter(F.col('fold')=='test').withColumnRenamed('pagerank','test').drop('fold'), df['ORIGIN']==all_pr_folds['id']).drop('id')\n",
    "\n",
    "        \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27c3c72-f832-420f-9c8f-1afcf200c5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr_final.filter(F.col('pagerank').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041393d3-fc33-4e88-94db-180b02b6741c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr=df_with_pr.filter(F.col('fold')!='test_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e530f8-b369-44ea-ab60-9b51f46ad3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad15c72-0355-477a-bd5b-47df9e18e43e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_pr = df_with_pr.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f2b26c-8dd4-4ebe-ae0e-b3e6f3dda74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "original_cols = df_with_pr_final.columns\n",
    "new_columns = original_cols.copy()\n",
    "new_columns[-1] = 'fold_label'  # Changes the last element from 'fold' to 'fold_label'\n",
    "df_with_pr_final = df_with_pr_final.toDF(*new_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c480a5-f3a2-4d27-b8e2-1c9e32501190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pr = df_with_pr.unionByName(df_with_pr_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49de8976-89d5-457a-b72d-0bff3771ce50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pr.groupBy('YEAR').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1572c414-b48b-4624-88cc-ed510c7b6cb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "min_date = df_pr.select(min(\"FL_DATE\")).first()[0]\n",
    "max_date = df_pr.select(max(\"FL_DATE\")).first()[0]\n",
    "\n",
    "min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "944836a2-87f0-460a-aec8-ed7305176ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_df_full.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05fa0ff7-84bd-48c7-9f3b-78d1ac87fa1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pr_df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7909476-93ff-4296-a2f8-2e4eea17512a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Graph Features Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfa784ed-dcd0-4524-826f-34bcaa191f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e33563-9851-4b76-8178-d6768f3c200d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec3d0a3-efba-4ce5-9db6-ac8d39c5faba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Idea: community detection. All airports are not concerned about all of each other, but would be concerned about the delays propagating within their communities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d0ee28-9337-4e37-b6d5-5147b04945d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecbf944-0758-48f1-9672-10054e04a50b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bba0bbe-467e-4981-ba83-eb71386a08bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ydf=df.filter(F.col('YEAR')==2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9705a8ea-d03e-4550-a463-a7cf9649a770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "v = ydf.select(\n",
    "    F.col(\"ORIGIN\").alias(\"id\")\n",
    ").union(\n",
    "    ydf.select(F.col(\"DEST\").alias(\"id\"))\n",
    ").distinct()\n",
    "\n",
    "\n",
    "e = ydf.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\")\n",
    ")\n",
    "\n",
    "g = GraphFrame(v, e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e641ff7-33a6-40c6-aefa-d9a7782169fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Label Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a82ec6a-7978-428e-905c-3c47da0cc6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Tl;dr as the spark docs says, literally just groups stuff into 1 label for the most part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07018cec-5cf9-4658-a54a-8bba29abf1d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = g.labelPropagation(maxIter=15)\n",
    "result.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ae2ed3-c815-4969-9baa-6ae1e1268471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "e.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f4a271-5939-4e6f-83a9-d9d3b94dd2c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.filter(F.col('label')=='1219770712064').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17f54316-340a-49a7-bb3e-1d2cd8ea6555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac2601fd-59b7-4b3f-a120-5363fcbe333f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Personalized PR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cdf06fd-1e69-4757-a7a9-237048517698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import MapType, IntegerType, DoubleType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4351282-4aac-46f3-a964-9bf9d7e8e38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Main idea: compute the importance of nodes relative to a specific source node or set of nodes. Then, can find metrics for relevant nodes. Here, conceptualizing a node = an airport.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a3bc2bcf-5152-417d-ad1a-279b93246153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ydf.groupBy(\"ORIGIN\").agg(F.countDistinct(\"DEST\").alias(\"dest_count\")).orderBy(F.desc(\"dest_count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0746c8-2176-4bde-b3d0-d8786925571f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Vertices (ordered alphabetically)\n",
    "v = ydf.select(F.col(\"ORIGIN\").alias(\"id\")) \\\n",
    "    .union(ydf.select(F.col(\"DEST\").alias(\"id\"))) \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"id\")\n",
    "\n",
    "# Edges (directed)\n",
    "e = ydf.select(F.col(\"ORIGIN\").alias(\"src\"), F.col(\"DEST\").alias(\"dst\"))\n",
    "\n",
    "# Build graph\n",
    "g = GraphFrame(v, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf071c4-9006-4785-a30f-2e3dec6832e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sources (ordered alphabetically to match vertices)\n",
    "sources_df = v.select(\"id\").distinct() \\\n",
    "    .orderBy(\"id\") \\\n",
    "    .withColumn(\"index\", F.row_number().over(Window.orderBy(\"id\")) - 1)\n",
    "\n",
    "sources_flat = sources_df.select(\"id\").rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478e1019-82b5-456d-b0e5-401ae028802e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run PPR with aligned sources\n",
    "pageranked = g.parallelPersonalizedPageRank(\n",
    "    resetProbability=0.15, \n",
    "    sourceIds=sources_flat, \n",
    "    maxIter=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1739822f-6bb6-4c9e-bac6-7fc189c6589d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked.vertices.filter(F.col('id')=='JFK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0b6197-be81-4ee6-9a5e-50400321fb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Check JFK's PPR scores\n",
    "jfk_index = sources_flat.index(\"JFK\")\n",
    "jfk_scores = pageranked.vertices.select(\"pageranks\").collect()[jfk_index]\n",
    "\n",
    "# Map scores to airport names\n",
    "sorted_scores = sorted(\n",
    "    zip(sources_flat, jfk_scores), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Exclude self-score (JFK)\n",
    "top_external = sorted_scores[1:11]  # Skip index 0 (JFK itself)\n",
    "print(top_external)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca3e533-7763-4897-853f-15890e6f9e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sources_flat[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e153c0-df32-4792-8b2c-b10162abc7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "broadcast_sources = sc.broadcast(sources_flat)\n",
    "\n",
    "result_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"origin\", StringType()),\n",
    "        StructField(\"score\", DoubleType())\n",
    "    ])\n",
    ")\n",
    "def vector_to_dict(vector):\n",
    "    # Retrieve the broadcasted list\n",
    "    sources = broadcast_sources.value\n",
    "    \n",
    "    # Sort the vector entries and take top 10\n",
    "    sorted_entries = sorted(\n",
    "        [(i, float(v)) for i, v in enumerate(vector)], \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )[:10]\n",
    "    \n",
    "    # Map indices to actual source IDs\n",
    "    return [(sources_flat[i], float(v)) for i, v in sorted_entries]\n",
    "\n",
    "# Define UDF\n",
    "vector_to_dict_udf = udf(vector_to_dict, result_schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = pageranked.vertices.withColumn(\"pagerank_dict\", vector_to_dict_udf(\"pageranks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118e558e-0726-4814-9f6c-41cdd5efdc0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[u for u in enumerate(sources_flat) if u[1]=='HYA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ce7064-60b5-4348-bed5-31b0fa65fcca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c040c863-0c68-47f2-84aa-843a848b78d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def vector_to_map(vector):\n",
    "    sources = broadcast_sources.value\n",
    "    indices = vector.indices.tolist()\n",
    "    values = vector.values.tolist()\n",
    "\n",
    "    return {sources[i]: float(v) for i, v in zip(indices, values)}\n",
    "\n",
    "vector_to_map_udf = udf(vector_to_map, MapType(StringType(), DoubleType()))\n",
    "results = results.withColumn(\"pagerank_map\", vector_to_map_udf(col(\"pageranks\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef6dad00-e1fc-49d5-90bb-5bbce96db804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "pageranks (VectorType): the pageranks of this vertex from all input source vertices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78f3d7b8-d93d-4915-b38c-85388ad33633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59cec8d9-2834-4e86-9e02-fef5d5a321bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked.edges.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b648a89-bda4-431d-9529-03b3abdc0dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7245d998-f798-49a3-a32d-730830fd2b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6950e138-c330-4312-babf-87501af1d7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.withColumn('pagerank', F.col('pageranks').cast('string')).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31dbdf8d-b4ce-4417-8e76-d95e55173e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "broadcast_sources = sc.broadcast(sources_list)\n",
    "\n",
    "result_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"origin\", StringType()),\n",
    "        StructField(\"score\", DoubleType())\n",
    "    ])\n",
    ")\n",
    "def vector_to_dict(vector):\n",
    "    # Retrieve the broadcasted list\n",
    "    sources = broadcast_sources.value\n",
    "    \n",
    "    # Sort the vector entries and take top 10\n",
    "    sorted_entries = sorted(\n",
    "        [(i, float(v)) for i, v in enumerate(vector)], \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )[:10]\n",
    "    \n",
    "    # Map indices to actual source IDs\n",
    "    return [(sources[i], float(v)) for i, v in sorted_entries]\n",
    "\n",
    "# Define UDF\n",
    "vector_to_dict_udf = udf(vector_to_dict, result_schema)\n",
    "\n",
    "# Apply UDF\n",
    "results = results.withColumn(\"pagerank_dict\", vector_to_dict_udf(\"pageranks\"))\n",
    "display(results.limit(5)) #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "761da4e4-59e1-49ff-8f33-8987ecac4639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.filter(F.col('id')=='JFK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd799bf-a8b4-48f2-ab47-1c1a1f765290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ydf.filter(F.col(\"ORIGIN\")==\"JFK\").filter(F.col(\"DEST\")==\"HYA\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864f3fbf-93b4-4ff9-9684-f32b7a4ecef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(F.col(\"ORIGIN\")==\"JFK\").groupBy(\"DEST\").count().orderBy(F.desc(F.col('count'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7c7ffc8-28f8-45c8-a864-e568e79e125c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(F.col(\"ORIGIN\")==\"HYA\").groupBy(\"DEST\").count().orderBy(F.desc(F.col('count'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62fae674-d631-48b1-baa6-b86a4344333e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(F.col(\"ORIGIN\")==\"HYA\").filter(F.col(\"DEST\")==\"JFK\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1652068e-a7eb-4b57-8d1a-2abb2862281c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(F.col(\"ORIGIN\")==\"JFK\").filter(F.col(\"DEST\")==\"HYA\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c85b65-0b77-4176-886c-4e74891c6f09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[i for i in enumerate(sources_list) if i[1]=='HYA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab0a44d-e411-42f4-a5b5-138a64085f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0256013-3a42-434b-a043-f3c37510b38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.filter(F.col('id')=='BGM').select(F.map_keys('pagerank_dict').cast(\"string\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99344690-ef56-4cef-8c2d-57b8c37490f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07aa861-8619-4a36-962d-6238df50bd73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results.select('id', 'pagerank_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc862057-1e67-4eba-9c23-120ece0da007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sources_list[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12087b81-2acd-49e1-ac8d-2488c1d018e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.vertices.withColumn(\"pagerank_dict\", vector_to_dict_udf(\"pageranks\")).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c53cfce-c09b-4668-9b7d-0af02d0ad6cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PPR: Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5047de67-a682-4db6-90b9-30d90bcbbed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7043a83b-9bd5-4c06-bc67-4b8db05ef39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e37f9b1b-dc37-43b0-9f5d-5b6d9af40006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e8601e-8aa4-4d3c-a7ac-30755b9c49ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('priorflight_sched_elapsed', f.col('priorflight_sched_elapsed').cast('int')/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "513decb0-647e-4648-90a9-e7de1e7b6317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.WithColumn('train',\n",
    "              F.when(F.col('sched_depart_utc') >= folds[0][\"date_min\"] & F.col('sched_depart_utc') <= folds[0][\"date_max\"], F.col('train_0').\n",
    "                     \n",
    "                     )\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec73642-6f6d-402c-95fb-f71396911178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define fold date ranges based on erica's cv\n",
    "folds = [\n",
    "    {\"fold\": 'train_0', \"date_min\": \"2014-12-31\", \"date_max\": \"2015-10-09\"},\n",
    "    {\"fold\": 'test_0', \"date_min\": \"2015-10-09\", \"date_max\": \"2016-07-17\"},\n",
    "    {\"fold\": 'train_1', \"date_min\": \"2015-08-14\", \"date_max\": \"2016-05-21\"},\n",
    "    {\"fold\": 'test_1', \"date_min\": \"2016-05-21\", \"date_max\": \"2017-02-27\"},\n",
    "    {\"fold\": 'train_2', \"date_min\": \"2016-03-27\", \"date_max\": \"2017-01-01\"},\n",
    "    {\"fold\": 'test_2', \"date_min\": \"2017-01-01\", \"date_max\": \"2017-10-10\"},\n",
    "    {\"fold\": 'train_3', \"date_min\": \"2016-11-08\", \"date_max\": \"2017-08-14\"},\n",
    "    {\"fold\": 'test_3', \"date_min\": \"2017-08-14\", \"date_max\": \"2018-05-23\"},\n",
    "    {\"fold\": 'train_4', \"date_min\": \"2017-06-22\", \"date_max\": \"2018-03-27\"},\n",
    "    {\"fold\": 'test_4', \"date_min\": \"2018-03-27\", \"date_max\": \"2018-12-31\"},\n",
    "    {\"fold\":\"test\", \"date_min\": \"2019-01-01\", \"date_max\": \"2019-12-31\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f6dc63-c8d5-48fe-9a6f-895901d2183b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn('train',\n",
    "                   F.when((F.col('sched_depart_utc') > folds[0][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[0][\"date_max\"]), F.col(folds[0][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[1][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[1][\"date_max\"]), F.col(folds[1][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[2][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[2][\"date_max\"]), F.col(folds[2][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[3][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[3][\"date_max\"]), F.col(folds[3][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[4][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[4][\"date_max\"]), F.col(folds[4][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[5][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[5][\"date_max\"]), F.col(folds[5][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[6][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[6][\"date_max\"]), F.col(folds[6][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[7][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[7][\"date_max\"]), F.col(folds[7][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[8][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[8][\"date_max\"]), F.col(folds[8][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') > folds[9][\"date_min\"]) & (F.col('sched_depart_utc') <= folds[9][\"date_max\"]), F.col(folds[9][\"fold\"]))\n",
    "                   .when((F.col('sched_depart_utc') >= folds[10][\"date_min\"]), F.col(folds[10][\"fold\"]))\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498a7c4f-c117-46f4-87b3-5a2dd6573f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49af986a-041a-45c7-8590-580bbb5da61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0= df.filter(F.col(\"sched_depart_utc\")>=\"2014-12-31\").filter(F.col(\"sched_depart_utc\")<=\"2015-10-09\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6d7af05-fb56-4760-9cc7-f2f0d2803869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pregel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8046fe5a-c08a-4b99-ae4b-9a49479572c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from graphframes.lib import Pregel\n",
    "from graphframes import GraphFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "278f0cbc-1f0d-4dec-b8a5-8f9b4f880e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),\n",
    "    F.col(\"priorflight_sched_deptime\").alias(\"timestamp\"),\n",
    "    F.col('priorflight_depdelay_calc')\n",
    ").orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f631874-c662-408f-9f94-b9695c73cd0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices = train0.select(F.col(\"ORIGIN\").alias(\"id\")) \\\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\"))) \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff39dc2-f922-430b-addc-3b8a27fe002f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define windows (e.g., hourly)\n",
    "window_duration = \"1 hour\"\n",
    "lookback_duration = \"2 hours\"\n",
    "\n",
    "# Add window start/end timestamps to edges\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(F.col(\"timestamp\"), window_duration)\n",
    ").select(\n",
    "    \"src\", \"dst\", \"timestamp\", \"priorflight_depdelay_calc\",\n",
    "    F.col(\"window.start\").alias(\"window_start\"),\n",
    "    F.col(\"window.end\").alias(\"window_end\")\n",
    ")\n",
    "\n",
    "# Generate all possible windows (sorted)\n",
    "windows = edges_with_windows.select(\"window_start\", \"window_end\").distinct() \\\n",
    "    .orderBy(\"window_start\") \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170452cf-b282-43fb-a541-bc324515075d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(edges_with_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54de0100-4ef6-4450-88b2-fdeedf441d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "g = GraphFrame(vertices, edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ef29a6d-bcc4-4237-b521-967fc0def3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numVertices = vertices.count()\n",
    "alpha=.15\n",
    "result = g.pregel \\\n",
    "    .withVertexColumn(\"rank\", lit(\"priorflight_depdelay_calc\"), \\\n",
    "              F.coalesce(Pregel.msg(), lit(0.0)) * lit(1.0 - alpha) + lit(alpha / numVertices)) \\\n",
    "    .sendMsgToDst(Pregel.src(\"rank\")*.5) \\\n",
    "    .aggMsgs(F.sum(Pregel.msg())) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b78f0f-d762-47d0-b15d-71d8f28a7ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lookback_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9153834f-f456-4469-aceb-0278ca09fc5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices = edges.selectExpr(\"src as id\").union(edges.selectExpr(\"dst as id\")).distinct() \\\n",
    "    .withColumn(\"state\", F.lit(0.0)) #initialize state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5e7f23-56d8-48dd-b3b6-5850539c6667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices = edges.selectExpr(\"src as id\").union(edges.selectExpr(\"dst as id\")).distinct() \\\n",
    "    .withColumn(\"state\", F.lit(0.0)) #initialize state=0\n",
    "for window in windows[:3]:\n",
    "    current_window_end = window.window_end\n",
    "    lookback_start = window.window_start - F.expr(f\"INTERVAL {lookback_duration}\")\n",
    "    \n",
    "    # Filter edges from [lookback_start, current_window_end]\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= lookback_start) & \n",
    "        (F.col(\"timestamp\") < current_window_end)\n",
    "    )\n",
    "    \n",
    "    # Build graph for this window\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "    \n",
    "    # Run Pregel with \"new_state\" column\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_state\",\n",
    "            F.col(\"state\"),  # Now safe to reference \"state\"\n",
    "            F.least(\n",
    "                F.lit(1.0), \n",
    "                F.coalesce(F.sum(Pregel.msg()).over(Window.partitionBy('id')), F.lit(0.0)) * 0.85 + F.lit(0.15 / numVertices)\n",
    "            )\n",
    "        ) \\\n",
    "        .sendMsgToDst(Pregel.src(\"new_state\") * Pregel.edge(\"priorflight_depdelay_calc\")) \\\n",
    "        .aggMsgs(F.sum(Pregel.msg())) \\\n",
    "        .run()\n",
    "    \n",
    "    # Rename \"new_state\" to \"state\" for next iteration\n",
    "    vertices = result.drop(\"state\").withColumnRenamed(\"new_state\", \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ec19f4-b3bc-490d-9f43-2cdf1b897a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(current_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b958f2-54ba-495f-bc3e-858a222bade2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize airports with delay state = 0.0\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .union(train0.select(F.col(\"priorflight_origin\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_state\", F.lit(0.0))\n",
    ")\n",
    "\n",
    "\n",
    "edges = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),  # Delay source\n",
    "    F.col(\"DEST\").alias(\"dst\"),              # Current origin receiving delay\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"mean_dep_delay\").alias(\"mean_delay\")\n",
    ").withColumn(F.greatest(F.col(\"mean_delay\"), F.lit(0.0))).orderBy(\"timestamp\")\n",
    "\n",
    "\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ").withColumn(\n",
    "    \"lookback_start\", \n",
    "    F.col(\"window.start\") - F.expr(\"INTERVAL 2 HOURS\")\n",
    ")\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\").distinct().orderBy(\"window.start\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c990c23-1917-4db4-af09-f5dbb570f388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ").withColumn(\n",
    "    \"lookback_start\", \n",
    "    F.col(\"window.start\") - F.expr(\"INTERVAL 2 HOURS\")\n",
    ")\n",
    "\n",
    "# Get sorted list of time windows\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\").distinct().orderBy(\"window.start\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac796740-f2de-432f-a8da-9d193cd7d0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1588b1f-6312-455f-9714-77f9c61acad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0.select(\"prop_delayed\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7efbea3-6a01-4e00-b646-fe90afc63d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aircraft_window = Window.partitionBy(\"TAIL_NUM\").orderBy('sched_depart_utc')\n",
    "train0 = train0.withColumn(\"priorflight_prop_delay\",\n",
    "                  F.lag(F.col(\"prop_delayed\")).over(aircraft_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45548622-8703-4e55-b97a-18f749b2f8ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0.filter(F.col(\"priorflight_prop_delay\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981c348b-7961-47b3-a512-a0c4f9f1093a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0.filter(F.col(\"priorflight_prop_delay\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd5fb450-4431-484a-9885-09e9d4948875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0.select(\"priorflight_depdelay_calc\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f5a169-07f7-4050-8f49-317c794f8203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.filter(F.col('prop_delayed')==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad5f7fa-2561-49ac-b0d9-e19d30ddfd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0795b99-9e40-4e1e-bf21-db28e374d93f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize airports with delay state = 0.0\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .union(train0.select(F.col(\"priorflight_origin\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_state\", F.lit(0.0))\n",
    ")\n",
    "\n",
    "\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),  # Delay source\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),              # Current origin receiving delay\n",
    "    F.col(\"prop_delayed\"),                     # Proportion of flights being \"currently\" delayed at origin\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"prior_delay_calc\") # amount of delay being passed on\n",
    ").withColumn(\"priorflight_depdelay_calc\", #trim at 0\n",
    "             F.greatest(F.col(\"prior_delay_calc\"), F.lit(0.0))\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ").withColumn(\n",
    "    \"lookback_start\", \n",
    "    F.col(\"window.start\") - F.expr(\"INTERVAL 2 HOURS\")\n",
    ")\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\").distinct().orderBy(\"window.start\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0807b74c-d1d2-4f72-8b79-1e481e66e06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize airports with delay state = 0.0\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .union(train0.select(F.col(\"priorflight_origin\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_state\", F.lit(0.0))\n",
    ")\n",
    "\n",
    "\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),  # Delay source\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),              # Current origin receiving delay\n",
    "    F.col(\"prop_delayed\"),                     # Proportion of flights being \"currently\" delayed at origin\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"prior_delay_calc\") # amount of delay being passed on\n",
    ").withColumn(\"priorflight_depdelay_calc\", #trim at 0\n",
    "             F.greatest(F.col(\"prior_delay_calc\"), F.lit(0.0))\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ").withColumn(\n",
    "    \"lookback_start\", \n",
    "    F.col(\"window.start\") - F.expr(\"INTERVAL 2 HOURS\")\n",
    ")\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\").distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[10:13]:\n",
    "    print(f'processing window {window}')\n",
    "    # Filter edges in [current_window - 2 hours, current_window]\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"lookback_start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # Group by (src, dst) to retain edge structure\n",
    "    current_edges = current_edges.groupBy(\"src\", \"dst\").agg(\n",
    "        F.avg(\"priorflight_depdelay_calc\").alias(\"avg_delay_edge\"),\n",
    "        F.first(\"prop_delayed\")  # Keep prop_delayed\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Compute max delay and normalize\n",
    "    max_delay = current_edges.agg(F.max(\"avg_delay_edge\")).first()[0]\n",
    "    current_edges = current_edges.withColumn(\n",
    "        \"delay_risk_edge\", \n",
    "        F.col(\"prop_delayed\") * (F.col(\"avg_delay_edge\") / max_delay)\n",
    "    )\n",
    "    \n",
    "    current_edges.select('delay_risk_edge').summary().show()\n",
    "\n",
    "    # Build graph and run Pregel (remaining code unchanged)\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_state\"),\n",
    "            F.coalesce(Pregel.msg(), F.lit(0.0)) * 0.8 + 0.2 / vertices.count()\n",
    "        ) \\\n",
    "        .sendMsgToDst(Pregel.src(\"new_delay_state\") * Pregel.edge(\"delay_risk_edge\")) \\\n",
    "        .aggMsgs(F.sum(Pregel.msg())) \\\n",
    "        .setMaxIter(5) \\\n",
    "        .run()\n",
    "    \n",
    "    vertices = result.drop(\"delay_state\").withColumnRenamed(\"new_delay_state\", \"delay_state\")\n",
    "    vertices.select(\"delay_state\").summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01dc8036-33f7-41eb-833c-11b26231de11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfab36dd-5222-4d39-93d4-ee77f8befae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01384afe-d191-4cdc-9677-4235f86f8ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize airports with delay state = 0.0\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .union(train0.select(F.col(\"priorflight_origin\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_state\", F.lit(0.0))\n",
    ")\n",
    "\n",
    "\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),  # Delay source\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),              # Current origin receiving delay\n",
    "    F.col(\"prop_delayed\"),                     # Proportion of flights being \"currently\" delayed at origin\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"prior_delay_calc\") # amount of delay being passed on\n",
    ").withColumn(\"priorflight_depdelay_calc\", #trim at 0\n",
    "             F.greatest(F.col(\"prior_delay_calc\"), F.lit(0.0))\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ").withColumn(\n",
    "    \"lookback_start\", \n",
    "    F.col(\"window.start\") - F.expr(\"INTERVAL 2 HOURS\")\n",
    ")\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\").distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[10:13]:\n",
    "    print(f'processing window {window}')\n",
    "    # Filter edges in [current_window - 2 hours, current_window]\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"lookback_start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # Group by (src, dst) to retain edge structure\n",
    "    current_edges = current_edges.groupBy(\"src\", \"dst\").agg(\n",
    "        F.avg(\"priorflight_depdelay_calc\").alias(\"avg_delay_edge\"),\n",
    "        F.first(\"prop_delayed\")  # Keep prop_delayed\n",
    "    )\n",
    "\n",
    "    current_edges.select('delay_risk_edge').summary().show()\n",
    "\n",
    "    # Build graph and run Pregel (remaining code unchanged)\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_state\"),\n",
    "            F.coalesce(Pregel.msg(), F.lit(0.0)) * 0.8 + 0.2 / vertices.count()\n",
    "        ) \\\n",
    "        .sendMsgToDst(Pregel.src(\"new_delay_state\") * Pregel.edge(\"delay_risk_edge\")) \\\n",
    "        .aggMsgs(F.sum(Pregel.msg())) \\\n",
    "        .setMaxIter(5) \\\n",
    "        .run()\n",
    "    \n",
    "    vertices = result.drop(\"delay_state\").withColumnRenamed(\"new_delay_state\", \"delay_state\")\n",
    "    vertices.select(\"delay_state\").summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d8f8ba-f61b-496f-8f2f-0cdb832c2c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize airports with delay state = 0.0\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_state\", F.lit(0.0))\n",
    ")\n",
    "\n",
    "\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),  # Delay source\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),              # Current origin receiving delay\n",
    "    F.col(\"prop_delayed\"),                     # Proportion of flights being \"currently\" delayed at origin\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"prior_delay_calc\") # amount of delay being passed on\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ").withColumn(\n",
    "    \"lookback_start\", \n",
    "    F.col(\"window.start\") - F.expr(\"INTERVAL 2 HOURS\")\n",
    ")\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\").distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[10:13]:\n",
    "    print(f'processing window {window}')\n",
    "    # Filter edges in [current_window - 2 hours, current_window]\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"lookback_start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # # Group by (src, dst) to retain edge structure\n",
    "    # current_edges = current_edges.groupBy(\"src\", \"dst\").agg(\n",
    "    #     F.avg(\"priorflight_depdelay_calc\").alias(\"avg_delay_edge\"),\n",
    "    #     F.first(\"prop_delayed\")  # Keep prop_delayed\n",
    "    # )\n",
    "\n",
    "\n",
    "\n",
    "    # Build graph and run Pregel (remaining code unchanged)\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_state\"),\n",
    "            F.coalesce(Pregel.msg(), F.lit(0.0)) * 0.8\n",
    "        ) \\\n",
    "        .sendMsgToDst(Pregel.src(\"new_delay_state\") * Pregel.edge(\"prop_delayed\")) \\\n",
    "        .aggMsgs(F.sum(Pregel.msg())) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()\n",
    "    \n",
    "    vertices = result.drop(\"delay_state\").withColumnRenamed(\"new_delay_state\", \"delay_state\")\n",
    "    vertices.select(\"delay_state\").summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7429543c-624c-4c29-9612-d2a2b20b8620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edges_with_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ff69a6-394f-41fa-afc9-163624bafc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_edges.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3d6861-dace-410e-ac65-aeab7a7783f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_edges.select('delay_risk_edge').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d2c768-123c-43ce-8140-26d3a042a6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5036c7c1-5248-4def-812d-f26df9b0e7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbac0ae3-7793-4a50-a8e1-4537bc5d5092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0.select('prop_delayed').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3679e36d-a658-4eef-a9c1-9e464ee9eabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    ".withVertexColumn(\n",
    "            \"_tmp_prop\",\n",
    "            F.col(\"_tmp_src_prop\"),\n",
    "            (F.col(\"_tmp_src_prop\") * 0.9) + F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0)) * 0.1\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"_tmp_delay\",\n",
    "            F.col(\"delay_load\"),\n",
    "            (F.col(\"_tmp_delay\") * 0.2) + F.coalesce(Pregel.msg().getItem(\"avg_delay\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.edge(\"edge_prop_delayed\").alias(\"prop\"),\n",
    "                (\n",
    "                    Pregel.src(\"_tmp_src_prop\") *  # Use temporary name\n",
    "                    Pregel.edge(\"edge_prop_delayed\") * \n",
    "                    Pregel.edge(\"delay_load\")\n",
    "                ).alias(\"delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.coalesce(F.avg(Pregel.msg().getItem(\"prop\")), F.lit(0.0)).alias(\"avg_prop\"),\n",
    "                F.coalesce(F.avg(Pregel.msg().getItem(\"delay\")), F.lit(0.0)).alias(\"avg_delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(1) \\\n",
    "        .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331fd881-def1-44a3-a37c-fe69f24ac72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.0))  # Vertex property\n",
    ")\n",
    "\n",
    "\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"), \n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    ").withColumn(\n",
    "    \"delay_load\", \n",
    "    F.greatest(F.col(\"delay_load\"), F.lit(0.0))\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "# Add temporal windows (unchanged)\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ").withColumn(\n",
    "    \"lookback_start\", \n",
    "    F.col(\"window.start\") - F.expr(\"INTERVAL 2 HOURS\")\n",
    ")\n",
    "\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7779255-9b7c-421a-9328-fbd6b498d62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528b62d6-183e-4749-838d-01a8fa5f701d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "        .withVertexColumn(\n",
    "            \"src_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            (F.col(\"prop_delayed\") * 0.9) + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0)) * 0.1\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"delay_load\",\n",
    "            F.col(\"delay_load\"),\n",
    "            (F.col(\"delay_load\") * 0.2) + \n",
    "            F.coalesce(Pregel.msg().getItem(\"sum_delay\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.edge(\"edge_prop_delayed\").alias(\"prop\"),\n",
    "                (\n",
    "                    Pregel.src(\"vertex_src_prop_delayed\") * \n",
    "                    Pregel.edge(\"edge_prop_delayed\") * \n",
    "                    Pregel.edge(\"delay_load\")\n",
    "                ).alias(\"delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.coalesce(F.avg(Pregel.msg().getItem(\"prop\")), F.lit(0.0)).alias(\"avg_prop\"),\n",
    "                F.coalesce(F.sum(Pregel.msg().getItem(\"delay\")), F.lit(0.0)).alias(\"sum_delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44aab7bd-fb0a-4c2a-ad76-ec1013ddf19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fca452b-3de9-45c9-bde7-116947305d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a181b0-0f81-4ca1-b7d9-1c17b32d3435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for window in windows[10:13]:\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"lookback_start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Precompute source's average edge_prop_delayed\n",
    "    src_prop = current_edges.groupBy(\"src\").agg(\n",
    "        F.avg(\"edge_prop_delayed\").alias(\"_tmp_src_prop\")\n",
    "    )\n",
    "    \n",
    "    # 3. Join with vertices and fill nulls\n",
    "    vertices_window = vertices.join(\n",
    "        src_prop,\n",
    "        vertices.id == src_prop.src,\n",
    "        \"left\"\n",
    "    ).fillna(0.0, subset=[\"_tmp_src_prop\"])\n",
    "\n",
    "    # 4. Build graph\n",
    "    g = GraphFrame(vertices_window, current_edges)\n",
    "\n",
    "    # 5. Run Pregel with structured messages\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            (F.col(\"delay_load\") * 0.2) +  # 80% decay\n",
    "            F.coalesce(Pregel.msg().getItem(\"sum_delay\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"_tmp_prop\",\n",
    "            F.col(\"_tmp_src_prop\"),\n",
    "            (F.col(\"_tmp_prop\") * 0.9) +  # 10% decay\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0)) * 0.1\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                (Pregel.edge(\"edge_prop_delayed\") * 0.85).alias(\"prop\"),  # Scaled by decay\n",
    "                (Pregel.edge(\"delay_load\") * Pregel.src(\"_tmp_prop\")).alias(\"delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")).alias(\"sum_delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()\n",
    "    \n",
    "    # 7. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    result.orderBy(F.desc('new_delay_state')).show(10, truncate=False)\n",
    "\n",
    "    # Checkpoint the vertices DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053a5e13-840e-4235-86c7-287980e55d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize vertices with non-zero values to force state changes\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.001))  # Non-zero initial state\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.001))\n",
    ")\n",
    "\n",
    "for window in windows[10:13]:\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"lookback_start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Precompute source's average edge_prop_delayed\n",
    "    src_prop = current_edges.groupBy(\"src\").agg(\n",
    "        F.avg(\"edge_prop_delayed\").alias(\"_tmp_src_prop\")\n",
    "    )\n",
    "    \n",
    "    # 3. Join with vertices and fill nulls\n",
    "    vertices_window = vertices.join(\n",
    "        src_prop,\n",
    "        vertices.id == src_prop.src,\n",
    "        \"left\"\n",
    "    ).fillna(0.0, subset=[\"_tmp_src_prop\"])\n",
    "\n",
    "    # 4. Build graph\n",
    "    g = GraphFrame(vertices_window, current_edges)\n",
    "\n",
    "    # 5. Run Pregel with corrected aggregation and decay\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            (F.col(\"delay_load\") * 0.2) + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_delay\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"_tmp_prop\",\n",
    "            F.col(\"_tmp_src_prop\"),\n",
    "            (F.col(\"_tmp_prop\") * 0.85) +\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\")*15, F.lit(0.0)) \n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                (Pregel.edge(\"edge_prop_delayed\") * 0.85).alias(\"prop\"),\n",
    "                (Pregel.edge(\"delay_load\") * Pregel.src(\"_tmp_prop\")).alias(\"delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")).alias(\"avg_delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()\n",
    "        \n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"_tmp_prop\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()  # Cache to ensure updates persist\n",
    "    \n",
    "    # 7. Force materialization to prevent plan recomputation\n",
    "    vertices.count()\n",
    "\n",
    "\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.orderBy(F.desc(\"delay_load\")).show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee320f1d-f020-472f-8c82-80fd3822377c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import Pregel\n",
    "\n",
    "# Initialize vertices with non-zero values to force state changes\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.001))  # Avoid zero initialization\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.001))\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes (no normalization)\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"priorflight_dest\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "# Add temporal windows (unchanged)\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ").withColumn(\n",
    "    \"lookback_start\", \n",
    "    F.col(\"window.start\") - F.expr(\"INTERVAL 2 HOURS\")\n",
    ")\n",
    "\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[1:13]:\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"lookback_start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Build graph using previous state\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "    g.edges.show()\n",
    "\n",
    "    # 3. Run Pregel with corrected logic\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            (F.col(\"delay_load\")*.1 + \n",
    "                F.coalesce(Pregel.msg().getItem(\"avg_delay\"), \n",
    "                           F.lit(0.0))\n",
    "                )  # Sum delays\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"new_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            (F.col(\"prop_delayed\") * 0.1) + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0)) * 0.1  # 70% new data\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.edge(\"edge_prop_delayed\").alias(\"prop\"),  # For prop_delayed updates\n",
    "                (Pregel.edge(\"delay_load\") * Pregel.src(\"new_prop_delayed\")).alias(\"delay\")  # Delay scaled by src's prop_delayed\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(Pregel.msg().getItem(\"prop\")*.15).alias(\"avg_prop\"),  # Average edge_prop_delayed\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")*.15).alias(\"avg_delay\")  # Average scaled delays\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(1) \\\n",
    "        .run()\n",
    "\n",
    "\n",
    "    \n",
    "    # 4. Update vertices for next window (cap values if needed)\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 5. Force materialization to persist state\n",
    "    vertices.count()\n",
    "\n",
    "    # 6. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.orderBy(F.desc(\"delay_load\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8428f8-3dd6-47b2-8eb9-b4900486f10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import Pregel\n",
    "\n",
    "# Initialize vertices with non-zero values to force state changes\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.001))  # Avoid zero initialization\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.001))\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes (no normalization)\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"priorflight_dest\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "# Add temporal windows (unchanged)\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"1 hour\")\n",
    ")\n",
    "\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[1:13]:\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Build graph using previous state\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "\n",
    "\n",
    "    # 3. Run Pregel with corrected logic\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            (F.col(\"delay_load\")*.1 + \n",
    "                F.coalesce(Pregel.msg().getItem(\"avg_delay\"), \n",
    "                           F.lit(0.0))\n",
    "                )  # Sum delays\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"new_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            (F.col(\"prop_delayed\") * 0.1) + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0)) * 0.1  # 70% new data\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                (F.when(Pregel.src(\"new_delay_state\")>15, 1).otherwise(0)).alias(\"prop\"),  # For prop_delayed updates\n",
    "                (Pregel.edge(\"delay_load\") * .8).alias(\"delay\")  # Delay scaled by src's prop_delayed\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),  # Average edge_prop_delayed\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")).alias(\"avg_delay\")  # Average scaled delays\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(1) \\\n",
    "        .run()\n",
    "\n",
    "\n",
    "    \n",
    "    # 4. Update vertices for next window (cap values if needed)\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 5. Force materialization to persist state\n",
    "    vertices.count()\n",
    "\n",
    "    # 6. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.orderBy(F.desc(\"delay_load\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcce6c51-726e-4fd0-a834-e04b979d2a7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cca534c-2762-41b3-9dda-ff611d193033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize vertices with non-zero values to force state changes\n",
    "vertices = (\n",
    "    train0.select(F.col(\"priorflight_origin\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"priorflight_dest\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes (no normalization)\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"priorflight_dest\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "\n",
    "# Add temporal windows (unchanged)\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hours\")\n",
    ").orderBy(\"sched_depart_utc\")\n",
    "\n",
    "\n",
    "edges_with_windows = edges_with_windows.limit(100)\n",
    "edges_with_windows.cache()\n",
    "\n",
    "\n",
    "\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy('window.start').collect()\n",
    "\n",
    "for window in windows[0:13]:\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    ).limit(50)\n",
    "\n",
    "\n",
    "    # 2. Build graph using previous state\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "\n",
    "\n",
    "    # 3. Run Pregel with corrected logic\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            F.col(\"delay_load\")*.1 + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_delay\")*.8, \n",
    "                           F.lit(0.0)) # Sum delays\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"new_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.src(\"new_prop_delayed\").alias(\"prop\"),  # For prop_delayed updates\n",
    "                (Pregel.edge(\"delay_load\") * .5).alias(\"delay\")  # Delay scaled by src's prop_delayed\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(F.when(Pregel.msg().getItem(\"delay\") > 15, 1).otherwise(0)).alias(\"avg_prop\"),  # Average edge_prop_delayed\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")).alias(\"avg_delay\")  # Average scaled delays\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()\n",
    "\n",
    "    print(f'result:')\n",
    "    result.orderBy(F.desc('new_delay_state')).show()\n",
    "    # 4. Update vertices for next window (cap values if needed)\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 5. Force materialization to persist state\n",
    "    vertices.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de2bdc5-12e1-4db8-ac5c-8bddc5e68e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# WORKING VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e8281f6-8c4c-4e8f-ad2c-19ab49c66d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds = [\n",
    "    {\"fold\": 'train_0', \"date_min\": \"2014-12-31\", \"date_max\": \"2015-10-09\"},\n",
    "    {\"fold\": 'test_0', \"date_min\": \"2015-10-09\", \"date_max\": \"2016-07-17\"},\n",
    "    {\"fold\": 'train_1', \"date_min\": \"2015-08-14\", \"date_max\": \"2016-05-21\"},\n",
    "    {\"fold\": 'test_1', \"date_min\": \"2016-05-21\", \"date_max\": \"2017-02-27\"},\n",
    "    {\"fold\": 'train_2', \"date_min\": \"2016-03-27\", \"date_max\": \"2017-01-01\"},\n",
    "    {\"fold\": 'test_2', \"date_min\": \"2017-01-01\", \"date_max\": \"2017-10-10\"},\n",
    "    {\"fold\": 'train_3', \"date_min\": \"2016-11-08\", \"date_max\": \"2017-08-14\"},\n",
    "    {\"fold\": 'test_3', \"date_min\": \"2017-08-14\", \"date_max\": \"2018-05-23\"},\n",
    "    {\"fold\": 'train_4', \"date_min\": \"2017-06-22\", \"date_max\": \"2018-03-27\"},\n",
    "    {\"fold\": 'test_4', \"date_min\": \"2018-03-27\", \"date_max\": \"2018-12-31\"},\n",
    "    {\"fold\":\"test\", \"date_min\": \"2019-01-01\", \"date_max\": \"2019-12-31\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1380cd26-cb13-41bd-9b3e-3e0140ae63d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74094f22-2516-473c-b35a-517efe586d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0_msg = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\"),\n",
    "    F.col(\"actual_arr_utc\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "49c7b8ee-b08e-4cf3-b503-b992efb3f099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ae096b-39c4-4878-8f0e-e91e319abbd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df227187-0b83-4e74-9110-1fd3f7468eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes (no normalization)\n",
    "edges = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"actual_arr_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"DEP_DELAY\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "\n",
    "# Add temporal windows (unchanged)\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hours\")\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "edges_with_windows.cache()\n",
    "edges_with_windows.count()\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy('window.start').collect()\n",
    "\n",
    "for window in windows:\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.lit(window.start)) & \n",
    "        (F.col(\"timestamp\") < F.lit(window.end))\n",
    "        )\n",
    "\n",
    "    # 2. Build graph using previous state\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "\n",
    "    # 3. Run Pregel with corrected logic\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            F.col(\"delay_load\")*.1 + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_delay\")*.8, \n",
    "                           F.lit(0.0)) # Sum delays\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"new_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.src(\"new_prop_delayed\").alias(\"prop\"),  # For prop_delayed updates\n",
    "                (Pregel.edge(\"delay_load\") * .5).alias(\"delay\")  # Delay scaled by src's prop_delayed\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(F.when(Pregel.msg().getItem(\"delay\") > 15, 1).otherwise(0)).alias(\"avg_prop\"),  # Average edge_prop_delayed\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")).alias(\"avg_delay\")  # Average scaled delays\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()\n",
    "\n",
    "    # 4. Update vertices for next window (cap values if needed)\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 5. Force materialization to persist state\n",
    "    vertices.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4fb0db-d909-40d2-b806-47249340a000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "F.lit(window.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b23f721-e2e0-47f1-89b7-d8fc03ba92a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "F.lit(window.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a825429f-08f8-4f87-8fe3-531e1519f4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.withColumn(\"window_end\", F.lit(window.end)).withColumn(\"window_start\",F.lit(window.start)).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ddd323d-a150-44e5-b129-04946d2cd0b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RDD WORKING VERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c674b24d-7396-4a4e-8690-b54a2fcfe93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/interim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85331c85-3ff5-4465-a1bd-10c10aec1717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.mllib.linalg import Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb46352c-6c8b-4752-9934-8ab24ae3c60e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import java.sql.Timestamp\n",
    "\n",
    "case class Flight(origin: String, dest: String, depDelay: Double, timestamp: Timestamp)\n",
    "\n",
    "val flights: RDD[Flight] = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")\n",
    "  .withColumnRenamed(\"actual_arr_utc\", \"timestamp\")\n",
    "  .withColumnRenamed(\"ORIGIN\", \"origin\")\n",
    "  .withColumnRenamed(\"DEST\", \"dest\")\n",
    "  .na.drop(Seq(\"origin\", \"dest\"))\n",
    "  .na.fill(Map(\"DEP_DELAY\" -> 0))\n",
    "  .withColumnRenamed(\"DEP_DELAY\", \"depDelay\")\n",
    "  .limit(100)\n",
    "  .as[Flight]\n",
    "  .rdd\n",
    "flights.take(10)\n",
    "\n",
    "// Assign 4-hour windows\n",
    "val windowedEdges = flights.map { f =>\n",
    "  val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "  (windowStart, Edge(f.origin.hashCode.toLong, f.dest.hashCode.toLong, f.depDelay))\n",
    "}\n",
    "\n",
    "val edgesByWindow = windowedEdges.groupByKey()\n",
    "\n",
    "val vertices = flights.flatMap(f => Seq(f.origin, f.dest))\n",
    "  .distinct()\n",
    "  .map { airportId =>\n",
    "    (airportId.hashCode.toLong, (0.0, 0.2)) // (id, (delayLoad, propDelayed))\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13854bcf-cec1-4150-8d8b-703bd286c428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import java.sql.Timestamp\n",
    "\n",
    "\n",
    "case class Flight(origin: String, dest: String, depDelay: Double, timestamp: Timestamp)\n",
    "\n",
    "val flights: RDD[Flight] = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")\n",
    "  .withColumnRenamed(\"actual_arr_utc\", \"timestamp\")\n",
    "  .withColumnRenamed(\"ORIGIN\", \"origin\")\n",
    "  .withColumnRenamed(\"DEST\", \"dest\")\n",
    "  .na.drop(Seq(\"origin\", \"dest\"))\n",
    "  .na.fill(Map(\"DEP_DELAY\" -> 0))\n",
    "  .withColumnRenamed(\"DEP_DELAY\", \"depDelay\")\n",
    "  .limit(100)\n",
    "  .as[Flight]\n",
    "  .rdd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d9ef14-7bd3-4ab3-865c-1312cfa5223f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b68635-1320-4f7d-87d9-a364594a2ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d6ab65-7e04-46dc-912c-dcbf0bc9441d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.functions.{coalesce, col}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40e6bdb-fef6-4290-9326-fc6f892f83b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "case class Flight(origin: String, dest: String, depDelay: Double, timestamp: Timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10c3412-17de-4208-a77b-49597a13fceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "// 1. Load data with coalesced timestamps\n",
    "val flights: RDD[Flight] = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")\n",
    "  .na.drop(Seq(\"ORIGIN\", \"DEST\")) // Filter rows with null origins/destinations first\n",
    "  .withColumn(\"timestamp\", coalesce(col(\"actual_arr_utc\"), col(\"sched_arr_utc\")).cast(\"timestamp\"))\n",
    "  .filter(col(\"timestamp\").isNotNull) // Remove rows with both timestamps null\n",
    "  .withColumnRenamed(\"ORIGIN\", \"origin\")\n",
    "  .withColumnRenamed(\"DEST\", \"dest\")\n",
    "  .na.fill(Map(\"DEP_DELAY\" -> 0))\n",
    "  .withColumnRenamed(\"DEP_DELAY\", \"depDelay\")\n",
    "  .as[Flight]\n",
    "  .rdd\n",
    "\n",
    "// 2. Verify flight data\n",
    "println(s\"Flight count after coalesce: ${flights.count()}\")\n",
    "// flights.limit(5).foreach { f =>\n",
    "//   println(s\"Flight: ${f.origin}->${f.dest} | Timestamp: ${f.timestamp} | Delay: ${f.depDelay}\")\n",
    "// }\n",
    "\n",
    "// 3. Rebuild airport ID map from FULL dataset\n",
    "val airportCodes = flights.flatMap(f => Seq(f.origin, f.dest)).distinct().collect()\n",
    "\n",
    "\n",
    "val airportIdMap = airportCodes.zipWithIndex.map { case (code, idx) => \n",
    "  (code, idx.toLong) \n",
    "}.toMap\n",
    "println(s\"Airport ID map size: ${airportIdMap.size}\")\n",
    "\n",
    "// val bcAirportIds = spark.sparkContext.broadcast(airportIdMap)\n",
    "\n",
    "// 4. Debug edge creation with coalesced timestamps\n",
    "val windowedEdges = flights.map { f =>\n",
    "  val srcId = airportIdMap(f.origin)\n",
    "  val dstId = airportIdMap(f.dest)\n",
    "  val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "  (windowStart, Edge(srcId, dstId, f.depDelay))\n",
    "}\n",
    "\n",
    "// 5. Print edges with type annotations\n",
    "println(\"First 10 windowed edges:\")\n",
    "windowedEdges.take(10).foreach { \n",
    "  case (window: Long, edge: Edge[Double]) =>\n",
    "    val windowTime = new Timestamp(window).toString\n",
    "    val origin = airportIdMap.find(_._2 == edge.srcId).map(_._1).getOrElse(\"UNKNOWN\")\n",
    "    val dest = airportIdMap.find(_._2 == edge.dstId).map(_._1).getOrElse(\"UNKNOWN\")\n",
    "    println(s\"$windowTime: ${origin}(${edge.srcId}) -> ${dest}(${edge.dstId}) | Delay: ${edge.attr} mins\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aef8e89-5941-462e-b6a6-935dbf729f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val edgesByWindow = windowedEdges.groupByKey()\n",
    "\n",
    "val vertices = airportCodes.map { code =>\n",
    "  (airportIdMap(code), (0.0, 0.2))  // (id, (delayLoad, propDelayed))\n",
    "}.toSeq\n",
    "val verticesRDD = spark.sparkContext.parallelize(vertices)\n",
    "\n",
    "var currentVertices: RDD[(VertexId, (Double, Double))] = verticesRDD\n",
    "val windowOrder = edgesByWindow.keys.collect().sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f6fca54-0abe-4853-ba1a-4a8d28d767fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Define message type to track sum and count for averages\n",
    "case class Message(sumProp: Double, sumDelay: Double, count: Long)\n",
    "\n",
    "// Class for saving results\n",
    "case class VertexResult(\n",
    "    id: Long, \n",
    "    delay_load: Double, \n",
    "    prop_delayed: Double, \n",
    "    window_start: Timestamp\n",
    ")\n",
    "\n",
    "\n",
    "// Vertex program definition\n",
    "def vProg(vertexId: VertexId, state: (Double, Double), msg: Message): (Double, Double) = {\n",
    "    println(s\"vProg called for vertex $vertexId with state $state and msg $msg\")\n",
    "    if (msg.count == 0) state else {\n",
    "        val avgProp = msg.sumProp / msg.count\n",
    "        val avgDelay = msg.sumDelay / msg.count\n",
    "        val newDelay = state._1 * 0.1 + avgDelay * 0.8\n",
    "        val newProp = avgProp\n",
    "        (newDelay, newProp)\n",
    "    }\n",
    "}\n",
    "\n",
    "// Message sending function\n",
    "def sendMsg(triplet: EdgeTriplet[(Double, Double), Double]): Iterator[(VertexId, Message)] = {\n",
    "  println(s\"sendMsg: src=${triplet.srcId}, dst=${triplet.dstId}, attr=${triplet.attr}\")\n",
    "    val srcProp = triplet.srcAttr._2\n",
    "    val scaledDelay = triplet.attr * 0.5\n",
    "    Iterator((triplet.dstId, Message(srcProp, scaledDelay, 1L)))\n",
    "}\n",
    "\n",
    "// Message merging function\n",
    "def mergeMsg(a: Message, b: Message): Message = {\n",
    "    Message(\n",
    "        a.sumProp + b.sumProp,\n",
    "        a.sumDelay + b.sumDelay, \n",
    "        a.count + b.count\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb083be3-606a-471d-879a-6c5410f6514c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "\n",
    "windowOrder.foreach { windowStart =>\n",
    "\n",
    "  println(s\"Window starting ${windowStart}\")\n",
    "  val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "  \n",
    "  val graph = Graph(currentVertices, spark.sparkContext.parallelize(edges))\n",
    "  \n",
    "  val updatedGraph = graph.pregel[Message](\n",
    "    Message(0.0, 0.0, 0L),  // Initial message\n",
    "    3,                       // Max iterations (neighbors of neighbors of neighbors)\n",
    "    EdgeDirection.Out        // Active direction\n",
    "  )(\n",
    "    vProg _,    // Convert methods to function values\n",
    "    sendMsg _,\n",
    "    mergeMsg _\n",
    "  )\n",
    "\n",
    "  currentVertices = updatedGraph.vertices\n",
    "  .mapValues { (v: (Double, Double)) => \n",
    "    v  // Identity function with explicit type\n",
    "  }.cache()\n",
    "\n",
    "  // Save results to Parquet\n",
    "  currentVertices\n",
    "    .map { case (id, (delay, prop)) =>\n",
    "      VertexResult(id, delay, prop, new Timestamp(windowStart))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46944ee5-3141-466c-a3ba-a2f146ac046c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "v = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices.parquet\")\n",
    "\n",
    "display(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc2df604-fd7c-4e76-a927-08f049b0b2e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## w/o avg prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17fba51-0897-4923-9bbd-dc8ef0c101b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "\n",
    "// 1. Load data with coalesced timestamps\n",
    "case class Flight(origin: String, dest: String, depDelay: Double, timestamp: Timestamp)\n",
    "\n",
    "val flights: RDD[Flight] = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")\n",
    "  .na.drop(Seq(\"ORIGIN\", \"DEST\")) // Filter rows with null origins/destinations first\n",
    "  .withColumn(\"timestamp\", coalesce(col(\"actual_arr_utc\"), col(\"sched_arr_utc\")).cast(\"timestamp\"))\n",
    "  .filter(col(\"timestamp\").isNotNull) // Remove rows with both timestamps null\n",
    "  .withColumnRenamed(\"ORIGIN\", \"origin\")\n",
    "  .withColumnRenamed(\"DEST\", \"dest\")\n",
    "  .na.fill(Map(\"DEP_DELAY\" -> 0))\n",
    "  .withColumnRenamed(\"DEP_DELAY\", \"depDelay\")\n",
    "  .as[Flight]\n",
    "  .rdd\n",
    "\n",
    "// 2. Verify flight data\n",
    "println(s\"Flight count after coalesce: ${flights.count()}\")\n",
    "// flights.limit(5).foreach { f =>\n",
    "//   println(s\"Flight: ${f.origin}->${f.dest} | Timestamp: ${f.timestamp} | Delay: ${f.depDelay}\")\n",
    "// }\n",
    "\n",
    "// 3. Rebuild airport ID map from FULL dataset\n",
    "val airportCodes = flights.flatMap(f => Seq(f.origin, f.dest)).distinct().collect()\n",
    "\n",
    "\n",
    "val airportIdMap = airportCodes.zipWithIndex.map { case (code, idx) => \n",
    "  (code, idx.toLong) \n",
    "}.toMap\n",
    "println(s\"Airport ID map size: ${airportIdMap.size}\")\n",
    "\n",
    "// val bcAirportIds = spark.sparkContext.broadcast(airportIdMap)\n",
    "\n",
    "// 4. Debug edge creation with coalesced timestamps\n",
    "val windowedEdges = flights.map { f =>\n",
    "  val srcId = airportIdMap(f.origin)\n",
    "  val dstId = airportIdMap(f.dest)\n",
    "  val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "  (windowStart, Edge(srcId, dstId, f.depDelay))\n",
    "}\n",
    "\n",
    "// 5. Print edges with type annotations\n",
    "println(\"First 10 windowed edges:\")\n",
    "windowedEdges.take(10).foreach { \n",
    "  case (window: Long, edge: Edge[Double]) =>\n",
    "    val windowTime = new Timestamp(window).toString\n",
    "    val origin = airportIdMap.find(_._2 == edge.srcId).map(_._1).getOrElse(\"UNKNOWN\")\n",
    "    val dest = airportIdMap.find(_._2 == edge.dstId).map(_._1).getOrElse(\"UNKNOWN\")\n",
    "    println(s\"$windowTime: ${origin}(${edge.srcId}) -> ${dest}(${edge.dstId}) | Delay: ${edge.attr} mins\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428b5896-cefa-4214-8d3a-430ea13af323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "// ~~~~~~~~~~` VALUES & VAR\n",
    "val edgesByWindow = windowedEdges.groupByKey()\n",
    "\n",
    "val vertices = airportCodes.map(code => (airportIdMap(code), 0.0))\n",
    "\n",
    "val verticesRDD = spark.sparkContext.parallelize(vertices)\n",
    "\n",
    "var currentVertices: RDD[(VertexId, Double)] = verticesRDD\n",
    "val windowOrder = edgesByWindow.keys.collect().sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588b6797-6dce-4b5b-aa68-9d0d136d9977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "// ~~~~~~~~~~~~~~~~~~~ CLASSES\n",
    "\n",
    "\n",
    "// Class for saving results\n",
    "case class VertexResult(\n",
    "    id: Long, \n",
    "    delay_load: Double, \n",
    "    window_start: Timestamp\n",
    ")\n",
    "\n",
    "// Define message type to track sum\n",
    "case class Message(sumDelay: Double, count: Long)\n",
    "\n",
    "// ~~~~~~~~~~~~~~~~~~~ FUNCTIONS\n",
    "\n",
    "// Vertex program definition\n",
    "def vProg(vertexId: VertexId, state: Double, msg: Message): Double = {\n",
    "    if (msg.count == 0) state else {\n",
    "        val avgDelay = msg.sumDelay / msg.count\n",
    "        state * 0.1 + avgDelay * 0.8  // Only update delay_load\n",
    "    }\n",
    "}\n",
    "// Message sending func\n",
    "def sendMsg(triplet: EdgeTriplet[Double, Double]): Iterator[(VertexId, Message)] = {  // Vertex data is now Double\n",
    "  val scaledDelay = triplet.attr * 0.5\n",
    "  Iterator((triplet.dstId, Message(scaledDelay, 1L)))  // No prop value\n",
    "}\n",
    "\n",
    "// Message merging function\n",
    "def mergeMsg(a: Message, b: Message): Message = {\n",
    "    Message(\n",
    "        a.sumDelay + b.sumDelay, \n",
    "        a.count + b.count\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dd2e63d9-68b3-4859-998e-e535c820c32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "windowOrder.foreach { windowStart =>\n",
    "\n",
    "  println(s\"Window starting ${windowStart}\")\n",
    "  val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "  \n",
    "  val graph = Graph(currentVertices, spark.sparkContext.parallelize(edges))\n",
    "  \n",
    "  val updatedGraph = graph.pregel[Message](\n",
    "    Message(0.0, 0L),  // Initial message\n",
    "    3,                       // Max iterations (neighbors of neighbors of neighbors)\n",
    "    EdgeDirection.Out        // Active direction\n",
    "  )(\n",
    "    vProg _,    // Convert methods to function values\n",
    "    sendMsg _,\n",
    "    mergeMsg _\n",
    "  )\n",
    "\n",
    "  currentVertices = updatedGraph.vertices.cache()\n",
    "\n",
    "  // Save results to Parquet\n",
    "  currentVertices\n",
    "    .map { case (id, delay) =>\n",
    "      VertexResult(id, delay, new Timestamp(windowStart))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb828aa8-4ea4-4233-ac0f-d7c69ffe49ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "\n",
    "// ~~~~~~~~~~~~ PROCESSING LOOP\n",
    "windowOrder.foreach { windowStart =>\n",
    "  println(s\"Window starting ${windowStart}\")\n",
    "\n",
    "  val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "  \n",
    "  val graph = Graph(verticesRDD, spark.sparkContext.parallelize(edges))  // verticesRDD is now RDD[(VertexId, Double)]\n",
    "  \n",
    "  val updatedGraph = graph.pregel[Message](\n",
    "    initialMsg = Message(0.0, 0L),  // Initial message without sumProp\n",
    "    maxIterations = 3,\n",
    "    activeDirection = EdgeDirection.Out\n",
    "  )(\n",
    "    vProg _,    // Updated function signature\n",
    "    sendMsg _,\n",
    "    mergeMsg _\n",
    "  )\n",
    "\n",
    "  val currentVertices = updatedGraph.vertices\n",
    "  \n",
    "  // Save results (without prop_delayed)\n",
    "  currentVertices\n",
    "    .map { case (id, delay) =>  // Single value now\n",
    "      VertexResult(id, delay, new Timestamp(windowStart))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_delay_only.parquet\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ff63208-2b05-4814-9f3c-8ccddd1fde8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## try to optimize for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7d757c-5ade-4345-b3f4-a4582593e6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.HashPartitioner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8322677b-92e8-4440-ac09-ec2d712a08b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Define message type to track sum and count for averages\n",
    "case class Message(sumProp: Double, sumDelay: Double, count: Long)\n",
    "\n",
    "// Class for saving results\n",
    "case class VertexResult(\n",
    "    id: Long, \n",
    "    delay_load: Double, \n",
    "    prop_delayed: Double, \n",
    "    window_start: Timestamp\n",
    ")\n",
    "\n",
    "\n",
    "// Vertex program definition\n",
    "def vProg(vertexId: VertexId, state: (Double, Double), msg: Message): (Double, Double) = {\n",
    "    println(s\"vProg called for vertex $vertexId with state $state and msg $msg\")\n",
    "    if (msg.count == 0) state else {\n",
    "        val avgProp = msg.sumProp / msg.count\n",
    "        val avgDelay = msg.sumDelay / msg.count\n",
    "        val newDelay = state._1 * 0.1 + avgDelay * 0.8\n",
    "        val newProp = avgProp\n",
    "        (newDelay, newProp)\n",
    "    }\n",
    "}\n",
    "\n",
    "// Message sending function\n",
    "def sendMsg(triplet: EdgeTriplet[(Double, Double), Double]): Iterator[(VertexId, Message)] = {\n",
    "  println(s\"sendMsg: src=${triplet.srcId}, dst=${triplet.dstId}, attr=${triplet.attr}\")\n",
    "    val srcProp = triplet.srcAttr._2\n",
    "    val scaledDelay = triplet.attr * 0.5\n",
    "    Iterator((triplet.dstId, Message(scaledDelay, 1L)))\n",
    "}\n",
    "\n",
    "// Message merging function\n",
    "def mergeMsg(a: Message, b: Message): Message = {\n",
    "    Message(\n",
    "        a.sumProp + b.sumProp,\n",
    "        a.sumDelay + b.sumDelay, \n",
    "        a.count + b.count\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22db3382-56fd-4d71-b171-90116cd272ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### optimized with no prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf31f810-0f5d-481d-908e-a45830c84706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "// ~~~~~~~~~~~~~~~~~~~ CLASSES\n",
    "\n",
    "\n",
    "// Class for saving results\n",
    "case class VertexResult(\n",
    "    id: Long, \n",
    "    delay_load: Double, \n",
    "    window_start: Timestamp\n",
    ")\n",
    "\n",
    "// Define message type to track sum\n",
    "case class Message(sumDelay: Double, count: Long)\n",
    "\n",
    "// ~~~~~~~~~~~~~~~~~~~ FUNCTIONS\n",
    "\n",
    "// Vertex program definition\n",
    "def vProg(vertexId: VertexId, state: Double, msg: Message): Double = {\n",
    "    if (msg.count == 0) state else {\n",
    "        val avgDelay = msg.sumDelay / msg.count\n",
    "        state * 0.1 + avgDelay * 0.8  // Only update delay_load\n",
    "    }\n",
    "}\n",
    "// Message sending func\n",
    "def sendMsg(triplet: EdgeTriplet[Double, Double]): Iterator[(VertexId, Message)] = {  // Vertex data is now Double\n",
    "  val scaledDelay = triplet.attr * 0.5\n",
    "  Iterator((triplet.dstId, Message(scaledDelay, 1L)))  // No prop value\n",
    "}\n",
    "\n",
    "// Message merging function\n",
    "def mergeMsg(a: Message, b: Message): Message = {\n",
    "    Message(\n",
    "        a.sumDelay + b.sumDelay, \n",
    "        a.count + b.count\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80de0a5f-e712-4b0d-b03f-4ac950a7568c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val numWindows = windowOrder.length // used previous value \n",
    "val numPartitions = math.max(numWindows, spark.sparkContext.defaultParallelism)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d8cc55-c83e-4c55-854a-d6f871549105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// // 1. Partition edges by window upfront\n",
    "\n",
    "\n",
    "val windowedEdgesPartitioned = flights\n",
    "  .map { f =>\n",
    "    val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "    (windowStart, Edge(airportIdMap(f.origin), airportIdMap(f.dest), f.depDelay))\n",
    "  }\n",
    "  .partitionBy(new HashPartitioner(numPartitions))\n",
    "  .persist()\n",
    "\n",
    "// 2. Group partitioned edges\n",
    "\n",
    "val edgesByWindow = windowedEdgesPartitioned.groupByKey()\n",
    "\n",
    "val vertices = airportCodes.map(code => (airportIdMap(code), 0.0))\n",
    "\n",
    "val verticesRDD = spark.sparkContext.parallelize(vertices)\n",
    "\n",
    "var currentVertices: RDD[(VertexId, Double)] = verticesRDD\n",
    "\n",
    "val windowOrder = edgesByWindow.keys.collect().sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41445523-772b-4d49-8fcd-668b5f9e84f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "// ~~~~~~~~~~~~~~~~~~~ Updated Processing Loop ~~~~~~~~~~~~~~~~~~~\n",
    "windowOrder.zipWithIndex.foreach { case (windowStart, idx) =>\n",
    "  println(s\"Window starting ${windowStart}\")\n",
    "\n",
    "  val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "  \n",
    "  if (edges.nonEmpty) {\n",
    "    val edgesRDD = spark.sparkContext.parallelize[Edge[Double]](edges, numPartitions)  // Type specified\n",
    "    val graph = Graph(currentVertices, edgesRDD)\n",
    "  \n",
    "    val updatedGraph = graph.pregel[Message](\n",
    "      Message(0.0, 0L),  // Correct initialization\n",
    "      3,\n",
    "      EdgeDirection.Out\n",
    "    )(\n",
    "      vProg,\n",
    "      sendMsg,\n",
    "      mergeMsg\n",
    "    )\n",
    "\n",
    "    currentVertices = updatedGraph.vertices\n",
    "      .mapValues(v => v)\n",
    "      .cache()\n",
    "    \n",
    "    // Checkpoint every 6 windows\n",
    "    if (idx % 6 == 0) {\n",
    "      currentVertices.checkpoint()\n",
    "      currentVertices.count()\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // Save results\n",
    "  currentVertices\n",
    "    .map { case (id, delay) =>\n",
    "      VertexResult(id, delay, new Timestamp(windowStart))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_delay_only_partitioned.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee48c7d-d0c9-4fa5-b43e-9b5ced39d475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894507d2-51be-46bb-aa73-67eac38bc9fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// ~~~~~~~~~~~~~~~~~~~ Updated Processing Loop ~~~~~~~~~~~~~~~~~~~\n",
    "windowOrder.zipWithIndex.foreach { case (windowStart, idx) =>\n",
    "  println(s\"Window starting ${windowStart}\")\n",
    "\n",
    "  val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "  \n",
    "  if (edges.nonEmpty) {\n",
    "    val edgesRDD = spark.sparkContext.parallelize[Edge[Double]](edges, numPartitions)  // Type specified\n",
    "    val graph = Graph(currentVertices, edgesRDD)\n",
    "  \n",
    "    val updatedGraph = graph.pregel[Message](\n",
    "      Message(0.0, 0L),  // Correct initialization\n",
    "      3,\n",
    "      EdgeDirection.Out\n",
    "    )(\n",
    "      vProg _,\n",
    "      sendMsg _,\n",
    "      mergeMsg _\n",
    "    )\n",
    "\n",
    "    currentVertices = updatedGraph.vertices\n",
    "      .mapValues(v => v)\n",
    "      .cache()\n",
    "    \n",
    "    // Checkpoint every 6 windows\n",
    "    if (idx % 6 == 0) {\n",
    "      currentVertices.checkpoint()\n",
    "      currentVertices.count()\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // Save results\n",
    "  currentVertices\n",
    "    .map { case (id, delay) =>\n",
    "      VertexResult(id, delay, new Timestamp(windowStart))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_delay_only_partitioned.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842fcf67-f0d6-4ded-bc45-3ad08f2214b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.graphx.PartitionStrategy.EdgePartition2D\n",
    "import java.sql.Timestamp\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "// 1. Classes\n",
    "case class VertexResult(id: Long, delay_load: Double, window_start: Timestamp)\n",
    "case class Message(sumDelay: Double, count: Long)\n",
    "case class Flight(origin: String, dest: String, depDelay: Double, timestamp: Timestamp)\n",
    "\n",
    "// 2. Pregel Functions (top-level)\n",
    "def vProg(vertexId: VertexId, state: Double, msg: Message): Double = \n",
    "  if (msg.count == 0) state else state * 0.1 + (msg.sumDelay / msg.count) * 0.8\n",
    "\n",
    "def sendMsg(triplet: EdgeTriplet[Double, Double]): Iterator[(VertexId, Message)] = \n",
    "  Iterator((triplet.dstId, Message(triplet.attr * 0.5, 1L)))\n",
    "\n",
    "def mergeMsg(a: Message, b: Message): Message = \n",
    "  Message(a.sumDelay + b.sumDelay, a.count + b.count)\n",
    "\n",
    "// 3. Main Code\n",
    "val spark = SparkSession.builder()\n",
    "  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "  .config(\"spark.kryo.registrator\", \"org.apache.spark.graphx.GraphKryoRegistrator\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// 4. Data Loading & Preprocessing (unchanged)\n",
    "// ...\n",
    "\n",
    "// 5. Edge Processing with Optimized Grouping\n",
    "\n",
    "val windowedEdges = flights\n",
    "  .map { f =>\n",
    "    val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "    (windowStart, Edge(airportIdMap(f.origin), airportIdMap(f.dest), f.depDelay))\n",
    "  }\n",
    "  .aggregateByKey(ArrayBuffer.empty[Edge[Double]])(\n",
    "    (buf, edge) => buf += edge,\n",
    "    (buf1, buf2) => buf1 ++= buf2\n",
    "  )\n",
    "  .mapValues(_.toArray)\n",
    "  .partitionBy(new HashPartitioner(numPartitions))\n",
    "  .persist()\n",
    "\n",
    "val edgesByWindow = windowedEdges.groupByKey()\n",
    "val windowOrder = edgesByWindow.keys.collect().sorted\n",
    "\n",
    "// 6. Batched Processing\n",
    "windowOrder.grouped(6).foreach { windowBatch =>\n",
    "  windowBatch.foreach { windowStart =>\n",
    "    println(s\"Window starting ${windowStart}\")\n",
    "\n",
    "    val edges = edgesByWindow.lookup(windowStart).flatMap(identity)\n",
    "    if (edges.nonEmpty) {\n",
    "      val edgesRDD = spark.sparkContext.parallelize(edges, numPartitions).flatMap(_.toSeq)\n",
    "      val graph = Graph(currentVertices, edgesRDD).partitionBy(EdgePartition2D)\n",
    "      val updatedGraph = graph.pregel[Message](\n",
    "        Message(0.0, 0L), 2, EdgeDirection.Out\n",
    "      )(vProg, sendMsg, mergeMsg)\n",
    "      currentVertices = updatedGraph.vertices.cache()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Direct Parquet Write\n",
    "  currentVertices\n",
    "    .flatMap { case (id, delay) =>\n",
    "      windowBatch.map(ws => VertexResult(id, delay, new Timestamp(ws)))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .partitionBy(\"window_start\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_delay_only_partitioned.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddefff3d-d4a9-4462-b11b-eb1fd973c1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import java.sql.Timestamp\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "// 1. Classes\n",
    "case class VertexResult(id: Long, delay_load: Double, window_start: Timestamp)\n",
    "case class Message(sumDelay: Double, count: Long)\n",
    "case class Flight(origin: String, dest: String, depDelay: Double, timestamp: Timestamp)\n",
    "\n",
    "// 2. Top-level Pregel functions (no object/closure)\n",
    "def vProg(vertexId: VertexId, state: Double, msg: Message): Double = \n",
    "  if (msg.count == 0) state else state * 0.1 + (msg.sumDelay / msg.count) * 0.8\n",
    "\n",
    "def sendMsg(triplet: EdgeTriplet[Double, Double]): Iterator[(VertexId, Message)] = \n",
    "  Iterator((triplet.dstId, Message(triplet.attr * 0.5, 1L)))\n",
    "\n",
    "def mergeMsg(a: Message, b: Message): Message = \n",
    "  Message(a.sumDelay + b.sumDelay, a.count + b.count)\n",
    "\n",
    "\n",
    "// 4. Data loading (unchanged)\n",
    "val flights: RDD[Flight] = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")\n",
    "  .na.drop(Seq(\"ORIGIN\", \"DEST\"))\n",
    "  .withColumn(\"timestamp\", coalesce(col(\"actual_arr_utc\"), col(\"sched_arr_utc\")).cast(\"timestamp\"))\n",
    "  .filter(col(\"timestamp\").isNotNull)\n",
    "  .withColumnRenamed(\"ORIGIN\", \"origin\")\n",
    "  .withColumnRenamed(\"DEST\", \"dest\")\n",
    "  .na.fill(Map(\"DEP_DELAY\" -> 0))\n",
    "  .withColumnRenamed(\"DEP_DELAY\", \"depDelay\")\n",
    "  .as[Flight]\n",
    "  .rdd\n",
    "\n",
    "// 5. Airport ID mapping\n",
    "val airportCodes = flights.flatMap(f => Seq(f.origin, f.dest)).distinct().collect()\n",
    "val airportIdMap = airportCodes.zipWithIndex.map { case (code, idx) => (code, idx.toLong) }.toMap\n",
    "\n",
    "// 6. Edge processing\n",
    "val numPartitions = 200\n",
    "val windowedEdges = flights\n",
    "  .map { f =>\n",
    "    val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "    (windowStart, Edge(airportIdMap(f.origin), airportIdMap(f.dest), f.depDelay))\n",
    "  }\n",
    "  .reduceByKey((a, b) => a) // Deduplicate edges per window\n",
    "  .partitionBy(new org.apache.spark.HashPartitioner(numPartitions))\n",
    "  .persist()\n",
    "\n",
    "// 7. Initialize vertices\n",
    "val vertices = airportCodes.map(code => (airportIdMap(code), 0.0))\n",
    "var currentVertices: RDD[(VertexId, Double)] = spark.sparkContext.parallelize(vertices)\n",
    "\n",
    "// 8. Process windows in batches\n",
    "windowOrder.grouped(6).foreach { windowBatch =>\n",
    "  windowBatch.foreach { windowStart =>\n",
    "    val edges = windowedEdges.lookup(windowStart)\n",
    "    if (edges.nonEmpty) {\n",
    "      val edgesRDD = spark.sparkContext.parallelize(edges, numPartitions)\n",
    "      val graph = Graph(currentVertices, edgesRDD)\n",
    "      val updatedGraph = graph.pregel[Message](\n",
    "        Message(0.0, 0L),\n",
    "        2,\n",
    "        EdgeDirection.Out\n",
    "      )(vProg, sendMsg, mergeMsg)\n",
    "      currentVertices = updatedGraph.vertices.cache()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Write results\n",
    "  windowBatch.foreach { windowStart =>\n",
    "    currentVertices\n",
    "      .map { case (id, delay) => VertexResult(id, delay, new Timestamp(windowStart)) }\n",
    "      .toDF()\n",
    "      .write\n",
    "      .mode(\"append\")\n",
    "      .partitionBy(\"window_start\")\n",
    "      .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_delay_only_partitioned.parquet\")\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0d9d241a-bb99-40ed-8999-8fcfd5dd2ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.graphx.PartitionStrategy.EdgePartition2D\n",
    "import java.sql.Timestamp\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "// 1. Classes\n",
    "case class VertexResult(id: Long, delay_load: Double, window_start: Timestamp)\n",
    "case class Message(sumDelay: Double, count: Long)\n",
    "case class Flight(origin: String, dest: String, depDelay: Double, timestamp: Timestamp)\n",
    "\n",
    "// 2. Pregel Functions (top-level)\n",
    "def vProg(vertexId: VertexId, state: Double, msg: Message): Double = \n",
    "  if (msg.count == 0) state else state * 0.1 + (msg.sumDelay / msg.count) * 0.8\n",
    "\n",
    "def sendMsg(triplet: EdgeTriplet[Double, Double]): Iterator[(VertexId, Message)] = \n",
    "  Iterator((triplet.dstId, Message(triplet.attr * 0.5, 1L)))\n",
    "\n",
    "def mergeMsg(a: Message, b: Message): Message = \n",
    "  Message(a.sumDelay + b.sumDelay, a.count + b.count)\n",
    "\n",
    "// 3. Main Code\n",
    "val spark = SparkSession.builder()\n",
    "  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "  .config(\"spark.kryo.registrator\", \"org.apache.spark.graphx.GraphKryoRegistrator\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// 4. Data Loading & Preprocessing (unchanged)\n",
    "// ...\n",
    "\n",
    "// 5. Edge Processing with Optimized Grouping\n",
    "\n",
    "val windowedEdges = flights\n",
    "  .map { f =>\n",
    "    val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "    (windowStart, Edge(airportIdMap(f.origin), airportIdMap(f.dest), f.depDelay))\n",
    "  }\n",
    "  .aggregateByKey(ArrayBuffer.empty[Edge[Double]])(\n",
    "    (buf, edge) => buf += edge,\n",
    "    (buf1, buf2) => buf1 ++= buf2\n",
    "  )\n",
    "  .mapValues(_.toArray)\n",
    "  .partitionBy(new HashPartitioner(numPartitions))\n",
    "  .persist()\n",
    "\n",
    "val edgesByWindow = windowedEdges.groupByKey()\n",
    "val windowOrder = edgesByWindow.keys.collect().sorted\n",
    "\n",
    "// 6. Batched Processing\n",
    "windowOrder.grouped(6).foreach { windowBatch =>\n",
    "  windowBatch.foreach { windowStart =>\n",
    "    println(s\"Window starting ${windowStart}\")\n",
    "\n",
    "    val edges = edgesByWindow.lookup(windowStart).flatMap(identity)\n",
    "    if (edges.nonEmpty) {\n",
    "      val edgesRDD = spark.sparkContext.parallelize(edges, numPartitions).flatMap(_.toSeq)\n",
    "      val graph = Graph(currentVertices, edgesRDD).partitionBy(EdgePartition2D)\n",
    "      val updatedGraph = graph.pregel[Message](\n",
    "        Message(0.0, 0L), 2, EdgeDirection.Out\n",
    "      )(vProg, sendMsg, mergeMsg)\n",
    "      currentVertices = updatedGraph.vertices.cache()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Direct Parquet Write\n",
    "  currentVertices\n",
    "    .flatMap { case (id, delay) =>\n",
    "      windowBatch.map(ws => VertexResult(id, delay, new Timestamp(ws)))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .partitionBy(\"window_start\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_delay_only_partitioned.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ff5d19-59f3-47eb-a7cd-986d7490473a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val windowedEdges = flights\n",
    "  .map { f =>\n",
    "    val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "    (windowStart, Edge(airportIdMap(f.origin), airportIdMap(f.dest), f.depDelay))\n",
    "  }\n",
    "  .aggregateByKey(ArrayBuffer.empty[Edge[Double]])(\n",
    "    (buf, edge) => buf += edge,\n",
    "    (buf1, buf2) => buf1 ++= buf2\n",
    "  )\n",
    "  .mapValues(_.toArray)\n",
    "  .partitionBy(new HashPartitioner(numPartitions))\n",
    "  .persist()\n",
    "\n",
    "val edgesByWindow = windowedEdges.groupByKey()\n",
    "val windowOrder = edgesByWindow.keys.collect().sorted\n",
    "\n",
    "// 6. Batched Processing\n",
    "windowOrder.grouped(batchSize).foreach { windowBatch =>\n",
    "  windowBatch.foreach { windowStart =>\n",
    "    val edges = edgesByWindow.lookup(windowStart).flatMap(identity)\n",
    "    if (edges.nonEmpty) {\n",
    "      val edgesRDD = spark.sparkContext.parallelize(edges, numPartitions)\n",
    "      val graph = Graph(currentVertices, edgesRDD).partitionBy(EdgePartition2D)\n",
    "      val updatedGraph = graph.pregel[Message](\n",
    "        Message(0.0, 0L), 2, EdgeDirection.Out\n",
    "      )(vProg, sendMsg, mergeMsg)\n",
    "      currentVertices = updatedGraph.vertices.cache()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Direct Parquet Write\n",
    "  currentVertices\n",
    "    .flatMap { case (id, delay) =>\n",
    "      windowBatch.map(ws => VertexResult(id, delay, new Timestamp(ws)))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .partitionBy(\"window_start\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_delay_only_partitioned.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945bea45-db68-492a-a44b-81277a4d5d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import java.sql.Timestamp\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "// 1. Data Classes\n",
    "case class VertexResult(id: Long, delay_load: Double, window_start: Timestamp)\n",
    "case class Message(sumDelay: Double, count: Long)\n",
    "case class Flight(origin: String, dest: String, depDelay: Double, timestamp: Timestamp)\n",
    "\n",
    "// 3. Data Loading & Preprocessing\n",
    "val flights: RDD[Flight] = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")\n",
    "  .na.drop(Seq(\"ORIGIN\", \"DEST\"))\n",
    "  .withColumn(\"timestamp\", coalesce(col(\"actual_arr_utc\"), col(\"sched_arr_utc\")).cast(\"timestamp\"))\n",
    "  .filter(col(\"timestamp\").isNotNull)\n",
    "  .withColumnRenamed(\"ORIGIN\", \"origin\")\n",
    "  .withColumnRenamed(\"DEST\", \"dest\")\n",
    "  .na.fill(Map(\"DEP_DELAY\" -> 0.0))\n",
    "  .withColumnRenamed(\"DEP_DELAY\", \"depDelay\")\n",
    "  .as[Flight]\n",
    "  .rdd\n",
    "\n",
    "// 4. Airport ID Mapping\n",
    "val airportCodes = flights.flatMap(f => Seq(f.origin, f.dest)).distinct().collect()\n",
    "val airportIdMap = airportCodes.zipWithIndex.map { case (code, idx) => (code, idx.toLong) }.toMap\n",
    "\n",
    "// 5. Edge Processing with Optimized Grouping\n",
    "println(s\"Partitioning\")\n",
    "\n",
    "val windowedEdges = flights\n",
    "  .map { f =>\n",
    "    val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "    (windowStart, Edge(airportIdMap(f.origin), airportIdMap(f.dest), f.depDelay))\n",
    "  }\n",
    "  .aggregateByKey(ArrayBuffer.empty[Edge[Double]])(\n",
    "    (buf, edge) => buf += edge,\n",
    "    (buf1, buf2) => buf1 ++= buf2\n",
    "  )\n",
    "  .mapValues(_.toArray)\n",
    "  .partitionBy(new HashPartitioner(numPartitions))\n",
    "  .persist()\n",
    "\n",
    "// 6. Vertex Initialization\n",
    "val vertices = airportCodes.map(code => (airportIdMap(code), 0.0))\n",
    "var currentVertices: RDD[(VertexId, Double)] = spark.sparkContext.parallelize(vertices)\n",
    "\n",
    "// 7. Pregel Functions\n",
    "def vProg(vertexId: VertexId, state: Double, msg: Message): Double = \n",
    "  if (msg.count == 0) state else state * 0.1 + (msg.sumDelay / msg.count) * 0.8\n",
    "\n",
    "def sendMsg(triplet: EdgeTriplet[Double, Double]): Iterator[(VertexId, Message)] = \n",
    "  Iterator((triplet.dstId, Message(triplet.attr * 0.5, 1L)))\n",
    "\n",
    "def mergeMsg(a: Message, b: Message): Message = \n",
    "  Message(a.sumDelay + b.sumDelay, a.count + b.count)\n",
    "\n",
    "// 8. Window Processing with Batched Optimization\n",
    "val windowOrder = windowedEdges.keys.collect().sorted\n",
    "val batchSize = 6  // Process 6 windows at a time\n",
    "val totalBatches = math.ceil(windowOrder.length.toDouble / batchSize).toInt\n",
    "\n",
    "windowOrder.grouped(batchSize).zipWithIndex.foreach { case (windowBatch, batchIdx) =>\n",
    "  // 8.1 Process Batch\n",
    "  windowBatch.foreach { windowStart =>\n",
    "    println(s\"Window starting ${windowStart}\")\n",
    "\n",
    "    windowedEdges.lookup(windowStart).foreach { edges =>\n",
    "      if (edges.nonEmpty) {\n",
    "        val graph = Graph(\n",
    "          currentVertices, \n",
    "          spark.sparkContext.parallelize(edges, numPartitions)\n",
    "        )\n",
    "        currentVertices = graph.pregel[Message](\n",
    "          initialMsg = Message(0.0, 0L),\n",
    "          maxIterations = 2,\n",
    "          activeDirection = EdgeDirection.Out\n",
    "        )(vProg, sendMsg, mergeMsg)\n",
    "          .vertices\n",
    "          .cache()\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // 8.2 Batched Checkpointing\n",
    "  if (batchIdx % math.max(totalBatches / 5, 1) == 0) {\n",
    "    currentVertices.checkpoint()\n",
    "    currentVertices.count()  // Force materialization\n",
    "  }\n",
    "  \n",
    "  // 8.3 Batched Parquet Write\n",
    "  currentVertices\n",
    "    .flatMap { case (id, delay) =>\n",
    "      windowBatch.map(ws => \n",
    "        VertexResult(id, delay, new Timestamp(ws))\n",
    "      )\n",
    "    }\n",
    "    .toDF()\n",
    "    .repartition(numPartitions, col(\"window_start\"))  // Control file count\n",
    "    .write\n",
    "    .partitionBy(\"window_start\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_partitioned.parquet\")\n",
    "}\n",
    "\n",
    "// 9. Cleanup\n",
    "windowedEdges.unpersist()\n",
    "spark.sparkContext.clearJobGroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4a4eb5a4-5d42-406b-b2de-77b4db8be940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Top-level function definitions (not inside any object or class)\n",
    "def vProg(vertexId: VertexId, state: Double, msg: Message): Double = {\n",
    "  if (msg.count == 0) state else {\n",
    "    val avgDelay = msg.sumDelay / msg.count\n",
    "    state * 0.1 + avgDelay * 0.8\n",
    "  }\n",
    "}\n",
    "\n",
    "def sendMsg(triplet: EdgeTriplet[Double, Double]): Iterator[(VertexId, Message)] = {\n",
    "  val scaledDelay = triplet.attr * 0.5\n",
    "  Iterator((triplet.dstId, Message(scaledDelay, 1L)))\n",
    "}\n",
    "\n",
    "def mergeMsg(a: Message, b: Message): Message = {\n",
    "  Message(a.sumDelay + b.sumDelay, a.count + b.count)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49e39a7b-2278-4f3c-8821-38509ebf2f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "with prop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9619d03a-0f0e-49a8-b846-5e23279ee6e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Optimized edge grouping\n",
    "val edgesByWindow = windowedEdgesPartitioned\n",
    "  .mapValues(edge => Iterator(edge))\n",
    "  .reduceByKey(_ ++ _)\n",
    "\n",
    "\n",
    "\n",
    "// ---- 7. Batched Window Processing ----\n",
    "val batchSize = 6\n",
    "val checkpointInterval = math.max(windowOrder.length / 10, 1)\n",
    "\n",
    "windowOrder.grouped(batchSize).zipWithIndex.foreach { case (windowBatch, batchIdx) =>\n",
    "  windowBatch.foreach { windowStart =>\n",
    "\n",
    "    println(s\"Window starting ${windowStart}\")\n",
    "    val edges = edgesByWindow.lookup(windowStart).flatMap(identity)\n",
    "    if (edges.nonEmpty) {\n",
    "      val edgesRDD = spark.sparkContext.parallelize(edges, numPartitions)\n",
    "      val graph = Graph(currentVertices, edgesRDD)\n",
    "      val updatedGraph = graph.pregel[Message](\n",
    "        Message(0.0, 0L),\n",
    "        2,  // Fewer iterations for speed\n",
    "        EdgeDirection.Out\n",
    "      )(vProg, sendMsg, mergeMsg)\n",
    "      currentVertices = updatedGraph.vertices.cache()\n",
    "      // Checkpoint every N batches\n",
    "      if (batchIdx % checkpointInterval == 0) {\n",
    "        currentVertices.checkpoint()\n",
    "        currentVertices.count()\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  // Write batch results\n",
    "  val batchResults = windowBatch.flatMap { windowStart =>\n",
    "    currentVertices\n",
    "      .map { case (id, delay) => VertexResult(id, delay, new Timestamp(windowStart)) }\n",
    "      .collect()\n",
    "  }\n",
    "  spark.createDataFrame(batchResults)\n",
    "    .write\n",
    "    .partitionBy(\"window_start\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_delay_only_partitioned.parquet\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a103a8ce-2f89-45b3-9ccd-8f9efcd97dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// 1. Define Pregel functions in a serializable object\n",
    "object PregelFunctions extends Serializable {\n",
    "  val vProg = (vertexId: VertexId, state: (Double, Double), msg: Message) => {\n",
    "    if (msg.count == 0) state else {\n",
    "      val avgProp = msg.sumProp / msg.count\n",
    "      val avgDelay = msg.sumDelay / msg.count\n",
    "      val newDelay = state._1 * 0.1 + avgDelay * 0.8\n",
    "      (newDelay, avgProp)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  val sendMsg = (triplet: EdgeTriplet[(Double, Double), Double]) => {\n",
    "    val isDelayed = if (triplet.attr > 15) 1.0 else 0.0\n",
    "    Iterator((triplet.dstId, Message(isDelayed, triplet.attr * 0.5, 1L)))\n",
    "  }\n",
    "\n",
    "  val mergeMsg = (a: Message, b: Message) => {\n",
    "    Message(a.sumProp + b.sumProp, a.sumDelay + b.sumDelay, a.count + b.count)\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "\n",
    "// 3. Calculate dynamic partitioning\n",
    "val avgEdgesPerWindow = 42373908 / windowOrder.length\n",
    "val numPartitions = math.ceil(avgEdgesPerWindow / 100000).toInt  // ~100k edges/partition\n",
    "\n",
    "// 4. Process windows in batches (6 windows per batch)\n",
    "val batchSize = 6\n",
    "windowOrder.grouped(batchSize).zipWithIndex.foreach { case (windowBatch, batchIdx) =>\n",
    "  // Process each window in the batch\n",
    "  windowBatch.foreach { windowStart =>\n",
    "    val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "    \n",
    "    if (edges.nonEmpty) {\n",
    "      val graph = Graph(\n",
    "        currentVertices,\n",
    "        spark.sparkContext.parallelize(edges, numPartitions)\n",
    "      )\n",
    "      \n",
    "      val updatedGraph = graph.pregel[Message](\n",
    "        initialMsg = Message(0.0, 0.0, 0L),\n",
    "        maxIterations = 2,  // Reduced from 3\n",
    "        activeDirection = EdgeDirection.Out\n",
    "      )(\n",
    "        PregelFunctions.vProg,\n",
    "        PregelFunctions.sendMsg,\n",
    "        PregelFunctions.mergeMsg\n",
    "      )\n",
    "      \n",
    "      currentVertices = updatedGraph.vertices\n",
    "        .mapValues(v => v)\n",
    "        .cache()\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // Write batched results\n",
    "  val batchResults = windowBatch.flatMap { windowStart =>\n",
    "    currentVertices.map { case (id, (delay, prop)) =>\n",
    "      VertexResult(id, delay, prop, new Timestamp(windowStart))\n",
    "    }.collect()\n",
    "  }\n",
    "  \n",
    "  spark.createDataFrame(batchResults)\n",
    "    .write\n",
    "    .partitionBy(\"window_start\")\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_batched\")\n",
    "  \n",
    "  // Checkpoint every 10% of batches\n",
    "  if (batchIdx % math.ceil(windowOrder.length / batchSize / 10).toInt == 0) {\n",
    "    currentVertices.checkpoint()\n",
    "    currentVertices.count()  // Force materialization\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec98e7c-1412-4d8c-b790-993e02d6292c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "\n",
    "windowOrder.zipWithIndex.foreach { case (windowStart, idx) =>\n",
    "  val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "  \n",
    "  if (edges.nonEmpty) {\n",
    "    val graph = Graph(currentVertices, spark.sparkContext.parallelize(edges, numPartitions))\n",
    "  \n",
    "  val updatedGraph = graph.pregel[Message](\n",
    "    Message(0.0, 0.0, 0L),  // Initial message\n",
    "    3,                       // Max iterations (neighbors of neighbors of neighbors)\n",
    "    EdgeDirection.Out        // Active direction\n",
    "  )(\n",
    "    vProg _,    // Convert methods to function values\n",
    "    sendMsg _,\n",
    "    mergeMsg _\n",
    "  )\n",
    "\n",
    "    currentVertices = updatedGraph.vertices\n",
    "      .mapValues(v => v)\n",
    "      .cache()\n",
    "    \n",
    "    // Checkpoint every 6 windows\n",
    "    if (idx % 6 == 0) {\n",
    "      currentVertices.checkpoint()\n",
    "      currentVertices.count()\n",
    "    }\n",
    "  }\n",
    "  \n",
    "\n",
    "  // Save results to Parquet\n",
    "  currentVertices\n",
    "  .map { case (id, (delay, prop)) =>\n",
    "    VertexResult(id, delay, prop, new Timestamp(windowStart))\n",
    "  }\n",
    "  .toDF()\n",
    "  .write\n",
    "  .partitionBy(\"window_start\")\n",
    "  .mode(\"append\")\n",
    "  .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_partitioned\")\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201c44d9-4426-478a-847c-e7f61449c5a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "\n",
    "windowOrder.zipWithIndex.foreach { case (windowStart, idx) =>\n",
    "  val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "  \n",
    "  if (edges.nonEmpty) {\n",
    "    val graph = Graph(currentVertices, spark.sparkContext.parallelize(edges, numPartitions))\n",
    "  \n",
    "  val updatedGraph = graph.pregel[Message](\n",
    "    Message(0.0, 0.0),  // Initial message\n",
    "    3,                       // Max iterations (neighbors of neighbors of neighbors)\n",
    "    EdgeDirection.Out        // Active direction\n",
    "  )(\n",
    "    vProg _,    // Convert methods to function values\n",
    "    sendMsg _,\n",
    "    mergeMsg _\n",
    "  )\n",
    "\n",
    "    currentVertices = updatedGraph.vertices\n",
    "      .mapValues(v => v)\n",
    "      .cache()\n",
    "    \n",
    "    // Checkpoint every 6 windows\n",
    "    if (idx % 6 == 0) {\n",
    "      currentVertices.checkpoint()\n",
    "      currentVertices.count()\n",
    "    }\n",
    "  }\n",
    "  \n",
    "\n",
    "  // Save results to Parquet\n",
    "  currentVertices\n",
    "  .map { case (id, (delay, prop)) =>\n",
    "    VertexResult(id, delay, prop, new Timestamp(windowStart))\n",
    "  }\n",
    "  .toDF()\n",
    "  .write\n",
    "  .partitionBy(\"window_start\")\n",
    "  .mode(\"append\")\n",
    "  .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices_partitioned\")\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1331b30d-64cf-4741-bc2b-ed605cfc4e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515691dd-366f-416a-860b-8934e107fbbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "// // Broadcast the airport ID map\n",
    "// val bcAirportIds = spark.sparkContext.broadcast(airportIdMap)\n",
    "\n",
    "// Build vertices\n",
    "// val vertices = airportCodes.map { code =>\n",
    "//   (airportIdMap(code), (0.0, 0.2))  // (id, (delayLoad, propDelayed))\n",
    "// }.toSeq\n",
    "// val verticesRDD = spark.sparkContext.parallelize(vertices)\n",
    "\n",
    "// // Build edges with mapped IDs\n",
    "// val windowedEdges = flights.flatMap { f =>\n",
    "//   val srcId = bcAirportIds.value.getOrElse(f.origin, -1L)\n",
    "//   val dstId = bcAirportIds.value.getOrElse(f.dest, -1L)\n",
    "//   if (srcId == -1L || dstId == -1L) {\n",
    "//     None  // Filtered out by flatMap\n",
    "//   } else {\n",
    "//     val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "//     Some((windowStart, Edge(srcId, dstId, f.depDelay)))\n",
    "//   }\n",
    "// }\n",
    "\n",
    "// val edgesByWindow = windowedEdges.groupByKey()\n",
    "\n",
    "// Define message type to track sum and count for averages\n",
    "case class Message(sumProp: Double, sumDelay: Double, count: Long)\n",
    "\n",
    "// Class for saving results\n",
    "case class VertexResult(\n",
    "    id: Long, \n",
    "    delay_load: Double, \n",
    "    prop_delayed: Double, \n",
    "    window_start: Timestamp\n",
    ")\n",
    "\n",
    "// Vertex program definition\n",
    "def vProg(vertexId: VertexId, state: (Double, Double), msg: Message): (Double, Double) = {\n",
    "    println(s\"vProg called for vertex $vertexId with state $state and msg $msg\")\n",
    "    if (msg.count == 0) state else {\n",
    "        val avgProp = msg.sumProp / msg.count\n",
    "        val avgDelay = msg.sumDelay / msg.count\n",
    "        val newDelay = state._1 * 0.1 + avgDelay * 0.8\n",
    "        val newProp = avgProp\n",
    "        (newDelay, newProp)\n",
    "    }\n",
    "}\n",
    "\n",
    "// Message sending function\n",
    "def sendMsg(triplet: EdgeTriplet[(Double, Double), Double]): Iterator[(VertexId, Message)] = {\n",
    "  println(s\"sendMsg: src=${triplet.srcId}, dst=${triplet.dstId}, attr=${triplet.attr}\")\n",
    "    val srcProp = triplet.srcAttr._2\n",
    "    val scaledDelay = triplet.attr * 0.5\n",
    "    Iterator((triplet.dstId, Message(srcProp, scaledDelay, 1L)))\n",
    "}\n",
    "\n",
    "// Message merging function\n",
    "def mergeMsg(a: Message, b: Message): Message = {\n",
    "    Message(\n",
    "        a.sumProp + b.sumProp,\n",
    "        a.sumDelay + b.sumDelay, \n",
    "        a.count + b.count\n",
    "    )\n",
    "}\n",
    "\n",
    "var currentVertices: RDD[(VertexId, (Double, Double))] = verticesRDD\n",
    "val windowOrder = edgesByWindow.keys.collect().sorted\n",
    "\n",
    "windowOrder.foreach { windowStart =>\n",
    "  val edges = edgesByWindow.lookup(windowStart).flatMap(_.iterator)\n",
    "  \n",
    "  val graph = Graph(currentVertices, spark.sparkContext.parallelize(edges))\n",
    "  \n",
    "  val updatedGraph = graph.pregel[Message](\n",
    "    Message(0.0, 0.0, 0L),  // Initial message\n",
    "    3,                       // Max iterations\n",
    "    EdgeDirection.Out        // Active direction\n",
    "  )(\n",
    "    vProg _,    // Convert methods to function values\n",
    "    sendMsg _,\n",
    "    mergeMsg _\n",
    "  )\n",
    "\n",
    "  currentVertices = updatedGraph.vertices\n",
    "  .mapValues { (v: (Double, Double)) => \n",
    "    v  // Identity function with explicit type\n",
    "  }.cache()\n",
    "\n",
    "  // Save results to Parquet\n",
    "  currentVertices\n",
    "    .map { case (id, (delay, prop)) =>\n",
    "      VertexResult(id, delay, prop, new Timestamp(windowStart))\n",
    "    }\n",
    "    .toDF()\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/vertices\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98cd72d1-f6d6-4d1c-98d6-ee57be9e4b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4900402-2103-4057-8ed3-5cabdafc379b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.functions.coalesce\n",
    "\n",
    "// 1. Load data with coalesced timestamps\n",
    "val flights: RDD[Flight] = spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined__timefeat_seasfeat_cleaned_pr_v2.parquet/\")\n",
    "  .na.drop(Seq(\"ORIGIN\", \"DEST\")) // Filter rows with null origins/destinations first\n",
    "  .withColumn(\"timestamp\", coalesce(col(\"actual_arr_utc\"), col(\"sched_arr_utc\")).cast(\"timestamp\"))\n",
    "  .filter(col(\"timestamp\").isNotNull) // Remove rows with both timestamps null\n",
    "  .withColumnRenamed(\"ORIGIN\", \"origin\")\n",
    "  .withColumnRenamed(\"DEST\", \"dest\")\n",
    "  .na.fill(Map(\"DEP_DELAY\" -> 0))\n",
    "  .withColumnRenamed(\"DEP_DELAY\", \"depDelay\")\n",
    "  .as[Flight]\n",
    "  .rdd\n",
    "\n",
    "// 2. Verify flight data\n",
    "println(s\"Flight count after coalesce: ${flights.count()}\")\n",
    "// flights.limit(5).foreach { f =>\n",
    "//   println(s\"Flight: ${f.origin}->${f.dest} | Timestamp: ${f.timestamp} | Delay: ${f.depDelay}\")\n",
    "// }\n",
    "\n",
    "// 3. Rebuild airport ID map from FULL dataset\n",
    "val airportCodes = flights.flatMap(f => Seq(f.origin, f.dest)).distinct().collect()\n",
    "val airportIdMap = airportCodes.zipWithIndex.map { case (code, idx) => \n",
    "  (code, idx.toLong) \n",
    "}.toMap\n",
    "println(s\"Airport ID map size: ${airportIdMap.size}\")\n",
    "\n",
    "// 4. Debug edge creation with coalesced timestamps\n",
    "val windowedEdges = flights.map { f =>\n",
    "  val srcId = airportIdMap(f.origin)\n",
    "  val dstId = airportIdMap(f.dest)\n",
    "  val windowStart = f.timestamp.getTime - (f.timestamp.getTime % (4 * 60 * 60 * 1000))\n",
    "  (windowStart, Edge(srcId, dstId, f.depDelay))\n",
    "}\n",
    "\n",
    "// 5. Print edges with type annotations\n",
    "println(\"First 10 windowed edges:\")\n",
    "windowedEdges.take(10).foreach { \n",
    "  case (window: Long, edge: Edge[Double]) =>\n",
    "    val windowTime = new Timestamp(window).toString\n",
    "    val origin = airportIdMap.find(_._2 == edge.srcId).map(_._1).getOrElse(\"UNKNOWN\")\n",
    "    val dest = airportIdMap.find(_._2 == edge.dstId).map(_._1).getOrElse(\"UNKNOWN\")\n",
    "    println(s\"$windowTime: ${origin}(${edge.srcId}) -> ${dest}(${edge.dstId}) | Delay: ${edge.attr} mins\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f687e946-2275-4d9b-ba0f-666319688be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Map airport codes to unique Long IDs\n",
    "vertex_ids = train0.select(\"ORIGIN\").union(train0.select(\"DEST\")) \\\n",
    "    .distinct().rdd.map(lambda r: r[0]).zipWithUniqueId().collectAsMap()\n",
    "\n",
    "# Vertices: (id, (delay_load, prop_delayed))\n",
    "vertices_rdd = sc.parallelize([\n",
    "    (vertex_ids[row[\"ORIGIN\"]], (0.0, 0.2)) for row in train0.select(\"ORIGIN\").distinct().collect()\n",
    "])\n",
    "\n",
    "# Edges: (src_id, dst_id, delay_load, timestamp)\n",
    "edges_rdd = train0.rdd.map(lambda r: Edge(\n",
    "    vertex_ids[r[\"ORIGIN\"]], \n",
    "    vertex_ids[r[\"DEST\"]], \n",
    "    (r[\"DEP_DELAY\"], r[\"actual_arr_utc\"])\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae4e580-c260-4aeb-9b86-b6bf93bf92f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b244e7-3a5c-43ae-bd2f-4ea67a5bab63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edges_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af6f12b-1652-424d-8d99-6fbb7ed3da91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def assign_window(ts):\n",
    "    ts = datetime.fromisoformat(ts)\n",
    "    window_start = ts - (ts.hour % 4) * timedelta(hours=1)\n",
    "    return (window_start, window_start + timedelta(hours=4))\n",
    "\n",
    "windowed_edges = edges_rdd.map(lambda e: (assign_window(e.attr[1]), e)) \\\n",
    "    .groupByKey()  # Group edges by their 4-hour window\n",
    "\n",
    "\n",
    "def run_pregel(edges, initial_vertices):\n",
    "    graph = Graph(initial_vertices, edges)\n",
    "    \n",
    "    # Vertex update function\n",
    "    def vprog(vid, old_state, msg):\n",
    "        old_delay, old_prop = old_state\n",
    "        avg_prop, avg_delay = msg if msg else (0.0, 0.0)\n",
    "        new_delay = old_delay * 0.1 + avg_delay * 0.8\n",
    "        new_prop = avg_prop\n",
    "        return (new_delay, new_prop)\n",
    "    \n",
    "    # Message function (send to destination)\n",
    "    def send_msg(triplet):\n",
    "        src_delay, src_prop = triplet.srcAttr\n",
    "        edge_delay = triplet.attr[0]\n",
    "        return [(triplet.dstId, (src_prop, edge_delay * 0.5))]\n",
    "    \n",
    "    # Merge messages\n",
    "    def merge_msg(a, b):\n",
    "        prop_a, delay_a = a\n",
    "        prop_b, delay_b = b\n",
    "        return (prop_a + prop_b, delay_a + delay_b)\n",
    "    \n",
    "    # Run Pregel\n",
    "    return graph.pregel(\n",
    "        initial_msg=(0.0, 0.0), \n",
    "        max_iterations=3,\n",
    "        vprog=vprog,\n",
    "        sendMsg=send_msg,\n",
    "        mergeMsg=merge_msg\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4fd65b7c-c9b8-4d2f-ab80-37a6bf9ba22c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize vertex state\n",
    "current_vertices = vertices_rdd\n",
    "\n",
    "for window, edges in windowed_edges.collect():\n",
    "    print(f\"Processing {window}\")\n",
    "    \n",
    "    # Convert edges to GraphX format\n",
    "    graphx_edges = edges.map(lambda e: Edge(e.srcId, e.dstId, e.attr[0]))\n",
    "    \n",
    "    # Run Pregel\n",
    "    updated_graph = run_pregel(graphx_edges, current_vertices)\n",
    "    \n",
    "    # Update vertices for next window\n",
    "    current_vertices = updated_graph.vertices\n",
    "    \n",
    "    # Store results\n",
    "    current_vertices.map(lambda x: (x[0], x[1][0], x[1][1], window[0])) \\\n",
    "        .toDF([\"id\", \"delay_load\", \"prop_delayed\", \"window_start\"]) \\\n",
    "        .write.parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/pregel_rdd.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a208a6b1-fa66-4c9c-bbae-d57c24ed2402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# WORKING VERS RUN FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e47e32-360e-4023-8a52-b05d034e8066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0_msg = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\"),\n",
    "    F.col(\"actual_arr_utc\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df5f1a3-e606-4f9f-9b7d-f4a22b4ca81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde0fbdb-3a96-4394-83bc-abf10ae6bd8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2)) #initialized from EDA\n",
    ")\n",
    "vertices_store = vertices.limit(1).withColumn('window_start', F.lit('PASS')).withColumn('window_end', F.lit('PASS'))\n",
    "# Define edges with raw delay minutes (no normalization)\n",
    "edges = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"actual_arr_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"DEP_DELAY\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "\n",
    "# Add temporal windows (unchanged)\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hours\")\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "edges_with_windows.cache()\n",
    "edges_with_windows.count()\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy('window.start').collect()\n",
    "\n",
    "for window in windows:\n",
    "    print(f\"Processing window {window}\")\n",
    "    train0_msg=train0_msg.withColumns({'window_start': F.lit(window.start), 'window_end': F.lit(window.end)})\n",
    "\n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.lit(window.start)) & \n",
    "        (F.col(\"timestamp\") < F.lit(window.end))\n",
    "        )\n",
    "\n",
    "    # 2. Build graph using previous state\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "\n",
    "    # 3. Run Pregel with corrected logic\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            F.col(\"delay_load\")*.1 + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_delay\")*.8, \n",
    "                           F.lit(0.0)) # Sum delays\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"new_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.src(\"new_prop_delayed\").alias(\"prop\"),  # For prop_delayed updates\n",
    "                (Pregel.edge(\"delay_load\") * .5).alias(\"delay\")  # Delay scaled by src's prop_delayed\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(F.when(Pregel.msg().getItem(\"delay\") > 15, 1).otherwise(0)).alias(\"avg_prop\"),  # Average edge_prop_delayed\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")).alias(\"avg_delay\")  # Average scaled delays\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()\n",
    "\n",
    "    # 4. Update vertices for next window (cap values if needed)\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    vertices_store = vertices_store.unionByName(vertices.withColumn('window_start', F.lit(window.start)) \\\n",
    "                         .withColumn('window_end', F.lit(window.end)))\n",
    "    \n",
    "    # 5. Force materialization to persist state\n",
    "    vertices.count()\n",
    "    vertices_store.cache()\n",
    "    vertices_store.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6201b9f3-45dd-4489-b684-d34de1d08b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.groupBy('FL_DATE').agg(F.countDistinct('ORIGIN').alias('distinct_origin_count')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad316e5d-0ed4-42ef-890e-ae45bb734175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2)) #initialized from EDA\n",
    ")\n",
    "vertices_store = vertices.limit(1).withColumn('window_start', F.lit('PASS')).withColumn('window_end', F.lit('PASS'))\n",
    "# Define edges with raw delay minutes (no normalization)\n",
    "edges = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"actual_arr_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"DEP_DELAY\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "\n",
    "# Add temporal windows (unchanged)\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hours\")\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "edges_with_windows.cache()\n",
    "edges_with_windows.count()\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy('window.start').collect()\n",
    "\n",
    "for idx, window in enumerate(windows):\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # Filter edges\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.lit(window.start)) & \n",
    "        (F.col(\"timestamp\") < F.lit(window.end))\n",
    "    )\n",
    "    \n",
    "    current_vertices = \n",
    "\n",
    "    # 2. Build graph using previous state\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "\n",
    "    # 3. Run Pregel with corrected logic\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            F.col(\"delay_load\")*.1 + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_delay\")*.8, \n",
    "                           F.lit(0.0)) # Sum delays\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"new_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.src(\"new_prop_delayed\").alias(\"prop\"),  # For prop_delayed updates\n",
    "                (Pregel.edge(\"delay_load\") * .5).alias(\"delay\")  # Delay scaled by src's prop_delayed\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(F.when(Pregel.msg().getItem(\"delay\") > 15, 1).otherwise(0)).alias(\"avg_prop\"),  # Average edge_prop_delayed\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")).alias(\"avg_delay\")  # Average scaled delays\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(2) \\\n",
    "        .run()\n",
    "\n",
    "    # 4. Update vertices for next window (cap values if needed)\n",
    "    vertices.unpersist()\n",
    "\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    vertices.count()\n",
    "    \n",
    "    # Checkpoint periodically\n",
    "    if idx % 5 == 0:\n",
    "        vertices = vertices.checkpoint(eager=True)\n",
    "    \n",
    "\n",
    "    vertices.withColumn('window_start', F.lit(window.start)).withColumn('window_end', F.lit(window.end)).write.mode(\"append\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/pregel.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f528d835-9cbc-442f-b451-3244e95cd740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Configure Spark for graph processing\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2000\")\n",
    "\n",
    "def optimize_graph_processing(df):\n",
    "    # Vertex initialization with efficient partitioning\n",
    "    vertices = (\n",
    "        train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "        .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "        .distinct()\n",
    "        .repartitionByRange(200, \"id\")  # Range partitioning for locality\n",
    "        .withColumn(\"delay_load\", F.lit(0.0))\n",
    "        .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    )\n",
    "\n",
    "    # Edge processing with window-based partitioning\n",
    "    edges_with_windows = (\n",
    "        train0.select(\n",
    "            F.col(\"ORIGIN\").alias(\"src\"),\n",
    "            F.col(\"DEST\").alias(\"dst\"),\n",
    "            F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "            F.col(\"actual_arr_utc\").alias(\"timestamp\"), \n",
    "            F.col(\"DEP_DELAY\").alias(\"delay_load\")\n",
    "        )\n",
    "        .withColumn(\"window\", F.window(\"timestamp\", \"4 hours\"))\n",
    "        .withColumn(\"window_start\", F.col(\"window.start\"))  # Explicit column\n",
    "        .repartition(\"window_start\")  # Valid partition column\n",
    "        .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    )\n",
    "\n",
    "    windows = edges_with_windows.select(\"window.start\", \"window.end\").distinct().orderBy('start').collect()\n",
    "\n",
    "    # Batch processing with incremental checkpointing\n",
    "    for window_row in windows:\n",
    "        window = (window_row.start, window_row.end)\n",
    "        window_id = windows.index(window_row)\n",
    "        \n",
    "        # Set job group for resource management\n",
    "        spark.sparkContext.setJobGroup(\n",
    "            f\"window_{window_id}\", \n",
    "            f\"Processing {window[0]} - {window[1]}\"\n",
    "        )\n",
    "\n",
    "        # Filter edges using partition pruning\n",
    "        current_edges = edges_with_windows.filter(\n",
    "            (F.col(\"timestamp\") >= window[0]) & \n",
    "            (F.col(\"timestamp\") < window[1])\n",
    "        )\n",
    "\n",
    "        # Dynamic vertex pruning\n",
    "        active_vertices = current_edges.select(\"src\").union(current_edges.select(\"dst\")).distinct()\n",
    "        active_vertices = active_vertices.withColumnRenamed(\"src\", \"id\").withColumnRenamed(\"dst\", \"id\")\n",
    "        window_vertices = vertices.join(active_vertices, \"id\", \"inner\").persist()\n",
    "\n",
    "        # Build optimized graph structure\n",
    "        g = GraphFrame(window_vertices, current_edges)\n",
    "\n",
    "        # Pregel execution with message aggregation\n",
    "        result = g.pregel \\\n",
    "            .withVertexColumn(\n",
    "                \"new_delay_state\",\n",
    "                F.col(\"delay_load\"),\n",
    "                F.col(\"delay_load\") * 0.1 + \n",
    "                F.coalesce(Pregel.msg().getItem(\"avg_delay\") * 0.8, F.lit(0.0))\n",
    "            ) \\\n",
    "            .withVertexColumn(\n",
    "                \"new_prop_delayed\",\n",
    "                F.col(\"prop_delayed\"),\n",
    "                F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0))\n",
    "            ) \\\n",
    "            .sendMsgToDst(\n",
    "                F.struct(\n",
    "                    Pregel.src(\"new_prop_delayed\").alias(\"prop\"),\n",
    "                    (Pregel.edge(\"delay_load\") * 0.5).alias(\"delay\")\n",
    "                )\n",
    "            ) \\\n",
    "            .aggMsgs(\n",
    "                F.struct(\n",
    "                    F.avg(F.when(Pregel.msg().getItem(\"delay\") > 15, 1).otherwise(0)).alias(\"avg_prop\"),\n",
    "                    F.avg(Pregel.msg().getItem(\"delay\")).alias(\"avg_delay\")\n",
    "                )\n",
    "            ) \\\n",
    "            .setMaxIter(2) \\\n",
    "            .run()\n",
    "\n",
    "        # Update global vertex state\n",
    "        new_vertices = result.select(\n",
    "            \"id\",\n",
    "            F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "            F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "        ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Merge with global state using incremental update\n",
    "        vertices = vertices.join(\n",
    "            new_vertices,\n",
    "            \"id\",\n",
    "            \"left_outer\"\n",
    "        ).select(\n",
    "            \"id\",\n",
    "            F.coalesce(new_vertices[\"prop_delayed\"], vertices[\"prop_delayed\"]).alias(\"prop_delayed\"),\n",
    "            F.coalesce(new_vertices[\"delay_load\"], vertices[\"delay_load\"]).alias(\"delay_load\")\n",
    "        ).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Checkpoint every 10 windows using reliable storage\n",
    "        if window_id % 10 == 0:\n",
    "            checkpoint_path = f\"dbfs:/checkpoints/vertices_{window_id}\"\n",
    "            vertices.write.mode(\"overwrite\").parquet(checkpoint_path)\n",
    "            vertices = spark.read.parquet(checkpoint_path).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Batch output with coalesced writes\n",
    "        window_vertices = vertices.withColumn('window_start', F.lit(window[0])) \\\n",
    "                                  .withColumn('window_end', F.lit(window[1])) \\\n",
    "                                  .coalesce(4)  # Reduce output files\n",
    "        \n",
    "        window_vertices.write.mode(\"append\").parquet(\n",
    "            \"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/pregel.parquet\"\n",
    "        )\n",
    "\n",
    "        # Cleanup intermediate datasets\n",
    "        window_vertices.unpersist()\n",
    "        current_edges.unpersist()\n",
    "        new_vertices.unpersist()\n",
    "\n",
    "    return vertices\n",
    "\n",
    "optimize_graph_processing(train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128c1be2-ff50-4ea1-8309-5f0e6732e387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/interim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7debc60f-d912-4f44-aebf-67a2d310d0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices_store.union(vertices.withColumn('window_start', F.lit(window.start)) \\\n",
    "                         .withColumn('window_end', F.lit(window.end)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e1961f-3ddc-4241-8234-0fc01fc83e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vertices.withColumn('window_start', F.lit(window.start)).withColumn('window_end', F.lit(window.end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a4fd38f-88d2-46fe-a75d-48e0300a7768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00af4e1-4dfd-44db-973b-415130bcddff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.orderBy('sched_depart_utc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70577b8b-c7a5-4b7d-9588-a68bb6843521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'window: {window}')\n",
    "display(edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.lit(window.start)) & \n",
    "        (F.col(\"timestamp\") < F.lit(window.end))\n",
    "    ).orderBy(F.desc(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e2b5c6-0884-4a34-9ff8-0f755cfbf5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for w in windows:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cb5d4f41-eb8c-4f70-9d16-0b7443b6d932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize vertices with non-zero values\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.001))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.001))\n",
    ")\n",
    "\n",
    "# Define edges limited to a single day and 10 records\n",
    "edges = train0.filter(F.col(\"sched_depart_utc\").between(\"2015-01-01\", \"2015-01-01 23:59:59\")) \\\n",
    "    .select(\n",
    "        F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "        F.col(\"ORIGIN\").alias(\"dst\"),\n",
    "        F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "        F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "        F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    "    ).limit(10)\n",
    "\n",
    "print(f'Edges:')\n",
    "edges.show()\n",
    "\n",
    "edges.cache()\n",
    "\n",
    "\n",
    "# Use 24-hour windows to group all test edges\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"24 hours\")\n",
    ")\n",
    "\n",
    "edges_with_windows.cache()\n",
    "print(f'Edges with windows: ')\n",
    "edges_with_windows.show()\n",
    "# Collect the single window\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\").distinct().collect()\n",
    "\n",
    "for window in windows:\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    print(f'Current edges:')\n",
    "    current_edges.show()\n",
    "\n",
    "    # Build graph and run Pregel (same as before)\n",
    "    # ...\n",
    "\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.orderBy(F.desc(\"delay_load\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac71e416-e1d3-420c-bddb-5c1318aa9ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(edges)\n",
    "display(current_edges)\n",
    "display(edges_with_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c035ba-8b07-418d-89c7-8fd64c94cea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(current_edges.filter(F.col(\"dst\") == \"PPG\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3dfb1b-4ab3-4294-ba4e-f3e835ebcac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.filter(F.col('priorflight_dest')=='PPG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2aad39ad-dae1-43e0-b680-ac33b67ae5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for window in windows[10:13]:\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"lookback_start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Precompute source's average edge_prop_delayed\n",
    "    src_prop = current_edges.groupBy(\"src\").agg(\n",
    "        F.avg(\"edge_prop_delayed\").alias(\"_tmp_src_prop\")\n",
    "    )\n",
    "    \n",
    "    # 3. Join with vertices and fill nulls\n",
    "    vertices_window = vertices.join(\n",
    "        src_prop,\n",
    "        vertices.id == src_prop.src,\n",
    "        \"left\"\n",
    "    ).fillna(0.0, subset=[\"_tmp_src_prop\"])\n",
    "\n",
    "    # 4. Build graph\n",
    "    g = GraphFrame(vertices_window, current_edges)\n",
    "\n",
    "    # 5. Run Pregel with structured messages\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            (F.col(\"delay_load\") * 0.2) +  # 80% decay\n",
    "            F.coalesce(Pregel.msg().getItem(\"sum_delay\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"_tmp_prop\",\n",
    "            F.col(\"_tmp_src_prop\"),\n",
    "            (F.col(\"_tmp_prop\") * 0.9) +  # 10% decay\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0)) * 0.1\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                (Pregel.edge(\"edge_prop_delayed\") * 0.85).alias(\"prop\"),  # Scaled by decay\n",
    "                (Pregel.edge(\"delay_load\") * Pregel.src(\"_tmp_prop\")).alias(\"delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),\n",
    "                F.avg(Pregel.msg().getItem(\"delay\")).alias(\"sum_delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()\n",
    "    \n",
    "    # 7. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    result.orderBy(F.desc('new_delay_state')).show(10, truncate=False)\n",
    "\n",
    "    # Checkpoint the vertices DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2408a70c-94e1-4100-861e-381d61e32612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import Pregel\n",
    "\n",
    "#initialize vertices\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(15)) #initialization super important - maybe better to use a more informed metric\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    ")\n",
    "\n",
    "#define edges: basically the t-1 flight for each record \n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "#non overlapping windows\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hour\")  # Non-overlapping by default\n",
    ")\n",
    "\n",
    "#collect distinct windows (no lookback_start needed)\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[1:13]:  #sample subset of windows\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges strictly within the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Build graph using previous vertex state carried over\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "\n",
    "    # 3. Run Pregel\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            F.col(\"delay_load\")*.5 + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_delay\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"new_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.edge(\"edge_prop_delayed\").alias(\"prop\"),\n",
    "                (Pregel.edge(\"delay_load\")*Pregel.src(\"new_prop_delayed\")).alias(\"delay_scaled\")\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),  # Avg proportion of delayed flights at the source\n",
    "                F.avg(Pregel.msg().getItem(\"delay_scaled\")).alias(\"avg_delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(5) \\\n",
    "        .run()\n",
    "    \n",
    "    # 4. Update vertices for next window\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 5. Force materialization\n",
    "    vertices.count()\n",
    "\n",
    "    # 6. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.filter(F.col('id').isin('JFK', 'BOS', 'ORD','LAX','SFO','MSP')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf836c3d-3350-4ac6-9956-e87469a86140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import Pregel\n",
    "\n",
    "#initialize vertices\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0)) #initialization super important - maybe better to use a more informed metric\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    ")\n",
    "\n",
    "\n",
    "edges = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"DEP_DELAY\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "#non overlapping windows\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hour\")  # Non-overlapping by default\n",
    ")\n",
    "\n",
    "#collect distinct windows (no lookback_start needed)\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[1:13]:  #sample subset of windows\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges strictly within the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Build graph using previous vertex state carried over\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "\n",
    "    # 3. Run Pregel\n",
    "    result = g.pregel \\\n",
    "        .withVertexColumn(\n",
    "            \"new_delay_state\",\n",
    "            F.col(\"delay_load\"),\n",
    "            F.col(\"delay_load\")*.4 + \n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_delay\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .withVertexColumn(\n",
    "            \"new_prop_delayed\",\n",
    "            F.col(\"prop_delayed\"),\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.0))\n",
    "        ) \\\n",
    "        .sendMsgToDst(\n",
    "            F.struct(\n",
    "                Pregel.edge(\"edge_prop_delayed\").alias(\"prop\"),\n",
    "                (Pregel.edge(\"delay_load\")*Pregel.src(\"new_prop_delayed\")).alias(\"delay_scaled\")\n",
    "            )\n",
    "        ) \\\n",
    "        .aggMsgs(\n",
    "            F.struct(\n",
    "                F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),  # Avg proportion of delayed flights at the source\n",
    "                F.avg(Pregel.msg().getItem(\"delay_scaled\")).alias(\"avg_delay\")\n",
    "            )\n",
    "        ) \\\n",
    "        .setMaxIter(3) \\\n",
    "        .run()\n",
    "    \n",
    "    # 4. Update vertices for next window\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_prop_delayed\").alias(\"prop_delayed\"),\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 5. Force materialization\n",
    "    vertices.count()\n",
    "\n",
    "    # 6. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.filter(F.col('id').isin('JFK', 'BOS', 'ORD','LAX','SFO','MSP')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee0d8aaf-976f-4733-9be2-fd44f305fc95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "EMA version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67aadf2-32ae-4ec6-a234-0f5516ab7164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "come back to this when i have acutal edge specific risks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496da275-fcfe-4b81-b2a9-cc1c9ed4b234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import Pregel\n",
    "\n",
    "# Initialize vertices with historical_prop\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    "    .withColumn(\"historical_prop\", F.lit(0.2))  # Initial historical proportion\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "# Add non-overlapping 4-hour windows\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hours\")  # Adjust window size as needed\n",
    ")\n",
    "\n",
    "# Collect distinct windows\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[100:113]:  # Process windows 1-12\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Compute current window's average edge_prop_delayed per source\n",
    "    window_src_prop = current_edges.groupBy(\"src\").agg(\n",
    "        F.avg(\"edge_prop_delayed\").alias(\"current_prop\")\n",
    "    )\n",
    "    \n",
    "    # 3. Join with vertices to update historical_prop (EMA: 70% new, 30% historical)\n",
    "    vertices_window = vertices.join(\n",
    "            window_src_prop,\n",
    "            vertices.id == window_src_prop.src,\n",
    "            \"left\"\n",
    "        ).withColumn('current_prop', \n",
    "                     F.coalesce(F.col(\"current_prop\"),  F.col(\"historical_prop\"))\n",
    "        ).withColumn(\n",
    "            \"new_historical_prop\",\n",
    "            F.col(\"historical_prop\") * 0.2 + F.col(\"current_prop\") * 0.8  #somewhat influenced by historical but not a lot\n",
    "        )\n",
    "\n",
    "    # 4. Build graph with updated historical_prop\n",
    "    g = GraphFrame(vertices_window, current_edges)\n",
    "\n",
    "    # 5. Run Pregel using historical_prop in messages\n",
    "    result = g.pregel \\\n",
    "    .withVertexColumn(\n",
    "        \"new_delay_state\",\n",
    "        F.col(\"delay_load\"),\n",
    "        F.col(\"delay_load\") * .15 + F.coalesce(\n",
    "            Pregel.msg().getItem(\"avg_delay\")*Pregel.msg().getItem(\"avg_prop\"), \n",
    "            F.lit(0.0))\n",
    "    ) \\\n",
    "    .withVertexColumn(\n",
    "            \"newhprop\",\n",
    "            F.col(\"new_historical_prop\"), # Retain EMA value\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.2))\n",
    "            ) \\\n",
    "    .sendMsgToDst(\n",
    "        F.struct(\n",
    "            Pregel.src(\"newhprop\").alias(\"prop\"),\n",
    "            (Pregel.edge(\"delay_load\")).alias(\"delay_scaled\")\n",
    "        )\n",
    "    ) \\\n",
    "    .aggMsgs(\n",
    "        F.struct(\n",
    "            F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),\n",
    "            F.avg(Pregel.msg().getItem(\"delay_scaled\")).alias(\"avg_delay\")\n",
    "        )\n",
    "    ) \\\n",
    "    .setMaxIter(3) \\\n",
    "    .run()\n",
    "\n",
    "    # 6. Update vertices for next window\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"newhprop\").alias(\"historical_prop\"),  # Carry forward\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 7. Force materialization\n",
    "    vertices.count()\n",
    "\n",
    "    # 8. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.filter(F.col('id').isin('JFK', 'BOS', 'ORD','LAX','SFO','DFW')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b58e4c2-491f-4d41-9333-1e75a661ad31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import Pregel\n",
    "\n",
    "# Initialize vertices with historical_prop\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    "    .withColumn(\"historical_prop\", F.lit(0.2))  # Initial historical proportion\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "# Add non-overlapping 4-hour windows\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hours\")  # Adjust window size as needed\n",
    ")\n",
    "\n",
    "# Collect distinct windows\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[100:113]:  # Process windows 1-12\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Compute current window's average edge_prop_delayed per source\n",
    "    window_src_prop = current_edges.groupBy(\"src\").agg(\n",
    "        F.avg(\"edge_prop_delayed\").alias(\"current_prop\")\n",
    "    )\n",
    "    \n",
    "    # 3. Join with vertices to update historical_prop (EMA: 70% new, 30% historical)\n",
    "    vertices_window = vertices.join(\n",
    "            window_src_prop,\n",
    "            vertices.id == window_src_prop.src,\n",
    "            \"left\"\n",
    "        ).withColumn('current_prop', \n",
    "                     F.coalesce(F.col(\"current_prop\"),  F.col(\"historical_prop\"))\n",
    "        ).withColumn(\n",
    "            \"new_historical_prop\",\n",
    "            F.col(\"historical_prop\") * 0.2 + F.col(\"current_prop\") * 0.8  # 50% historical, 50% current\n",
    "        )\n",
    "\n",
    "    # 4. Build graph with updated historical_prop\n",
    "    g = GraphFrame(vertices_window, current_edges)\n",
    "\n",
    "    # 5. Run Pregel using historical_prop in messages\n",
    "    result = g.pregel \\\n",
    "    .withVertexColumn(\n",
    "        \"new_delay_state\",\n",
    "        F.col(\"delay_load\"),\n",
    "        (F.col(\"delay_load\") * F.col('new_historical_prop')) +  #some of previous delay load + scaled incoming delay load\n",
    "        F.coalesce(Pregel.msg().getItem(\"avg_delay\"), F.lit(0.0))\n",
    "    ) \\\n",
    "    .withVertexColumn(\n",
    "            \"newhprop\",\n",
    "            F.col(\"new_historical_prop\"), # Retain EMA value\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.2))\n",
    "            ) \\\n",
    "    .sendMsgToDst(\n",
    "        F.struct(\n",
    "            Pregel.src(\"newhprop\").alias(\"prop\"),\n",
    "            (Pregel.edge(\"delay_load\") * 8).alias(\"delay_scaled\")\n",
    "        )\n",
    "    ) \\\n",
    "    .aggMsgs(\n",
    "        F.struct(\n",
    "            F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),\n",
    "            F.avg(Pregel.msg().getItem(\"delay_scaled\")).alias(\"avg_delay\")\n",
    "        )\n",
    "    ) \\\n",
    "    .setMaxIter(2) \\\n",
    "    .run()\n",
    "\n",
    "    # 6. Update vertices for next window\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"newhprop\").alias(\"historical_prop\"),  # Carry forward\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 7. Force materialization\n",
    "    vertices.count()\n",
    "\n",
    "    # 8. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.filter(F.col('id').isin('JFK', 'BOS', 'ORD','LAX','SFO','DFW')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dd73bf-78f6-4db3-9acf-4b1ae9985210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize vertices with historical_prop\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0.0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes\n",
    "edges = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\"),\n",
    "    F.col(\"actual_arr_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"DEP_DELAY\").alias(\"delay_load\").cast('double')\n",
    ")\n",
    "\n",
    "# Add non-overlapping 4-hour windows\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hours\")  # Adjust window size as needed\n",
    ")\n",
    "\n",
    "# Collect distinct windows\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "\n",
    "\n",
    "for window in windows[100:113]:  # Process windows 1-12\n",
    "    print(f\"Processing window {window}\")\n",
    "\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "    \n",
    "\n",
    "    g = GraphFrame(vertices, current_edges)\n",
    "\n",
    "    result = g.pregel \\\n",
    "    .withVertexColumn(\n",
    "        \"new_delay_state\",\n",
    "        F.col(\"delay_load\"), #initial\n",
    "        F.col(\"delay_load\") *.2 + F.coalesce(Pregel.msg().getItem(\"avg_delay\"), F.lit(0.0)) #update\n",
    "    ) \\\n",
    "    .withVertexColumn(\n",
    "        \"new_prop_delayed\",\n",
    "        F.col(\"prop_delayed\"), #initial\n",
    "        F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.2)) #update\n",
    "    ) \\\n",
    "    .sendMsgToDst(\n",
    "    F.struct(\n",
    "        (Pregel.src(\"prop_delayed\")*.8).alias(\"prop\"),  # Dynamic per window\n",
    "        (Pregel.edge(\"delay_load\")).alias(\"delay_scaled\")\n",
    "    )\n",
    ") \\\n",
    "    .aggMsgs(\n",
    "        F.struct(\n",
    "            F.avg(F.when(Pregel.msg().getItem('delay_scaled') > 15, 1).otherwise(0)\n",
    "                  ).alias(\"avg_prop\"),\n",
    "            F.avg(Pregel.msg().getItem(\"delay_scaled\")).alias(\"avg_delay\")\n",
    "        )\n",
    "    ) \\\n",
    "    .setMaxIter(2) \\\n",
    "    .run()\n",
    "\n",
    "    # 6. Update vertices for next window\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\"),\n",
    "        F.col('new_prop_delayed').alias('prop_delayed')\n",
    "    ).cache()\n",
    "    \n",
    "    # 7. Force materialization\n",
    "    vertices.count()\n",
    "\n",
    "    # 8. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.filter(F.col('id').isin('JFK', 'BOS', 'ORD','LAX','SFO','DFW')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4b95ea-cc87-4a93-9344-b2fa038f035b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(current_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2a0ad6-88c8-4498-afca-b585f7d447e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57424b7b-c0a6-4fb8-a89e-4bf1e14b0c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = g.pregel \\\n",
    "    .withVertexColumn(\n",
    "        \"new_delay_state\",\n",
    "        F.lit(0.0),\n",
    "        (F.col(\"delay_load\").cast(\"double\") * 0.2) + \n",
    "        F.coalesce(Pregel.msg().getItem(\"avg_delay\"), F.lit(0.0))\n",
    "    ) \\\n",
    "    .sendMsgToDst(\n",
    "        F.struct(\n",
    "            Pregel.edge(\"window_prop_delayed\").alias(\"prop\"),\n",
    "            (Pregel.edge(\"delay_load\").cast(\"double\") * Pregel.edge(\"window_prop_delayed\")).alias(\"delay_scaled\")\n",
    "        )\n",
    "    ) \\\n",
    "    .aggMsgs(\n",
    "        F.struct(\n",
    "            F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),\n",
    "            F.avg(Pregel.msg().getItem(\"delay_scaled\")).alias(\"avg_delay\")\n",
    "        )\n",
    "    ) \\\n",
    "    .setMaxIter(2) \\\n",
    "    .run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d31837-8020-4363-9113-fb658ae069f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize vertices with historical_prop\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    "    .withColumn(\"historical_prop\", F.lit(0.2))  # Initial historical proportion\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes\n",
    "edges = train0.select(\n",
    "    F.col(\"priorflight_origin\").alias(\"src\"),\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"priorflight_depdelay_calc\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "# Add non-overlapping 4-hour windows\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"4 hours\")  # Adjust window size as needed\n",
    ")\n",
    "\n",
    "# Collect distinct windows\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[100:113]:  # Process windows 1-12\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Compute current window's average edge_prop_delayed per source\n",
    "    window_src_prop = current_edges.groupBy(\"src\").agg(\n",
    "        F.avg(\"edge_prop_delayed\").alias(\"current_prop\")\n",
    "    )\n",
    "    \n",
    "    # 3. Join with vertices to update historical_prop (EMA: 70% new, 30% historical)\n",
    "    vertices_window = vertices.join(\n",
    "            window_src_prop,\n",
    "            vertices.id == window_src_prop.src,\n",
    "            \"left\"\n",
    "        ).withColumn('current_prop', \n",
    "                     F.coalesce(F.col(\"current_prop\"),  F.col(\"historical_prop\"))\n",
    "        ).withColumn(\n",
    "            \"new_historical_prop\",\n",
    "            F.col(\"historical_prop\") * 0.2 + F.col(\"current_prop\") * 0.8  # 50% historical, 50% current\n",
    "        )\n",
    "\n",
    "    # 4. Build graph with updated historical_prop\n",
    "    g = GraphFrame(vertices_window, current_edges)\n",
    "\n",
    "    # 5. Run Pregel using historical_prop in messages\n",
    "    result = g.pregel \\\n",
    "    .withVertexColumn(\n",
    "        \"new_delay_state\",\n",
    "        F.col(\"delay_load\"),\n",
    "        (F.col(\"delay_load\") * .2) +  #some of previous delay load + scaled incoming delay load\n",
    "        F.coalesce(Pregel.msg().getItem(\"avg_delay\"), F.lit(0.0))\n",
    "    ) \\\n",
    "    .withVertexColumn(\n",
    "            \"newhprop\",\n",
    "            F.col(\"new_historical_prop\"), # Retain EMA value\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.2))\n",
    "            ) \\\n",
    "    .sendMsgToDst(\n",
    "        F.struct(\n",
    "            Pregel.src(\"newhprop\").alias(\"prop\"),\n",
    "            (Pregel.edge(\"delay_load\") * Pregel.src('newhprop')).alias(\"delay_scaled\")\n",
    "        )\n",
    "    ) \\\n",
    "    .aggMsgs(\n",
    "        F.struct(\n",
    "            F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),\n",
    "            F.avg(Pregel.msg().getItem(\"delay_scaled\")).alias(\"avg_delay\")\n",
    "        )\n",
    "    ) \\\n",
    "    .setMaxIter(2) \\\n",
    "    .run()\n",
    "\n",
    "    # 6. Update vertices for next window\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"newhprop\").alias(\"historical_prop\"),  # Carry forward\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 7. Force materialization\n",
    "    vertices.count()\n",
    "\n",
    "    # 8. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.filter(F.col('id').isin('JFK', 'BOS', 'ORD','LAX','SFO','DFW')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f1c9dc-077b-4d05-9428-c59dac8ac89b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from graphframes.lib import Pregel\n",
    "\n",
    "# Initialize vertices with historical_prop\n",
    "vertices = (\n",
    "    train0.select(F.col(\"ORIGIN\").alias(\"id\"))\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\")))\n",
    "    .distinct()\n",
    "    .withColumn(\"delay_load\", F.lit(0))\n",
    "    .withColumn(\"prop_delayed\", F.lit(0.2))\n",
    "    .withColumn(\"historical_prop\", F.lit(0.2))  # Initial historical proportion\n",
    ")\n",
    "\n",
    "# Define edges with raw delay minutes\n",
    "edges = train0.select(\n",
    "    F.col(\"ORIGIN\").alias(\"src\"),\n",
    "    F.col(\"DEST\").alias(\"dst\"),\n",
    "    F.col(\"prop_delayed\").alias(\"edge_prop_delayed\"),\n",
    "    F.col(\"sched_depart_utc\").alias(\"timestamp\"), \n",
    "    F.col(\"DEP_DELAY\").alias(\"delay_load\")\n",
    ")\n",
    "\n",
    "# Add non-overlapping 4-hour windows\n",
    "edges_with_windows = edges.withColumn(\n",
    "    \"window\", \n",
    "    F.window(\"timestamp\", \"8 hours\")  # Adjust window size as needed\n",
    ")\n",
    "\n",
    "# Collect distinct windows\n",
    "windows = edges_with_windows.select(\"window.start\", \"window.end\") \\\n",
    "    .distinct().orderBy(\"window.start\").collect()\n",
    "\n",
    "for window in windows[100:113]:  # Process windows 1-12\n",
    "    print(f\"Processing window {window}\")\n",
    "    \n",
    "    # 1. Filter edges in the current window\n",
    "    current_edges = edges_with_windows.filter(\n",
    "        (F.col(\"timestamp\") >= F.col(\"window.start\")) & \n",
    "        (F.col(\"timestamp\") < F.col(\"window.end\"))\n",
    "    )\n",
    "\n",
    "    # 2. Compute current window's average edge_prop_delayed per source\n",
    "    window_src_prop = current_edges.groupBy(\"src\").agg(\n",
    "        F.avg(\"edge_prop_delayed\").alias(\"current_prop\")\n",
    "    )\n",
    "    \n",
    "    # 3. Join with vertices to update historical_prop (EMA: 70% new, 30% historical)\n",
    "    vertices_window = vertices.join(\n",
    "            window_src_prop,\n",
    "            vertices.id == window_src_prop.src,\n",
    "            \"left\"\n",
    "        ).withColumn('current_prop', \n",
    "                     F.coalesce(F.col(\"current_prop\"),  F.col(\"historical_prop\"))\n",
    "        )\n",
    "\n",
    "    # 4. Build graph with updated historical_prop\n",
    "    g = GraphFrame(vertices_window, current_edges)\n",
    "\n",
    "    # 5. Run Pregel using historical_prop in messages\n",
    "    result = g.pregel \\\n",
    "    .withVertexColumn(\n",
    "        \"new_delay_state\",\n",
    "        F.col(\"delay_load\"),\n",
    "        F.col(\"delay_load\") * .1 + F.coalesce(\n",
    "            Pregel.msg().getItem(\"avg_delay\"), \n",
    "            F.lit(0.0))\n",
    "    ) \\\n",
    "    .withVertexColumn(\n",
    "            \"newhprop\",\n",
    "            F.col(\"current_prop\"), # Retain EMA value\n",
    "            F.coalesce(Pregel.msg().getItem(\"avg_prop\"), F.lit(0.2))\n",
    "            ) \\\n",
    "    .sendMsgToDst(\n",
    "        F.struct(\n",
    "            Pregel.src(\"newhprop\").alias(\"prop\"),\n",
    "            (Pregel.edge(\"delay_load\")*.5).alias(\"delay_scaled\")\n",
    "        )\n",
    "    ) \\\n",
    "    .aggMsgs(\n",
    "        F.struct(\n",
    "            F.avg(Pregel.msg().getItem(\"prop\")).alias(\"avg_prop\"),\n",
    "            F.avg(Pregel.msg().getItem(\"delay_scaled\")).alias(\"avg_delay\")\n",
    "        )\n",
    "    ) \\\n",
    "    .setMaxIter(3) \\\n",
    "    .run()\n",
    "\n",
    "    # 6. Update vertices for next window\n",
    "    vertices = result.select(\n",
    "        \"id\",\n",
    "        F.col(\"newhprop\").alias(\"historical_prop\"),  # Carry forward\n",
    "        F.col(\"new_delay_state\").alias(\"delay_load\")\n",
    "    ).cache()\n",
    "    \n",
    "    # 7. Force materialization\n",
    "    vertices.count()\n",
    "\n",
    "    # 8. Show results\n",
    "    print(\"\\nFinal vertex states:\")\n",
    "    vertices.filter(F.col('id').isin('JFK', 'BOS', 'ORD','LAX','SFO','DFW')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263441fb-da97-4d40-9812-052944ed1c17",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_76b4019\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_46348256\",\"enabled\":true,\"columnId\":\"src\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterValues\":[\"ORD\"],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1744905922956}],\"syncTimestamp\":1744905922956}",
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(current_edges.filter(F.col(\"timestamp\") >= F.col(\"window.start\")).filter(F.col(\"timestamp\") < F.col(\"window.end\")).filter(F.col('delay_load')>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55da3f04-8b3f-4dfc-8a11-f45635dddae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.filter(F.col('ORIGIN')=='JFK').filter(F.col('TAIL_NUM').isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2aaf24b-b536-466c-b476-9c21d18d584d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.filter(F.col('priorflight_depdelay_calc')<0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85cc7c36-da59-48a7-aa0c-dd752bafd38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Personalized PR - scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75de5649-e30a-4cf1-a10c-e1d9add1a36c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Vertices (ordered alphabetically)\n",
    "v = train0.select(F.col(\"ORIGIN\").alias(\"id\")) \\\n",
    "    .union(train0.select(F.col(\"DEST\").alias(\"id\"))) \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"id\")\n",
    "\n",
    "# Edges (directed)\n",
    "e = train0.select(F.col(\"ORIGIN\").alias(\"src\"), F.col(\"DEST\").alias(\"dst\"))\n",
    "\n",
    "# Build graph\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Sources (ordered alphabetically to match vertices)\n",
    "sources_df = v.select(\"id\").distinct() \\\n",
    "    .orderBy(\"id\") \\\n",
    "    .withColumn(\"index\", F.row_number().over(Window.orderBy(\"id\")) - 1)\n",
    "\n",
    "sources_flat = sources_df.select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Run PPR with aligned sources\n",
    "pageranked = g.parallelPersonalizedPageRank(\n",
    "    resetProbability=0.15, \n",
    "    sourceIds=sources_flat, \n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "broadcast_sources = sc.broadcast(sources_flat)\n",
    "\n",
    "result_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"origin\", StringType()),\n",
    "        StructField(\"score\", DoubleType())\n",
    "    ])\n",
    ")\n",
    "def vector_to_dict(vector):\n",
    "    # Retrieve the broadcasted list\n",
    "    sources = broadcast_sources.value\n",
    "    \n",
    "    # Sort the vector entries and take top 10\n",
    "    sorted_entries = sorted(\n",
    "        [(i, float(v)) for i, v in enumerate(vector)], \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )[:10]\n",
    "    \n",
    "    # Map indices to actual source IDs\n",
    "    return [(sources_flat[i], float(v)) for i, v in sorted_entries]\n",
    "\n",
    "# Define UDF\n",
    "vector_to_dict_udf = udf(vector_to_dict, result_schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = pageranked.vertices.withColumn(\"pagerank_dict\", vector_to_dict_udf(\"pageranks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98a5930-f86a-4b17-83f2-9f32172ca115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0_edges = pageranked.edges.groupBy('src','dst').agg(F.sum('weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dcdde22-c8c0-46ce-b645-218ed56c3048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.filter(F.col('id')=='JFK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd070817-b69e-4f76-bcc4-4bef5d3858f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.filter(F.col('id')=='HYA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c34e3ec-0be4-4b32-8252-f96a7cd8f042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results.filter(F.col('id')=='LAX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c87a7b26-80fb-4352-bb37-c1ed32271369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "398bf554-7f82-45db-bbc6-74da8716a76f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7967a4e7-02e6-49f2-986b-419341bab23f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked.edges.filter(F.col('dst')=='JFK').orderBy(F.desc('weight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8501505b-0912-4288-9776-85684f2c8dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked.edges.filter(F.col('src')=='JFK').orderBy(F.desc('weight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c8b1dfb-404b-4f81-b497-06e0e655d335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0_edges.filter(F.col('src')=='JFK').orderBy(F.desc('sum(weight)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5041d65-2043-4889-87f2-dc324b6b9ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.filter(F.col('ORIGIN')=='JFK').groupBy('DEST').count().orderBy(F.col('count').desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc19f66-7905-4888-a66e-ea85f273ef4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0_edges.filter(F.col('dst')=='JFK').orderBy(F.desc('sum(weight)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940d30d2-8410-4c8c-94f0-768201297bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edge_delay_weights = train0.groupBy(\"ORIGIN\", \"DEST\").agg(\n",
    "    (F.sum(F.when(F.col(\"outcome\") == 1, 1).otherwise(0)) / F.count(\"*\")).alias(\"delay_ratio\")\n",
    ")\n",
    "\n",
    "reversed_edges = edge_delay_weights.select(\n",
    "    F.col(\"DEST\").alias(\"src\"),  # Incoming airport\n",
    "    F.col(\"ORIGIN\").alias(\"dst\"),  # Source of delays\n",
    "    F.col(\"delay_ratio\").alias(\"delay_weight\")  # Probability of delay propagation\n",
    ")\n",
    "\n",
    "# Build reversed graph\n",
    "g_reversed = GraphFrame(v, reversed_edges)\n",
    "target_airport = \"JFK\"  # Airport to predict delays for\n",
    "\n",
    "# Run PPR with target as the teleportation source\n",
    "pageranked1 = g_reversed.parallelPersonalizedPageRank(\n",
    "    resetProbability=0.15,\n",
    "    sourceIds=sources_flat,  # Focus on delays impacting this airport\n",
    "    maxIter=20\n",
    ")\n",
    "\n",
    "# Extract top influencers for the target\n",
    "def get_top_influencers(pagerank_vector, sources):\n",
    "    sorted_scores = sorted(\n",
    "        [(sources[i], float(score)) for i, score in enumerate(pagerank_vector)],\n",
    "        key=lambda x: -x[1]\n",
    "    )\n",
    "    return [x[0] for x in sorted_scores[1:11]]  # Exclude self-score\n",
    "\n",
    "get_top_influencers_udf = F.udf(\n",
    "    lambda v: get_top_influencers(v, sources_flat), \n",
    "    ArrayType(StringType())\n",
    ")\n",
    "\n",
    "results2 = pageranked1.vertices.filter(F.col(\"id\") == target_airport) \\\n",
    "    .withColumn(\"top_influencers\", get_top_influencers_udf(\"pageranks\")) \\\n",
    "    .select(\"id\", \"top_influencers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b71d612-9f86-49f7-8eab-c6552517a1ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results2.filter(F.col('id')=='JFK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d857c4ee-3800-4a44-9e1c-c840c747a91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked.edges.filter(F.col('dst')=='JFK').distinct().orderBy(F.desc('weight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd9173c-4f06-4391-a97b-4b8c564bad21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked.edges.filter(F.col('src')=='JFK').distinct().orderBy(F.desc('weight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f495bc-c357-4c33-88de-fdfb435ab5a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked1.edges.filter(F.col('dst')=='JFK').groupBy('src').agg(F.sum('weight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315df104-42bc-4950-9b57-e25a0a5fac21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked.edges.filter(F.col('dst')=='JFK').groupBy('src').agg(F.sum('weight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e56c55b-091e-4db2-80b8-91cff0f163a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked1.edges.filter(F.col('dst')=='JFK').distinct().orderBy(F.desc('weight')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2e878e-ff65-4e57-8d25-221b2ffd4a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked1.edges.filter(F.col('src')=='JFK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3696ab-04eb-4b4a-92cc-789690a251fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pageranked1.vertices.filter(F.col('id')=='JFK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d43be293-a481-4c20-b7eb-83dea846f8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.14-sg-5y-joined-graph-feateng-clean",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
