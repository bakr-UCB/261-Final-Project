{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7242c0b-e377-4108-94d5-687826addb3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modeling on Joined Data with Engineered Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea57f82-909d-40c5-a5ef-ff6655abcf9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e4de20-2fac-4c65-aa47-724b68203cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d62c3-ed6a-47d0-bb21-3c0faee08bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf, lag, pandas_udf\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7e45df-c877-46e1-b5f9-fa843a68c574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de30d1e2-82a4-461f-a323-79e9ca866d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"1y\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds\n",
    "k = 3\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup)\n",
    "compute_seasonality = False\n",
    "\n",
    "# define train/test split date\n",
    "min_test_dt = \"2019-10-01\"\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7820b43-a23e-488f-a7f3-d9b07493cf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data and perform simple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0b6fc6-dd49-4b2d-8230-dd9dc767304a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/modeling_checkpoints\")\n",
    "\n",
    "# read in joined, cleaned, engineered dataset\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_1y_weather_cleaned_combo.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcde94d-04e9-42e9-bb3e-28944f486832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf, lag, pandas_udf\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a34c73f-b66b-4117-9cfe-68eb5a6bf662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### lag cols: v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485233fa-d1fb-497c-8412-de2899166450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WindowConditions = Window.partitionBy(\"TAIL_NUM\").orderBy(\"sched_depart_utc\")\n",
    "\n",
    "WhenConditions = (f.col(\"ORIGIN\") == f.col(\"priorflight_dest\")) & (f.col(\"priorflight_deptime\") >= f.col(\"twentysix_hours_prior_depart_UTC\"))\n",
    "\n",
    "\n",
    "def add_lags(df):\n",
    "    #todo: add in when conditions, handle cancellations\n",
    "    result_df = (df\n",
    "                 .withColumn(\"special_cases\", f.when(WhenConditions, 1).otherwise(f.lit(0.0)))\n",
    "                 .withColumn(\"priorflight_origin\",\n",
    "                             lag(\"ORIGIN\").over(WindowConditions))\n",
    "                 .withColumn(\"priorflight_dest\",\n",
    "                             lag(\"DEST\").over(WindowConditions))\n",
    "                 .withColumn('priorflight_cancelled_true',  #~~~~true cancellation status, assumed known (?)\n",
    "                             lag('CANCELLED').over(WindowConditions))\n",
    "                 .withColumn(\"twentysix_hours_prior_depart_UTC\",\n",
    "                             (f.col(\"two_hours_prior_depart_UTC\") - f.expr(\"INTERVAL 24 HOURS\")).cast(\"timestamp\")\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_sched_deptime\",\n",
    "                             f.when(WhenConditions, lag(\"sched_depart_utc\").over(WindowConditions))\n",
    "                             .otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_elapsed_time_calc\", #~~~crs estimated\n",
    "                             lag(\"CRS_ELAPSED_TIME\").over(WindowConditions))\n",
    "                    .withColumn(\"priorflight_elapsed_time_calc\", #~~~turned into interval\n",
    "                                f.when(WhenConditions,f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_elapsed_time_calc\"))\n",
    "                                .otherwise(None)\n",
    "                    )\n",
    "\n",
    "                 .withColumn(\"priorflight_depdelay_true\", #~~~true dep delay\n",
    "                             f.when(WhenConditions,lag(\"DEP_DELAY\").over(WindowConditions))\n",
    "                             .otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "\n",
    "\n",
    "                 .withColumn(\"priorflight_deptime_true\", #~~~true dep time based on true dep delay\n",
    "                     f.when(WhenConditions, (f.col(\"priorflight_sched_deptime\") + \n",
    "                     (f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_depdelay_true\"))))\n",
    "                     .otherwise(None)\n",
    "                 )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_isdeparted\", #~~~ only 1 when we definitely knew it left already\n",
    "                             f.when((f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")) \n",
    "                                    & WhenConditions, 1).otherwise(0) #we don't really know about the prior flight for the when conditions\n",
    "                                 ) \n",
    "                 .withColumn(\"priorflight_depdelay_calc\", #~~~estimated dep delay\n",
    "                        f.when(\n",
    "                            # Case 1: Flight departed BEFORE observation window ***** add in when conditions\n",
    "                            ((f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")) & WhenConditions),\n",
    "                            f.col(\"priorflight_depdelay_true\")  # Full delay known\n",
    "                        ).when(\n",
    "                            # Case 2: Flight scheduled to depart BEFORE window, but departed DURING observation window\n",
    "                            (f.col(\"priorflight_sched_deptime\") <= f.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                            (f.col(\"priorflight_deptime_true\") > f.col(\"two_hours_prior_depart_UTC\")) &\n",
    "                            WhenConditions,\n",
    "                            (f.col(\"two_hours_prior_depart_UTC\").cast('long') - f.col(\"priorflight_sched_deptime\").cast('long')) / 60  # Partial delay\n",
    "                        ).otherwise(\n",
    "                            f.lit(0.0)  # Flight scheduled to depart AFTER window; we know nothing and assume departed on time\n",
    "                        )\n",
    "                    )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_deptime_calc\", #~~~estimated dep time based on estimated dep delay\n",
    "                                f.col(\"priorflight_sched_deptime\") + \n",
    "                                (f.expr(\"INTERVAL 1 MINUTE\") * f.col(\"priorflight_depdelay_calc\")) \n",
    "                            )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_isdelayed_calc\", #~~~estimated delay indicator ** ADJUSTED FOR CANCELLED\n",
    "                             f.when( ((f.col(\"priorflight_depdelay_calc\") >= 15) | f.col('priorflight_cancelled_true') == 1), 1).otherwise(0)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"elapsed_time_true\", #~~~true elapsed time for current flight\n",
    "                             f.when(WhenConditions,(f.col(\"AIR_TIME\") + f.col(\"TAXI_IN\") + f.col(\"TAXI_OUT\")\n",
    "                             ).cast(\"int\")).otherwise(None)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"arr_time_true\", #~~~true arrival time based on true dp time + true elased time\n",
    "                             (\n",
    "                                f.col(\"priorflight_deptime_true\").cast(\"long\") + \n",
    "                                (f.col(\"priorflight_elapsed_time_calc\").cast(\"long\")/60)\n",
    "                            ).cast(\"timestamp\")\n",
    "                        )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_arr_time_true\", #~~~true prior flight arrival time\n",
    "                             lag(\"arr_time_true\").over(WindowConditions)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_isarrived_calc\", #~~~estimated arrival indicator based on whether flight landed before window\n",
    "                             f.when(f.col(\"priorflight_arr_time_true\") <= f.col(\"two_hours_prior_depart_UTC\") & WhenConditions,1).otherwise(0)\n",
    "                             )\n",
    "                 \n",
    "                 .withColumn(\"priorflight_arr_time_calc\", #~~~estimated arrival time based on 3 scenarios\n",
    "                        f.when(\n",
    "                            f.col(\"priorflight_isarrived_calc\") == 1,  # Case 1: Dep before window, arr after window\n",
    "                            f.col(\"priorflight_arr_time_true\") #so we know the info\n",
    "                        ).when(\n",
    "                            (f.col(\"priorflight_isarrived_calc\") == 0) &  # Case 2: Dep before window, arr after window\n",
    "                            (f.col(\"priorflight_deptime_true\") <= f.col(\"two_hours_prior_depart_UTC\")), \n",
    "                            f.col(\"priorflight_deptime_true\") + f.col(\"priorflight_elapsed_time_calc\")\n",
    "                        ).otherwise(\n",
    "                            f.col(\"priorflight_deptime_calc\") + f.col(\"priorflight_elapsed_time_calc\")  # dep after window, arr after window\n",
    "                        ))\n",
    "                 \n",
    "                 .withColumn(\"turnaround_time_calc\", \n",
    "                             #~~~estimated how much time we have between estimated arrival of prior flight and scheduled departure of current flight\n",
    "                    (f.when(WhenConditions,\n",
    "                        ((f.col(\"sched_depart_utc\").cast(\"long\") - \n",
    "                        f.col(\"priorflight_arr_time_calc\").cast(\"long\")) / 60\n",
    "                    ).cast(\"double\"))\n",
    "                    ).otherwise(None))\n",
    "    )\n",
    "\n",
    "\n",
    "    #fill in edge cases where prior flight info is misleading or unhelpful: \n",
    "            #1) prior flight is cancelled\n",
    "            #2) prior flight dest is different from current flight origin\n",
    "\n",
    "    #cols to estimate: DEP_DELAY,\n",
    "    result_df = (result_df\n",
    "                 .withColumn()\n",
    "                 \n",
    "\n",
    "    )\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "result = add_lags(df)\n",
    "\n",
    "\n",
    "\n",
    "#prior flight departed - strong yes OR no\n",
    "#put in when conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c37b499-2e88-4a99-b2c0-f382278a381b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result=spark.read.parquet(\"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined_1y_weather_cleaned_combo.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348d9085-aa19-4cc5-8ba6-596c61f968d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')=='259NV').filter(f.col('sched_depart_utc').contains('2019-01-02')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06094348-b65b-460d-b644-91f156c58271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col(\"ORIGIN\") != f.col(\"priorflight_dest\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b36c047-655a-46da-9713-2e58fcb46ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769b23b3-a792-4cec-bafd-aa50d73574c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WindowConditions = Window.partitionBy(\"TAIL_NUM\").orderBy(\"sched_depart_utc\")\n",
    "\n",
    "display(result.withColumn(\"prior_cancelled\", \n",
    "                  lag(\"CANCELLED\").over(WindowConditions)).filter(f.col('prior_cancelled')==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32b7d21c-cb9d-41c2-8c5b-4877723380ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "impute missing turnaroudns based on current origin -> destination ema something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c6e822-8f5f-4406-bc4b-f6b70c18af15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('TAIL_NUM')==\"N102UW\").filter(f.col('sched_depart_utc').contains('2019-04-09')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed797ad-5388-450c-8c14-623a6c6f72d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.filter(f.col('DEP_DELAY').isNull()).groupBy('CANCELLED').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "955a167e-fe31-4e03-a092-9403f663315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "null_counts = result.select(\n",
    "    [f.count(f.when(f.col(c).cast(\"long\").isNull() | f.isnan(f.col(c).cast(\"long\")), c)).alias(c) \n",
    "     for c in result.columns]\n",
    ")\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e661571-df29-46b9-88b0-697aefd80db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result.filter(f.col('priorflight_arr_time_calc').isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2989a01-5f51-4b25-89ac-8a5b001ebfab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(f.col('TAIL_NUM').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc5635c-67b3-4783-9c2d-3e3f23df9ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "output_path = \"dbfs:/student-groups/Group_4_1/interim/join_checkpoints/joined_1y_weather_cleaned_combo_lags.parquet\"\n",
    "(\n",
    "    result.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour variable (needed for seasonality)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b48f564-6cf2-41bb-9b00-d48f11b5da1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt)\n",
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7063c0-5d7c-419e-9c71-77077666a075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train.count() #force materialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e079e4-266b-4e77-9ba8-154a1f0addfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits(df, k=3, blocking=False, dep_utc_varname=\"dep_datetime\", verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation\n",
    "    '''\n",
    "    n = df.count()\n",
    "    df = df.withColumn(\"row_id\", f.row_number()\n",
    "            .over(Window.partitionBy().orderBy(dep_utc_varname)))\n",
    "    chunk_size = np.floor(n/(k+1))\n",
    "\n",
    "    idx = np.arange(0,)\n",
    "    idx = np.arange(0,n,chunk_size)\n",
    "    idx[-1] = n-1\n",
    "    idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print('')\n",
    "        print(f'Number of validation datapoints for each fold is {chunk_size:,}')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    bin_edges = df.filter(f.col(\"row_id\").isin(idx)).select(\"row_id\",dep_utc_varname).toPandas()\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = bin_edges[dep_utc_varname][0]\n",
    "        else:\n",
    "            t_min_train = bin_edges[dep_utc_varname][i]\n",
    "        # define maximum training time\n",
    "        t_max_train = bin_edges[dep_utc_varname][i+1]\n",
    "        # define minimum test time\n",
    "        t_min_test = bin_edges[dep_utc_varname][i+1]\n",
    "        # define maximum test_time\n",
    "        t_max_test = bin_edges[dep_utc_varname][i+2]\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get cross-validation split times\n",
    "cv_cutoffs = get_cv_time_limits(df_train.select(dep_utc_varname), k=3, blocking=True, \n",
    "    dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaba55bd-601f-4def-a082-b259656574c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train seasonality models for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831adc2c-61e3-4984-a797-764958c5d2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# informed by: https://www.databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html\n",
    "\n",
    "def forecast_delay(history_pd: pd.DataFrame) -> pd.DataFrame: \n",
    "    \n",
    "    # define Prophet model\n",
    "    model = Prophet(\n",
    "        interval_width=0.9,\n",
    "        growth='linear',\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        # holidays=us_holidays,\n",
    "        # seasonality_mode='multiplicative'\n",
    "    )\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(history_pd)\n",
    "    \n",
    "    # configure predictions\n",
    "    future_pd = model.make_future_dataframe(\n",
    "        periods=24*7, \n",
    "        freq='h',\n",
    "        include_history=False\n",
    "    )\n",
    "    \n",
    "    # make predictions\n",
    "    results_pd = model.predict(future_pd)\n",
    "\n",
    "    # ref date and dow\n",
    "    ref_date = history_pd.ds.iloc[0].date()\n",
    "    ref_dow = history_pd.DAY_OF_WEEK[0]\n",
    "\n",
    "    # helper function: get day of the week,\n",
    "    # using reference date and dow\n",
    "    def get_dow(x,ref_date,dow):\n",
    "        d_days = (x.date() - ref_date).days + dow\n",
    "        d_days = d_days%7\n",
    "        if d_days == 0:\n",
    "            d_days = 7\n",
    "        return d_days\n",
    "\n",
    "    # get dow for forecasted points\n",
    "    results_pd['dow'] = results_pd.ds.apply(lambda x: get_dow(x,ref_date,ref_dow))\n",
    "\n",
    "    # get hour for forecasted points\n",
    "    results_pd['hour'] = results_pd.ds.apply(lambda x: x.hour)\n",
    "\n",
    "    # store origin\n",
    "    results_pd['ORIGIN'] = history_pd.ORIGIN.iloc[0]\n",
    "        \n",
    "    # return components\n",
    "    return results_pd[['dow','hour','weekly','daily','ORIGIN']]\n",
    "\n",
    "schema = StructType([StructField('dow', LongType(), True),\n",
    "                     StructField('hour', LongType(), True),\n",
    "                     StructField('weekly', DoubleType(), True),\n",
    "                     StructField('daily', DoubleType(), True),\n",
    "                     StructField('ORIGIN', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3583341c-b65e-4213-bd4e-005b21d1ff44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality( df, t_min, t_max, \n",
    "    dep_utc_varname=dep_utc_varname, delay_varname=\"DEP_DELAY\", \n",
    "    forecast_fn=forecast_delay, schema=schema ):\n",
    "\n",
    "    return (\n",
    "        df.filter((df[dep_utc_varname] >= t_min) & \\\n",
    "            (df[dep_utc_varname] < t_max))\n",
    "        .withColumnRenamed(delay_varname, \"y\")\n",
    "        .withColumnRenamed(dep_utc_varname, \"ds\")\n",
    "        .groupBy('ORIGIN')\n",
    "            .applyInPandas(forecast_fn, schema=schema)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53913682-beee-47d2-9f66-20151ae8913b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if compute_seasonality:\n",
    "    # train seasonality model for each cross validation split\n",
    "    for i in range(k):\n",
    "        # train seasonality model for this cross validation split\n",
    "        model = get_seasonality(df_train, cv_cutoffs[\"train_min\"][i], cv_cutoffs[\"train_max\"][i])\n",
    "        # write out\n",
    "        fn_out = f\"seasonality_model_{period}_cv{i}of{k}.parquet\"\n",
    "        model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")\n",
    "\n",
    "    # train seasonality model for full training data\n",
    "    model = get_seasonality(df_train, datetime(1970,1,1), min_test_dt)\n",
    "    # write out\n",
    "    fn_out = f\"seasonality_model_{period}_train.parquet\"\n",
    "    model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5bdccd-7230-44de-9d2e-c2fba8819855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ca65bd-089d-4899-8384-4f7bc2025ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define columns to be used in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70cedcfe-3f9c-4619-84bc-a2d7bff0df35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b21a3fcd-ae80-4d80-a53a-d8d3b3043771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c4ef7d-8964-4aca-a618-768a5ef36855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\", \"origin_HourlyDryBulbTemperature\", \"origin_HourlyWindSpeed\"] #keep wet bulb & gust speed\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\"]\n",
    "\n",
    "# date related columns\n",
    "# TODO: encode as ordinal categorical rather than numeric ???\n",
    "date_cols = [\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\"]\n",
    "\n",
    "# prior delay columns\n",
    "priors_num = [\"priorflight_depdelay_final\", \"priorflight_deptime_final\",\"est_tail_turnaround_window_min\", \"priorflight_arr_time_calc_final\", \"priorflght_est_arr_time_final\", \"est_tail_turnaround_window_min\", \"priorflight_distance\"]\n",
    "\n",
    "priors_cat = [\"priorflight_isdelayed\",\"priorflight_arrived\"]\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",\"DAY_OF_WEEK\",\"ORIGIN\",\"dep_hour_utc\",dep_utc_varname]\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *date_cols, *priors_num]\n",
    "categorical_cols = [*flight_metadata_cols, *priors_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49b3a36-1d30-4e49-8a46-d09c615a27b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "null_counts = df.select(\n",
    "    [count(when(col(c).cast(\"double\").isNull() | isnan(col(c).cast(\"double\")), c)).alias(c) \n",
    "     for c in priors_num]\n",
    ")\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2686b64-5709-4da3-9d87-2983b4564854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(f.col('priorflight_deptime_final').isNotNull()).filter(f.col('est_tail_turnaround_window_min').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fe7adff-209b-47fa-ab83-297bbcc85a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "prior flight dep time not available -> use prior flight sched dep time (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d19302e-e0e4-48e0-96e6-0d6f6f9e7cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(f.col('est_tail_turnaround_window_min').isNull()) \\\n",
    "        .filter(f.col('priorflight_arr_time_calc_final').isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd54247-2eb6-491e-9ef8-695e334711e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_noloc = df.filter(f.col('origin_LATITUDE').isNotNull()).select(\n",
    "    [count(when(col(c).cast(\"double\").isNull() | isnan(col(c).cast(\"double\")), c)).alias(c) \n",
    "     for c in priors_num]\n",
    ")\n",
    "\n",
    "display(null_noloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b9676c-7da2-427a-b2e7-f88b13d5f28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3cd118-b967-4e04-a17e-e45022a0b13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# Index and encode each categorical column\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\",handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_vec\",handleInvalid=\"keep\")\n",
    "    stages += [indexer, encoder]\n",
    "# define encoded categorical feature names\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "# assemble features\n",
    "features = numeric_cols + categorical_vec_columns\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid='keep')\n",
    "\n",
    "# scale features\n",
    "scaler = MinMaxScaler(inputCol=\"features\", \\\n",
    "    outputCol=\"features_scaled\")\n",
    "\n",
    "# logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features_scaled', \\\n",
    "    labelCol='outcome',maxIter=50)\n",
    "\n",
    "# construct pipeline object from all components\n",
    "pipeline = Pipeline(stages=stages+[assembler,scaler,lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b582b393-c135-43e6-a3c4-21177449b2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2d4b17-f16a-44c5-919b-a49112ff8fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality_data(df, fold, k):\n",
    "  \"\"\"\n",
    "  Look up seasonlaity features from saved seasonality model.\n",
    "  \"\"\"\n",
    "  if fold == 'full':\n",
    "      fn_model = f\"seasonality_model_{period}_train.parquet\"\n",
    "  else:\n",
    "      fn_model = f\"seasonality_model_{period}_cv{fold}of{k}.parquet\"\n",
    "  model = spark.read.parquet(f\"{team_BASE_DIR}/interim/{fn_model}\")\n",
    "\n",
    "  joined_df = df.join(model, \n",
    "                    (df[\"ORIGIN\"] == model[\"ORIGIN\"]) & \n",
    "                    (df[\"DAY_OF_WEEK\"] == model[\"dow\"]) & \n",
    "                    (df[\"dep_hour_utc\"] == model[\"hour\"]),\n",
    "                    how=\"left\").drop(model[\"ORIGIN\"])\n",
    "  \n",
    "  return joined_df\n",
    "\n",
    "# CODE BELOW DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def upsample(train_df,verbose=False):\n",
    "  '''Upsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = non_delay_count / delay_count\n",
    "\n",
    "  train_delay = train_df.filter(f.col('outcome') == 0)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 1).sample(withReplacement=True, fraction=keep_percent,seed=42)\n",
    "  train_upsampled = train_delay.union(train_non_delay)\n",
    "  return train_upsampled\n",
    "\n",
    "\n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n",
    "\n",
    "def cv_eval(preds):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  return F2, pr\n",
    "\n",
    "def timeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timSeriesSplit k-fold cross validation \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    # elif sampling=='weights':\n",
    "    #   train_df = add_class_weights(train_df).cache()\n",
    "      \n",
    "    #print info on train and dev set for this fold\n",
    "    if verbose:\n",
    "      print('    TRAIN set for fold {} goes from {} to {}, count is {:,} flights ({})'.format((i+1), \n",
    "                                                                                      train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      train_df.count(),\n",
    "                                                                                      sampling + '-sampled' if sampling else 'no sampling'))\n",
    "      print('    DEV set for fold {} goes from {} to {}, count is {:,} flights'.format((i+1), \n",
    "                                                                                      dev_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      dev_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      dev_df.count()))\n",
    "      \n",
    "    # TODO: remove once feat engineering applied outside\n",
    "    train_df = get_seasonality_data(train_df, i, k)\n",
    "    train_df = train_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "    dev_df = get_seasonality_data(dev_df, i, k)\n",
    "    dev_df = dev_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "\n",
    "    # print(train_df.dtypes)\n",
    "    # print(dev_df.dtypes)\n",
    "        \n",
    "    # Fit params on the model\n",
    "    model = pipeline.fit(train_df)\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "  # # Train on full df\n",
    "  # print('Training on full train dataset, and validating on dev dataset with best parameters from CV:')\n",
    "  # print(best_parameters)\n",
    "    \n",
    "  # if verbose:\n",
    "  #   print('    TRAIN set for best parameter fitted model goes from {} to {}, count is {:,} flights ({})'.format(train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.count(),\n",
    "  #                                                                                                    sampling + '-sampled' if sampling else 'no sampling'))\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4243676d-12cf-46f5-94d0-c235a3741a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train, cross validate, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd38f66-1f12-49fe-a619-227a69cecd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7e5aaf-398f-43c4-9b80-a4c7858bf91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "df_train = downsample(df_train).cache()\n",
    "df_train = get_seasonality_data(df_train, 'full', k).cache()\n",
    "df_test = get_seasonality_data(df_test, 'full', k).cache()\n",
    "model = pipeline.fit(df_train)\n",
    "dev_pred = model.transform(df_test)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.12-sg-joined-modeling-feateng",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
