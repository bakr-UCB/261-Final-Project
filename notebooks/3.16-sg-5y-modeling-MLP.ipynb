{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea57f82-909d-40c5-a5ef-ff6655abcf9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a6327e-1199-4273-86b3-bb53c01e3c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Configure Spark settings for better performance\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder\\\n",
    "#     .config(\"spark.executor.memory\", \"16g\")\\\n",
    "#     .config(\"spark.executor.cores\", 4)\\\n",
    "#     .appName('Final Project Training')\\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "# spark.conf.set(\"spark.default.parallelism\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d62c3-ed6a-47d0-bb21-3c0faee08bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, when\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0961268-eafb-4755-9d8c-7541236ade55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cbc4b7a-aaa3-4ba6-8df3-e98af097300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7e45df-c877-46e1-b5f9-fa843a68c574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de30d1e2-82a4-461f-a323-79e9ca866d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds and overlap\n",
    "k = 5\n",
    "overlap = 0.2\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup)\n",
    "compute_seasonality = False\n",
    "apply_seasonality = False\n",
    "\n",
    "# define train/test split date\n",
    "if period == \"3m\":\n",
    "    min_test_dt = \"2015-03-01\"\n",
    "elif period == \"1y\":\n",
    "    min_test_dt = \"2019-10-01\"\n",
    "elif period == \"\":\n",
    "    min_test_dt = \"2019-01-01\"\n",
    "print(f\"Min test set date for {period} dataset: {min_test_dt}\")\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7820b43-a23e-488f-a7f3-d9b07493cf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data and perform simple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "\n",
    "# read in joined, cleaned dataset\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather_{period}.parquet\") # !!!\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_weather_cleaned_combo.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather{period}_v1.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned.parquet\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned_pr_v2.parquet\")\n",
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour and date variables (needed for seasonality and CV splits, respectively)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname))) \\\n",
    "    .withColumn(\"dep_date_utc\", to_date(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5455f469-d289-4a5c-935a-ebe06e5f16d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Group by the year and count the number of records for each year\n",
    "# df_year_counts = df.groupBy(\"YEAR\").count()\n",
    "\n",
    "# # Display the result\n",
    "# display(df_year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f01162-ab6e-4c58-b491-27d618bccdb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('pagerank_train', (df['train_0'] + df['train_1'] + df['train_2'] + df['train_3'] + df['train_4']) / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "# df_train.cache()\n",
    "# print(f\"Train data: {df_train.count()} records\")\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt) \\\n",
    "    .filter(f.col(dep_utc_varname) < \"2020-01-01\")\n",
    "# df_test.cache()\n",
    "# print(f\"Test data: {df_test.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa30565b-2ab1-45f9-ac57-8832abc556d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get cross-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc820cf1-c179-4279-8fe0-505d1758f9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits_by_days_with_overlap(df, k=3, blocking=False, overlap=0, dep_utc_varname=dep_utc_varname, verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation, based on # days in dataset\n",
    "    '''\n",
    "    \n",
    "    min_date = df.select(f.min(\"dep_date_utc\")).collect()[0][0]\n",
    "    max_date = df.select(f.max(\"dep_date_utc\")).collect()[0][0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "    total_width = k+1 - overlap*(k-1)\n",
    "    chunk_size = np.ceil(n_days/total_width) # last chunk may be slightly smaller than the others\n",
    "\n",
    "    # idx = np.arange(0,)\n",
    "    # idx = np.arange(0,n_days,chunk_size)\n",
    "    # idx[-1] = n_days-1\n",
    "    # idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Splitting data into {k} folds with {overlap} overlap')\n",
    "        print(f'Min date: {min_date}, max date: {max_date}')\n",
    "        print(f'{chunk_size:,} days per fold')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define indices based on chunk size and overlap\n",
    "        if i == 0:\n",
    "            train_min_offset = 0\n",
    "            train_max_offset = chunk_size\n",
    "        else:\n",
    "            train_min_offset += np.ceil((1-overlap)*chunk_size)\n",
    "            train_max_offset += np.floor((1-overlap)*chunk_size)\n",
    "        test_min_offset = train_max_offset\n",
    "        test_max_offset = test_min_offset + chunk_size\n",
    "\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = min_date\n",
    "        else:\n",
    "            t_min_train = min_date + timedelta(days=train_min_offset)\n",
    "        # define maximum training time\n",
    "        t_max_train = min_date + timedelta(days=train_max_offset)\n",
    "        # define minimum test time\n",
    "        t_min_test = min_date + timedelta(days=test_min_offset)\n",
    "        # define maximum test_time\n",
    "        t_max_test = min_date + timedelta(days=test_max_offset)\n",
    "\n",
    "        if t_max_test > max_date + timedelta(1):\n",
    "            t_max_test = max_date + timedelta(1)\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        print(\"(Note that the max dates are non-inclusive)\")\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc02f46-6e68-4780-91f9-e82265e6a3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs = [\n",
    "    {\"train_min\": \"2014-12-31\", \"train_max\": \"2015-10-09\", \"test_min\": \"2015-10-09\", \"test_max\": \"2016-07-17\"},\n",
    "    {\"train_min\": \"2015-08-14\", \"train_max\": \"2016-05-21\",\"test_min\": \"2016-05-21\", \"test_max\": \"2017-02-27\"},\n",
    "    {\"train_min\": \"2016-03-27\", \"train_max\": \"2017-01-01\",\"test_min\": \"2017-01-01\", \"test_max\": \"2017-10-10\"},\n",
    "    {\"train_min\": \"2016-11-08\", \"train_max\": \"2017-08-14\",\"test_min\": \"2017-08-14\", \"test_max\": \"2018-05-23\"},\n",
    "    {\"train_min\": \"2017-06-22\", \"train_max\": \"2018-03-27\",\"test_min\": \"2018-03-27\", \"test_max\": \"2019-01-01\"}\n",
    "    ]\n",
    "cv_cutoffs = pd.DataFrame(cv_cutoffs)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # get cross-validation split times\n",
    "# cv_cutoffs = get_cv_time_limits_by_days_with_overlap(df_train.select(\"dep_date_utc\"), k=k, blocking=True, overlap=overlap,\n",
    "#     dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "# cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5bdccd-7230-44de-9d2e-c2fba8819855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modeling Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ca65bd-089d-4899-8384-4f7bc2025ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define columns to be used in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c4ef7d-8964-4aca-a618-768a5ef36855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\"]\n",
    "\n",
    "# date related columns\n",
    "date_cols = [\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\"]\n",
    "\n",
    "# prior & current flight cols\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                ]\n",
    "\n",
    "bool_flight_cols = ['priorflight_isdeparted', \n",
    "                    'priorflight_isarrived_calc',\n",
    "                    'priorflight_isdelayed_calc',\n",
    "                    'priorflight_cancelled_true']\n",
    "\n",
    "# graph columns\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",dep_utc_varname] + [col for col in df.columns if \"train\" in col or \"test\" in col or \"daily\" in col or \"weekly\" in col or \"yearly\" in col or \"holidays\" in col]\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "categorical_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef77554-7fce-44d3-a857-07385a623e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_cols = [*keep_me, *numeric_cols, *categorical_cols]\n",
    "filter_cols = [c for c in filter_cols if c in df.columns]\n",
    "\n",
    "df_train = df_train.select(filter_cols)\n",
    "df_train.cache()\n",
    "df_test = df_test.select(filter_cols)\n",
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b582b393-c135-43e6-a3c4-21177449b2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19622a56-5023-4b73-8ba4-c4e30ed7f17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2d4b17-f16a-44c5-919b-a49112ff8fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE BELOW DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def upsample(train_df,verbose=False):\n",
    "  '''Upsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = non_delay_count / delay_count\n",
    "\n",
    "  train_delay = train_df.filter(f.col('outcome') == 0)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 1).sample(withReplacement=True, fraction=keep_percent,seed=42)\n",
    "  train_upsampled = train_delay.union(train_non_delay)\n",
    "  return train_upsampled\n",
    "\n",
    "\n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n",
    "\n",
    "def cv_eval(preds):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  return F2, pr\n",
    "\n",
    "def timeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timSeriesSplit k-fold cross validation \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "    # print(f\"{train_df.count()} (unsampled) TRAIN records in fold {i+1}\")\n",
    "    # print(f\"{dev_df.count()} DEV records in fold {i+1}\")\n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    # elif sampling=='weights':\n",
    "    #   train_df = add_class_weights(train_df).cache()\n",
    "      \n",
    "    # #print info on train and dev set for this fold\n",
    "    # if verbose:\n",
    "    #   print('    TRAIN set for fold {} goes from {} to {}, count is {:,} flights ({})'.format((i+1), \n",
    "    #                                                                                   train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "    #                                                                                   train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "    #                                                                                   train_df.count(),\n",
    "    #                                                                                   sampling + '-sampled' if sampling else 'no sampling'))\n",
    "    #   print('    DEV set for fold {} goes from {} to {}, count is {:,} flights'.format((i+1), \n",
    "    #                                                                                   dev_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "    #                                                                                   dev_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "    #                                                                                   dev_df.count()))\n",
    "    \n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    \n",
    "    print(f\"training for fold {i}\")\n",
    "        \n",
    "    # Fit params on the model\n",
    "    model = pipeline.fit(train_df)\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "  # # Train on full df\n",
    "  # print('Training on full train dataset, and validating on dev dataset with best parameters from CV:')\n",
    "  # print(best_parameters)\n",
    "    \n",
    "  # if verbose:\n",
    "  #   print('    TRAIN set for best parameter fitted model goes from {} to {}, count is {:,} flights ({})'.format(train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.count(),\n",
    "  #                                                                                                    sampling + '-sampled' if sampling else 'no sampling'))\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7e5aaf-398f-43c4-9b80-a4c7858bf91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "def final_eval(df_train, df_test,pipeline):\n",
    "    df_train = downsample(df_train).cache()\n",
    "    df_train = df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"train\",\"pagerank\")\n",
    "    df_train = df_train.fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "    df_test = df_test \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"train\",\"pagerank\")\n",
    "    df_test = df_test.fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "\n",
    "    model = pipeline.fit(df_train)\n",
    "    dev_pred = model.transform(df_test)\n",
    "    # get f2 score\n",
    "    score = cv_eval(dev_pred)[0]\n",
    "    print(score)\n",
    "\n",
    "    return dev_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "560256b9-895a-4c47-9758-9ae23ce8eb2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Base MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d1b1af-5b9f-4948-beae-798173129f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62e3fbd-6145-4245-9e0f-1a1a78472662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def timeSeriesSplitCV(df, \n",
    "                      pre_pipeline,\n",
    "                      cv_info, \n",
    "                      sampling='down', \n",
    "                      metric='f2', \n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timeSeriesSplit k-fold cross validation. Params:\n",
    "  1) pre_pipeline: indexers, encoders, and vector assembler\n",
    "  2) cross validation info\n",
    "\n",
    "\n",
    "  note that the scaling+classification pipeline is initialized and fit in this method itself \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    print(f\"processing for fold {i}\")\n",
    "    ppl = pre_pipeline # hopefully avoid getting the recursive depth issue\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Fit the first pipeline on the model to get feature encodings:\n",
    "\n",
    "    print(f\"fitting encoding pipeline for fold {i}\")\n",
    "\n",
    "    train_df_transformed_model = ppl.fit(train_df)\n",
    "\n",
    "    print(f\"encoding train set for fold {i}\")\n",
    "    train_df_transformed= train_df_transformed_model.transform(train_df)\n",
    "    \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_df_transformed = train_df_transformed_model.transform(dev_df)\n",
    "\n",
    "    # Fit the second pipeline on the model to get scaling and classification:\n",
    "\n",
    "    print(f\"getting layer sizes for fold {i}\")\n",
    "    layers = [train_df_transformed.first()['features'].size, 4, 2, 2]\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "    classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=50,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=128,\n",
    "                                                seed=1234)\n",
    "    pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n",
    "    print(f\"fitting encoded train df for fold {i}\") #added logging msg\n",
    "    mlp_model = pipeline_mlp.fit(train_df_transformed.select('features','outcome'))\n",
    "\n",
    "    print(f\"transforming encoded dev df for fold {i}\") #added logging msg \n",
    "    dev_pred = mlp_model.transform(dev_df_transformed.select('features','outcome')) #added select features only\n",
    "\n",
    "    if metric=='f2':\n",
    "      evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"outcome\", \n",
    "        metricName=\"fMeasureByLabel\",\n",
    "        beta=2.0,\n",
    "        metricLabel=1.0\n",
    "      )\n",
    "\n",
    "      score = evaluator.evaluate(dev_pred)\n",
    "\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d42c32e4-0212-4270-a141-0fafeeba5630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"origin_region\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb352b8-ae57-45c0-85b8-f0dd04bcc822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\"]\n",
    "\n",
    "# prior & current flight cols\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME'\n",
    "                ]\n",
    "graph_cols = [\"pagerank_train\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c9a322-c0b7-4378-8631-380bdc0ccd63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(\n",
    "    inputCol='{0}_index'.format(column), \n",
    "    outputCol='{0}_ohe'.format(column)\n",
    "    ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "[encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "[encoders[i].getHandleInvalid() for i in range(len(encoders))] #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cf594c-0096-40ed-9738-710c86794d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fill missing values with 0 for the specified columns\n",
    "df_filled = df_train.fillna({c: 0 for c in numeric_cols if c in df_train.columns})\n",
    "\n",
    "# # Ensure the dataframe is properly formatted and has no issues with dimensions\n",
    "# df_filled = df_filled.repartition(200)  # Adjust the number of partitions as needed\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "timeSeriesSplitCV(\n",
    "    df_filled,\n",
    "    vec_pipeline_full,\n",
    "    cv_cutoffs,\n",
    "    sampling='down',\n",
    "    metric='f2',\n",
    "    verbose=True,\n",
    "    dep_utc_varname='sched_depart_utc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72155a22-3e28-4b45-9341-eb9f13cede96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred_mlp = final_eval(df_train.fillna({c: 0 for c in numeric_cols if c in df_train.columns}), df_test.fillna({c: 0 for c in numeric_cols if c in df_train.columns}), pipeline_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4da7226-31bc-4db2-80f7-324fd8d05bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing values with 0 for the specified columns\n",
    "df_filled = df_train.fillna({c: 0 for c in numeric_cols if c in df_train.columns})\n",
    "\n",
    "# # Ensure the dataframe is properly formatted and has no issues with dimensions\n",
    "# df_filled = df_filled.repartition(200)  # Adjust the number of partitions as needed\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dc22c39-6c29-4976-9bc0-7293a4901256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_filled = df_test.fillna({c: 0 for c in numeric_cols if c in df_test.columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "934b252f-3c2b-458b-9ae3-591163cc30cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_filled = df_filled.select(*numeric_cols, *cat_cols, 'outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ddf463-1b81-430a-9cf4-643f9b9ccbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "encoding_model = vec_pipeline_full.fit(df_train_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fa9612-ce8a-4e8f-9264-6abf7fd3108d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_train_encoded = encoding_model.transform(df_train_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf7bb19-5925-44cb-9ac1-a61d404c36d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "layers = [df_train_encoded.first()['features'].size, 4, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2b8efc-f8ba-40a8-ac40-01b59d59609a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a83b57-3292-40f6-a441-aa923dd80734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_test_encoded = encoding_model.transform(test_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1ad66e-406d-48ee-81c4-d26209bd3dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test_encoded.first()['features'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b60678-e355-4ffe-95ad-9473b0fb7015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[df_test_encoded.first()['features'].size] + [4, 8, 16, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca6f0de-6c99-492d-9b39-6e14e646292d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828c7b57-0d57-4cd3-bfcc-c7c535573c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=50,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=128,\n",
    "                                                seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=[scaler, classifier])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a46dc7b-3152-491f-86ba-a1b2a31ac0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_train_encoded_selected = df_train_encoded.select('features','outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec369e3e-1bcd-447c-9b64-eaeac3445873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f415747d-a4cb-4821-bfad-ec21f86932a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "downsampled_train = downsample(df_train_encoded_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad87c19-c23d-4531-83b6-bc07d09ea915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlp_model = pipeline_mlp.fit(downsampled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd71b828-6305-4c10-8fbf-330e13775bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_test_encoded_selected = df_test_encoded.select('features','outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec1da82-49fa-4b32-97df-91981522d418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_preds = mlp_model.transform(df_test_encoded_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb907bde-3c7b-43c9-83e1-a705927283b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\", \n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "evaluator.evaluate(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c647065-f874-4cd3-9bd4-d8b4d903e802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "486f5eed-932c-42cd-835d-ab2ba86a9b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2544586c-fa98-49b2-b765-47a92fb322e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Upgrading MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a98c5df0-30ff-47e9-b794-efde9465c051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('priorflight_sched_elapsed', f.col('priorflight_sched_elapsed').cast('int')/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e82f0553-5766-4a17-a368-3ec1ee836f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df=df.fillna({'priorflight_sched_elapsed': 60})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb3aa22-0558-41a8-b4c8-0037e4575bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "# df_train.cache()\n",
    "# print(f\"Train data: {df_train.count()} records\")\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt) \\\n",
    "    .filter(f.col(dep_utc_varname) < \"2020-01-01\")\n",
    "# df_test.cache()\n",
    "# print(f\"Test data: {df_test.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9790f639-ed8e-4626-889f-ca027d0659c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "seasonality_cols_cv = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank_train\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]\n",
    "numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1ad11e-272b-4883-b691-53cab183a8e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def timeSeriesSplitCV(df, \n",
    "                      pre_pipeline,\n",
    "                      cv_info, \n",
    "                      hidden_layers,\n",
    "                      sampling='down', \n",
    "                      metric='f2', \n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timeSeriesSplit k-fold cross validation. Params:\n",
    "  1) pre_pipeline: indexers, encoders, and vector assembler\n",
    "  2) cross validation info\n",
    "  3) hidden layer sizes in a list\n",
    "\n",
    "\n",
    "  note that the scaling+classification pipeline is initialized and fit in this method itself \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    print(f\"processing for fold {i}\")\n",
    "    ppl = pre_pipeline # hopefully avoid getting the recursive depth issue\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        \n",
    "    # Fit the first pipeline on the model to get feature encodings:\n",
    "\n",
    "    print(f\"fitting encoding pipeline for fold {i}\")\n",
    "\n",
    "    train_df_transformed_model = ppl.fit(train_df)\n",
    "\n",
    "    print(f\"encoding train set for fold {i}\")\n",
    "    train_df_transformed= train_df_transformed_model.transform(train_df)\n",
    "    \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_df_transformed = train_df_transformed_model.transform(dev_df)\n",
    "\n",
    "    # Fit the second pipeline on the model to get scaling and classification:\n",
    "\n",
    "    print(f\"getting layer sizes for fold {i}\")\n",
    "    layers = [train_df_transformed.first()['features'].size] + hidden_layers + [2]\n",
    "    #input features, hidden layers, classification head\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "    classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=50,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=128,\n",
    "                                                seed=1234)\n",
    "    pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n",
    "    print(f\"fitting encoded train df for fold {i}\")\n",
    "    mlp_model = pipeline_mlp.fit(train_df_transformed.select('features','outcome'))\n",
    "\n",
    "    print(f\"transforming encoded dev df for fold {i}\")\n",
    "    dev_pred = mlp_model.transform(dev_df_transformed.select('features','outcome'))\n",
    "\n",
    "    if metric=='f2':\n",
    "      evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"outcome\", \n",
    "        metricName=\"fMeasureByLabel\",\n",
    "        beta=2.0,\n",
    "        metricLabel=1.0\n",
    "      )\n",
    "\n",
    "      score = evaluator.evaluate(dev_pred)\n",
    "\n",
    "    scores.append(score)\n",
    "    print(f'Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f304bba1-7d00-44ef-a05a-27bcf7973535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs = [\n",
    "    {\"train_min\": \"2014-12-31\", \"train_max\": \"2015-10-09\", \"test_min\": \"2015-10-09\", \"test_max\": \"2016-07-17\"},\n",
    "    {\"train_min\": \"2015-08-14\", \"train_max\": \"2016-05-21\",\"test_min\": \"2016-05-21\", \"test_max\": \"2017-02-27\"},\n",
    "    {\"train_min\": \"2016-03-27\", \"train_max\": \"2017-01-01\",\"test_min\": \"2017-01-01\", \"test_max\": \"2017-10-10\"},\n",
    "    {\"train_min\": \"2016-11-08\", \"train_max\": \"2017-08-14\",\"test_min\": \"2017-08-14\", \"test_max\": \"2018-05-23\"},\n",
    "    {\"train_min\": \"2017-06-22\", \"train_max\": \"2018-03-27\",\"test_min\": \"2018-03-27\", \"test_max\": \"2019-01-01\"}\n",
    "    ]\n",
    "cv_cutoffs = pd.DataFrame(cv_cutoffs)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7403a022-3b46-45af-a013-6bdf500729b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pre-pipeline\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(\n",
    "    inputCol='{0}_index'.format(column), \n",
    "    outputCol='{0}_ohe'.format(column)\n",
    "    ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "[encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "[encoders[i].getHandleInvalid() for i in range(len(encoders))] #sanity check\n",
    "\n",
    "# Fill missing values with 0 for the specified columns\n",
    "df_filled = df_train.fillna({c: 0 for c in numeric_cols_cv if c in df_train.columns})\n",
    "\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "timeSeriesSplitCV(\n",
    "    df_filled,\n",
    "    vec_pipeline_full,\n",
    "    cv_cutoffs,\n",
    "    hidden_layers = [8,4,4],\n",
    "    sampling='down',\n",
    "    metric='f2',\n",
    "    verbose=True,\n",
    "    dep_utc_varname='sched_depart_utc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c30440-e023-4747-8e74-e155dc49c3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "seasonality_cols_cv = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank_train\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]\n",
    "numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac377066-b9c2-4087-a8bf-37bb420a0e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5827e796-dfd7-4f28-9a85-cb31a710763b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Features Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79bd6e27-f967-455d-a837-7868649ef603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b310d91-0a46-48a6-af11-da6defcfee1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfb14f7-67c8-4665-8ff4-85cb5227b212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('priorflight_sched_elapsed', f.col('priorflight_sched_elapsed').cast('int')/60)\n",
    "df=df.fillna({'priorflight_sched_elapsed': 60})\n",
    "\n",
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "# df_train.cache()\n",
    "# print(f\"Train data: {df_train.count()} records\")\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt) \\\n",
    "    .filter(f.col(dep_utc_varname) < \"2020-01-01\")\n",
    "# df_test.cache()\n",
    "# print(f\"Test data: {df_test.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6909d92e-d1e0-4dff-8621-5c85bec4d83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "seasonality_cols_cv = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [\"origin_HourlyDewPointTemperature\", \"origin_HourlyPrecipitation\", \"origin_HourlyWindGustSpeed\", \"origin_HourlyVisibility\", \"origin_HourlyPressureChange\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank_train\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]\n",
    "numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a64dc2d-0522-49e5-b5ce-48ef29f92d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def timeSeriesSplitCV(df, \n",
    "                      pre_pipeline,\n",
    "                      cv_info, \n",
    "                      hidden_layers,\n",
    "                      sampling='down', \n",
    "                      metric='f2', \n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timeSeriesSplit k-fold cross validation. Params:\n",
    "  1) pre_pipeline: indexers, encoders, and vector assembler\n",
    "  2) cross validation info\n",
    "  3) hidden layer sizes in a list\n",
    "\n",
    "\n",
    "  note that the scaling+classification pipeline is initialized and fit in this method itself \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    print(f\"processing for fold {i}\")\n",
    "    ppl = pre_pipeline # hopefully avoid getting the recursive depth issue\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed','priororigin_mean_dep_delay']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed', 'priororigin_mean_dep_delay']})\n",
    "        \n",
    "    # Fit the first pipeline on the model to get feature encodings:\n",
    "\n",
    "    print(f\"fitting encoding pipeline for fold {i}\")\n",
    "\n",
    "    train_df_transformed_model = ppl.fit(train_df)\n",
    "\n",
    "    print(f\"encoding train set for fold {i}\")\n",
    "    train_df_transformed= train_df_transformed_model.transform(train_df)\n",
    "    \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_df_transformed = train_df_transformed_model.transform(dev_df)\n",
    "\n",
    "    # Fit the second pipeline on the model to get scaling and classification:\n",
    "\n",
    "    print(f\"getting layer sizes for fold {i}\")\n",
    "    layers = [train_df_transformed.first()['features'].size] + hidden_layers + [2]\n",
    "    #input features, hidden layers, classification head\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "    classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=50,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=256,\n",
    "                                                seed=1234)\n",
    "    pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n",
    "    print(f\"fitting encoded train df for fold {i}\")\n",
    "    mlp_model = pipeline_mlp.fit(train_df_transformed.select('features','outcome'))\n",
    "\n",
    "    print(f\"transforming encoded dev df for fold {i}\")\n",
    "    dev_pred = mlp_model.transform(dev_df_transformed.select('features','outcome'))\n",
    "\n",
    "    if metric=='f2':\n",
    "      evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"outcome\", \n",
    "        metricName=\"fMeasureByLabel\",\n",
    "        beta=2.0,\n",
    "        metricLabel=1.0\n",
    "      )\n",
    "\n",
    "      score = evaluator.evaluate(dev_pred)\n",
    "\n",
    "    scores.append(score)\n",
    "    print(f'Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7440be5f-9bc9-4ae5-b411-6f3f241644cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5b64c2-3e19-460c-9ba7-c2b826c86881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pre-pipeline\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(\n",
    "    inputCol='{0}_index'.format(column), \n",
    "    outputCol='{0}_ohe'.format(column)\n",
    "    ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "[encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "[encoders[i].getHandleInvalid() for i in range(len(encoders))] #sanity check\n",
    "\n",
    "# Fill missing values with 0 for the specified columns\n",
    "# df_filled = df_train.fillna({c: 0 for c in numeric_cols_cv if c in df_train.columns})\n",
    "\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "timeSeriesSplitCV(\n",
    "    df_filled,\n",
    "    vec_pipeline_full,\n",
    "    cv_cutoffs,\n",
    "    hidden_layers = [4,2],\n",
    "    sampling='down',\n",
    "    metric='f2',\n",
    "    verbose=True,\n",
    "    dep_utc_varname='sched_depart_utc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0dcc114-fbba-48e6-bd0e-4eab659cb05e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "test_filled = df_test.fillna({c: 0 for c in numeric_cols if c in df_test.columns})\n",
    "\n",
    "df_train_filled = df_filled.select(*numeric_cols, *cat_cols, 'outcome')\n",
    "\n",
    "encoding_model = vec_pipeline_full.fit(df_train_filled)\n",
    "print(f\"Fit the encoding model\")\n",
    "\n",
    "\n",
    "df_train_encoded = encoding_model.transform(df_train_filled)\n",
    "print(f\"Encoded train set\")\n",
    "\n",
    "\n",
    "layers = [df_train_encoded.first()['features'].size,4,4,2]\n",
    "print(f\"Got layer sizes: {layers}\")\n",
    "\n",
    "df_test_encoded = encoding_model.transform(test_filled)\n",
    "print(f\"Encoded test set\")\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=50,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=128,\n",
    "                                                seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n",
    "df_train_encoded_selected = df_train_encoded.select('features','outcome')\n",
    "downsampled_train = downsample(df_train_encoded_selected)\n",
    "print(f\"Downsampled train set\")\n",
    "\n",
    "\n",
    "mlp_model = pipeline_mlp.fit(downsampled_train)\n",
    "print(f\"Fit train set\")\n",
    "\n",
    "\n",
    "df_test_encoded_selected = df_test_encoded.select('features','outcome')\n",
    "\n",
    "test_preds = mlp_model.transform(df_test_encoded_selected)\n",
    "print(f\"Transformed test set\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95bad084-f390-4d61-9d17-7c65da5c7b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "seasonality_cols_cv = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [\"origin_HourlyDewPointTemperature\", \"origin_HourlyPrecipitation\", \"origin_HourlyWindGustSpeed\", \"origin_HourlyVisibility\", \"origin_HourlyPressureChange\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank_train\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]\n",
    "numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7731de52-5821-4ba4-bf67-3bee6d6015b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pre-pipeline\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(\n",
    "    inputCol='{0}_index'.format(column), \n",
    "    outputCol='{0}_ohe'.format(column)\n",
    "    ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "[encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "[encoders[i].getHandleInvalid() for i in range(len(encoders))] #sanity check\n",
    "\n",
    "# Fill missing values with 0 for the specified columns\n",
    "# df_filled = df_train.fillna({c: 0 for c in numeric_cols_cv if c in df_train.columns})\n",
    "\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "timeSeriesSplitCV(\n",
    "    df_filled,\n",
    "    vec_pipeline_full,\n",
    "    cv_cutoffs,\n",
    "    hidden_layers = [4,4],\n",
    "    sampling='down',\n",
    "    metric='f2',\n",
    "    verbose=True,\n",
    "    dep_utc_varname='sched_depart_utc'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515efe49-587f-4653-bc21-e31da2242f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28b4cc5c-3f12-479d-9a67-f5e209eb025d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b646e1c1-a29d-4f78-8d3e-c60d15aeafc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4461e37f-c3e4-4af4-9f4c-f0d619adc004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_series_cv_folds(\n",
    "    df,\n",
    "    time_col: str,\n",
    "    k: int=3,\n",
    "    blocking: bool=False,\n",
    "    overlap: float=0.0,\n",
    "    verbose: bool=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Split a time-series PySpark DataFrame into k train/test folds with optional overlap and blocking.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): PySpark DataFrame with a timestamp column.\n",
    "        dep_utc_time_colvarname (str): Name of the timestamp column.\n",
    "        k (int): Number of folds.\n",
    "        blocking (bool): Whether to block the training set to avoid cumulative data.\n",
    "        overlap (float): Fraction of overlap between validation windows (e.g. 0.2 = 20% overlap).\n",
    "        verbose (bool): Whether to print the time splits.\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_df, val_df) tuples.\n",
    "    \"\"\"\n",
    "    # Get time boundaries\n",
    "    min_date = df.select(F.min(time_col)).first()[0]\n",
    "    max_date = df.select(F.max(time_col)).first()[0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "\n",
    "    # Adjust chunk sizing\n",
    "    total_width = k + 1 - overlap * (k - 1)\n",
    "    chunk_size = int(np.ceil(n_days / total_width))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Splitting data into {k} folds with {overlap*100:.0f}% overlap\")\n",
    "        print(f\"Min date: {min_date}, Max date: {max_date}\")\n",
    "        print(f\"{chunk_size:,} days per fold\")\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        # Offset calculation with overlap\n",
    "        train_start_offset = 0 if not blocking else int(i * (1 - overlap) * chunk_size)\n",
    "        train_end_offset = int((i + 1) * chunk_size)\n",
    "        val_start_offset = train_end_offset\n",
    "        val_end_offset = int(val_start_offset + chunk_size)\n",
    "\n",
    "        # Compute actual timestamps\n",
    "        train_start = min_date + timedelta(days=train_start_offset)\n",
    "        train_end = min_date + timedelta(days=train_end_offset)\n",
    "        val_start = min_date + timedelta(days=val_start_offset)\n",
    "        val_end = min_date + timedelta(days=val_end_offset)\n",
    "\n",
    "        if val_start >= max_date:\n",
    "            break\n",
    "        if val_end > max_date:\n",
    "            val_end = max_date + timedelta(days=1)\n",
    "\n",
    "        # Apply filters\n",
    "        train_df = df.filter((F.col(time_col) >= train_start) & (F.col(time_col) < train_end))\n",
    "        val_df = df.filter((F.col(time_col) >= val_start) & (F.col(time_col) < val_end))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold {i + 1}:\")\n",
    "            print(f\"  TRAIN: {train_start.date()}  {train_end.date()} ({train_df.count():,} rows)\")\n",
    "            print(f\"  VAL:   {val_start.date()}  {val_end.date()} ({val_df.count():,} rows)\")\n",
    "            print(\"------------------------------------------------------------\")\n",
    "\n",
    "        folds.append((train_df, val_df))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d203c88d-e948-404a-a643-8f83ddd44eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Testing Time Series CV function\n",
    "folds = time_series_cv_folds(\n",
    "    df_train,\n",
    "    time_col=\"sched_depart_utc\",\n",
    "    k=5,\n",
    "    overlap=.2,\n",
    "    blocking=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db18fbba-c48b-45ad-bbe5-d83ad6df3250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8a83f2-af2b-47c4-9ec3-351155909ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(cv_cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6461a9bb-a191-4922-8ac2-90b3bd130970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def timeSeriesSplitCV(df, \n",
    "                      pre_pipeline,\n",
    "                      hidden_layers,\n",
    "                      stepSize,\n",
    "                      maxIter,\n",
    "                      blockSize,\n",
    "                      cv_info= cv_cutoffs,\n",
    "                      sampling='down', \n",
    "                      metric='f2', \n",
    "                      verbose=True,\n",
    "                      dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timeSeriesSplit k-fold cross validation. Params:\n",
    "  1) pre_pipeline: indexers, encoders, and vector assembler\n",
    "  2) cross validation info\n",
    "  3) hidden layer sizes in a list\n",
    "\n",
    "\n",
    "  note that the scaling+classification pipeline is initialized and fit in this method itself \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    print(f\"processing for fold {i}\")\n",
    "    ppl = pre_pipeline # hopefully avoid getting the recursive depth issue\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed','priororigin_mean_dep_delay']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed', 'priororigin_mean_dep_delay']})\n",
    "        \n",
    "    # Fit the first pipeline on the model to get feature encodings:\n",
    "\n",
    "    print(f\"fitting encoding pipeline for fold {i}\")\n",
    "\n",
    "    train_df_transformed_model = ppl.fit(train_df)\n",
    "\n",
    "    print(f\"encoding train set for fold {i}\")\n",
    "    train_df_transformed= train_df_transformed_model.transform(train_df)\n",
    "    \n",
    "    print(f\"encoding dev set for fold {i}\")\n",
    "    dev_df_transformed = train_df_transformed_model.transform(dev_df)\n",
    "\n",
    "    # Fit the second pipeline on the model to get scaling and classification:\n",
    "\n",
    "    print(f\"getting layer sizes for fold {i}\")\n",
    "    layers = [train_df_transformed.first()['features'].size] + hidden_layers + [2]\n",
    "    #input features, hidden layers, classification head\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features\", \n",
    "        outputCol=\"features_scaled\")\n",
    "    \n",
    "    classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                                featuresCol='features_scaled',\n",
    "                                                maxIter=maxIter,\n",
    "                                                stepSize=stepSize,\n",
    "                                                layers=layers,\n",
    "                                                blockSize=blockSize,\n",
    "                                                seed=1234)\n",
    "    pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n",
    "    print(f\"fitting encoded train df for fold {i}\")\n",
    "    mlp_model = pipeline_mlp.fit(train_df_transformed.select('features','outcome'))\n",
    "\n",
    "    print(f\"transforming encoded dev df for fold {i}\")\n",
    "    dev_pred = mlp_model.transform(dev_df_transformed.select('features','outcome'))\n",
    "\n",
    "    if metric=='f2':\n",
    "      evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"outcome\", \n",
    "        metricName=\"fMeasureByLabel\",\n",
    "        beta=2.0,\n",
    "        metricLabel=1.0\n",
    "      )\n",
    "\n",
    "      score = evaluator.evaluate(dev_pred)\n",
    "\n",
    "    scores.append(score)\n",
    "    print(f'Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e15008-70c0-498d-b568-24be3abc62fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily_full\",\"weekly_full\",\"yearly_full\",\"holidays_full\"]\n",
    "seasonality_cols_cv = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [\"origin_HourlyDewPointTemperature\", \"origin_HourlyPrecipitation\", \"origin_HourlyWindGustSpeed\", \"origin_HourlyVisibility\", \"origin_HourlyPressureChange\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"train\"]\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]\n",
    "numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c504707a-ab9a-4318-840f-c636928a98d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any, Union,Callable\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    RandomForestClassifier,\n",
    "    MultilayerPerceptronClassifier\n",
    ")\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "import mlflow\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a181460-ce05-453f-a0cf-55cd64d8ec60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "    encoders = [OneHotEncoder(\n",
    "        inputCol='{0}_index'.format(column), \n",
    "        outputCol='{0}_ohe'.format(column)\n",
    "        ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "    [encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "    [encoders[i].getHandleInvalid() for i in range(len(encoders))] #sanity check\n",
    "\n",
    "    # Fill missing values with 0 for the specified columns\n",
    "    # df_filled = df_train.fillna({c: 0 for c in numeric_cols_cv if c in df_train.columns})\n",
    "\n",
    "\n",
    "    featuresCreator = VectorAssembler(\n",
    "        inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "        outputCol='features', handleInvalid='skip')\n",
    "\n",
    "    stages = indexers + encoders\n",
    "\n",
    "    pre_pipeline = Pipeline(stages= stages + [featuresCreator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f833006-a043-4399-b7f2-06b0a19ec6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_tuner(\n",
    "    model_name: str,\n",
    "    model_params: Dict[str, Any],\n",
    "    mlflow_run_name: str = \"/Users/m.bakr@berkeley.edu/flight_delay_tuning\",\n",
    "    metric: str = \"F2\",\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Union[float, str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Universal tuning function for PySpark classification models using time-series cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of ['logreg', 'rf', 'mlp', 'xgb']\n",
    "        model_params (Dict[str, Any]): Parameters to apply to the model\n",
    "        folds (List of (train_df, val_df)): Time-aware CV folds\n",
    "        mlflow_run_name (str): Optional MLflow parent run name\n",
    "        verbose (bool): Whether to log outputs during tuning\n",
    "\n",
    "    Returns:\n",
    "        Dict with best average F2 or pr score, model name, and parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Model factory\n",
    "    model_factory = {\n",
    "        \"logreg\": LogisticRegression,\n",
    "        \"rf\": RandomForestClassifier,\n",
    "        \"mlp\": MultilayerPerceptronClassifier,\n",
    "        \"xgb\": SparkXGBClassifier\n",
    "    }\n",
    "\n",
    "    assert model_name in model_factory, f\"Unsupported model: {model_name}\"\n",
    "\n",
    "    ModelClass = model_factory[model_name]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "\n",
    "        indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "        encoders = [OneHotEncoder(\n",
    "        inputCol='{0}_index'.format(column), \n",
    "        outputCol='{0}_ohe'.format(column)\n",
    "        ) for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "        [encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "\n",
    "        featuresCreator = VectorAssembler(\n",
    "            inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols_cv,\n",
    "            outputCol='features', handleInvalid='skip')\n",
    "\n",
    "        stages = indexers + encoders\n",
    "\n",
    "        pre_pipeline = Pipeline(stages= stages + [featuresCreator])\n",
    "        scores = timeSeriesSplitCV(df=df_train,\n",
    "                                   pre_pipeline= pre_pipeline, **model_params)\n",
    "        \n",
    "\n",
    "        avg_score = float(np.mean(scores))\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_params(model_params)\n",
    "        mlflow.log_metric(\"avg_{metric}_score\", avg_score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\" Average {metric} Score: {avg_score:.4f} | Model: {model_name}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"params\": model_params,\n",
    "        \"avg_f2_score\": avg_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4080ca35-1cad-40e7-a9a9-798c0a6c0d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ad7f48-7fda-4751-9922-a98a404ba70f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d9675e1-3d38-4cbc-95f0-9104a92221c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_hyperopt_objective(\n",
    "    model_name: str,\n",
    "    param_space_converter: Callable[[Dict[str, Any]], Dict[str, Any]],\n",
    "    mlflow_experiment_name: str = \"Hyperopt_Universal_Tuning\",\n",
    "    verbose: bool = True\n",
    ") -> Callable[[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Creates a Hyperopt-compatible objective function for any PySpark classifier.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of 'logreg', 'rf', 'mlp', 'xgb'.\n",
    "        folds (List of (train_df, val_df)): Time-series CV folds.\n",
    "        param_space_converter (Callable): Converts Hyperopt sample into model params.\n",
    "        mlflow_experiment_name (str): MLflow experiment name.\n",
    "        verbose (bool): Logging toggle.\n",
    "\n",
    "    Returns:\n",
    "        Callable that can be passed as fn to hyperopt.fmin()\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(sampled_params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Convert sampled param space to Spark-friendly params\n",
    "        model_params = param_space_converter(sampled_params)\n",
    "\n",
    "        result = model_tuner(\n",
    "            model_name=model_name,\n",
    "            model_params=model_params,\n",
    "            mlflow_run_name=f\"hyperopt_{model_name}\",\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": -result[\"avg_f2_score\"],  # Minimize negative F2\n",
    "            \"status\": STATUS_OK,\n",
    "            \"params\": result[\"params\"]\n",
    "        }\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1bf2bb2-1307-4c8c-a1d6-4da60e2d1afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_space = {\n",
    "    \"hidden_layers\": hp.choice(\"hidden_layers\", [[64, 32], [32, 8, 4], [128, 32]]),\n",
    "    \"stepSize\": hp.uniform(\"stepSize\", 0.01, 0.3),\n",
    "    \"maxIter\": hp.choice(\"maxIter\", [100, 200]),\n",
    "    \"blockSize\": hp.choice(\"blockSize\", [64, 128])\n",
    "}\n",
    "\n",
    "def mlp_param_mapper(sampled):\n",
    "    return {\n",
    "        \"hidden_layers\": list(sampled[\"hidden_layers\"]),\n",
    "        \"stepSize\": sampled[\"stepSize\"],\n",
    "        \"maxIter\": sampled[\"maxIter\"],\n",
    "        \"blockSize\": sampled[\"blockSize\"]\n",
    "    }\n",
    "\n",
    "mlp_obj = make_hyperopt_objective(\n",
    "    model_name=\"mlp\",\n",
    "    param_space_converter=mlp_param_mapper,\n",
    "    mlflow_experiment_name=\"MLP_Hyperopt\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_mlp = fmin(\n",
    "    fn=mlp_obj,\n",
    "    space=mlp_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Best MLP params:\", best_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e0c45f-292e-4f15-9c42-c7c948d1c9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Must show training and performance scores, including training curves by epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24e2222-d2b0-490d-9b75-bd6e26d4a47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c377876e-33e7-4626-952e-08c2df52f3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9533167b-3f2d-485e-adc5-b4f7c548db5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Downsampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9cd204-dcdf-4981-b1db-f7fa50d7d6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def downsample_time_buckets(train_df, time_col='sched_depart_utc', bucket='week', verbose=False):\n",
    "    '''\n",
    "    Downsample majority class within time buckets\n",
    "\n",
    "    Inputs:\n",
    "      time_col: column name with datetime/timestamp\n",
    "      bucket: 'day', 'week', 'month' - time bucket for stratification\n",
    "    '''\n",
    "    # Add time bucket column\n",
    "    if bucket == 'day':\n",
    "        train_df = train_df.withColumn('time_bucket', F.to_date(time_col))\n",
    "    elif bucket == 'week':\n",
    "        train_df = train_df.withColumn('time_bucket', F.date_trunc('week', time_col))\n",
    "    elif bucket == 'month':\n",
    "        train_df = train_df.withColumn('time_bucket', F.date_trunc('month', time_col))\n",
    "    else:\n",
    "        raise ValueError(\"bucket must be 'day', 'week', or 'month'\")\n",
    "    \n",
    "    # Compute counts per bucket and class\n",
    "    counts = train_df.groupBy('time_bucket', 'outcome').count()\n",
    "    \n",
    "    # Collect counts to driver for fraction calculation (if dataset is large, consider approximate methods)\n",
    "    counts_pd = counts.toPandas()\n",
    "    \n",
    "    # Calculate sampling fractions per bucket\n",
    "    fractions = {}\n",
    "    for bucket_val in counts_pd['time_bucket'].unique():\n",
    "        bucket_data = counts_pd[counts_pd['time_bucket'] == bucket_val]\n",
    "        delay_count = bucket_data.loc[bucket_data['outcome'] == 1, 'count'].values\n",
    "        non_delay_count = bucket_data.loc[bucket_data['outcome'] == 0, 'count'].values\n",
    "        \n",
    "        if len(delay_count) == 0 or len(non_delay_count) == 0:\n",
    "            # If only one class present, keep all\n",
    "            fractions[(bucket_val, 0)] = 1.0\n",
    "            fractions[(bucket_val, 1)] = 1.0\n",
    "        else:\n",
    "            delay_count = delay_count[0]\n",
    "            non_delay_count = non_delay_count[0]\n",
    "            frac = delay_count / non_delay_count if non_delay_count > delay_count else 1.0\n",
    "            fractions[(bucket_val, 0)] = frac\n",
    "            fractions[(bucket_val, 1)] = 1.0\n",
    "    \n",
    "    # Downsample majority class per bucket using sampleBy\n",
    "    sampled_df = train_df.sampleBy(\n",
    "        col='outcome',\n",
    "        fractions={0: 1.0, 1: 1.0},  # We'll sample manually below\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Because sampleBy doesn't support stratification on two columns, do per bucket filtering:\n",
    "    downsampled_parts = []\n",
    "    for bucket_val in fractions.keys():\n",
    "        class_label = bucket_val[1]\n",
    "        frac = fractions[bucket_val]\n",
    "        subset = train_df.filter(\n",
    "            (F.col('time_bucket') == bucket_val[0]) & (F.col('outcome') == class_label)\n",
    "        )\n",
    "        if class_label == 0 and frac < 1.0:\n",
    "            subset = subset.sample(withReplacement=False, fraction=frac, seed=42)\n",
    "        downsampled_parts.append(subset)\n",
    "    \n",
    "    downsampled_df = downsampled_parts[0]\n",
    "    for part in downsampled_parts[1:]:\n",
    "        downsampled_df = downsampled_df.union(part)\n",
    "    \n",
    "    downsampled_df = downsampled_df.drop('time_bucket')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Downsampled training set size: {downsampled_df.count()}\")\n",
    "    \n",
    "    return downsampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb87f5d9-47d9-45ae-8a7e-398fe1e9a45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train0.withColumn('time_bucket', F.date_trunc('week', 'sched_depart_utc')).select('sched_depart_utc','time_bucket'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f314a4c0-0142-4a19-b889-f702d2b0d8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train0= df_train.filter(F.col(\"sched_depart_utc\")>=\"2014-12-31\").filter(F.col(\"sched_depart_utc\")<=\"2015-10-09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44c8fc9-ada4-4b74-afb4-84ff184c5390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "downsample1= downsample(train0)\n",
    "downsample2 = downsample_time_buckets(train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d73131b5-cd4a-49b3-aaef-dd955af29c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f892795a-dafe-47a6-9c65-822e2437d0f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d47ea5-ea47-4d96-a7a3-95bf64c17be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65eed5c4-6ded-48ba-8c3b-95e970a5cc17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION 1 \n",
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"origin_type\",\n",
    "    \"origin_region\"\n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol='{0}_index'.format(\n",
    "    column), outputCol='{0}_ohe'.format(\n",
    "    column, handleInvalid='keep')) for column in cat_cols]\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "scaler = MinMaxScaler(inputCol='features',outputCol='features_scaled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399d5b2b-50e0-4bc6-a4c7-e05ded95b313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194eb009-15a5-44a1-9fac-ef1c67f23da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION 2\n",
    "pipeline0 = Pipeline(stages=indexers + encoders + [featuresCreator])\n",
    "model0 = pipeline0.fit(df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"train\",\"pagerank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6614bed3-712b-406d-bda4-09b0ff35c672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp = model0.transform(df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"test\",\"pagerank\")).limit(1).select('features').toPandas()\n",
    "L = tmp.iloc[0,0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39f1ba5-3b25-4480-8dfd-efa046ceb46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "L # 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861fda78-2000-41ac-9273-6751e5bf5eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION 3\n",
    "layers = [47, 4, 2, 2]\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                            featuresCol='features_scaled',\n",
    "                                            maxIter=50,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=indexers + encoders + [featuresCreator, scaler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eb8be9-9f45-40cf-9828-86bf61cc9006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"train_1\",\"pagerank\")\n",
    "df1=df_train.filter(f.col('YEAR')==2015).limit(100)\n",
    "df2=df_train.filter(f.col('YEAR')==2016).limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c5a1f0-ca54-4fc8-8c5d-8d35137d00f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ALTERNATE\n",
    "# SOLUTION 1 \n",
    "stages=[]\n",
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc'\n",
    "    ]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol='{0}_index'.format(\n",
    "    column), outputCol='{0}_ohe'.format(\n",
    "    column, handleInvalid='keep')) for column in cat_cols]\n",
    "\n",
    "[encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n",
    "\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "pre_pipeline = Pipeline(stages=indexers + encoders + [featuresCreator])\n",
    "pre_model = pre_pipeline.fit(df1)\n",
    "dim_size= pre_model.transform(df1).limit(1).select('features').get()[0][0].size\n",
    "\n",
    "scaler = MinMaxScaler(inputCol='features',outputCol='features_scaled')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layers = [dim_size, 4, 2, 2]\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                            featuresCol='features_scaled',\n",
    "                                            maxIter=50,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=[scaler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef8503d-44c8-497e-84c6-6e57114aad8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pre_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382fb7b9-bc87-4fb3-ba99-29bac0ac2aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pre_model.stages[1].getHandleInvalid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8662ea7b-285c-4789-b7ee-46bf1a02404a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp.loc[0,0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893c0926-140c-4a04-987f-ce0e56f7330a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp.limit(1).first()['features'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e94c15-e061-4d9b-a219-218774b57fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_tmp= pre_model.transform(df2) #ERROR HAPPENS HERE?\n",
    "L2 = df2_tmp.limit(1).first()['features'].size\n",
    "print('df2 size: {L2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b959d12-2b52-4d84-8853-29225fc96416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(\n",
    "        inputCol=\"OP_UNIQUE_CARRIER\", \n",
    "        outputCol= \"OP_UNIQUE_CARRIER_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "        inputCol=\"OP_UNIQUE_CARRIER_index\", \n",
    "        outputCol=\"OP_UNIQUE_CARRIER_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "047a5ae4-594f-4fac-a078-158a59f442e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab8b69e-10d4-49ea-bb00-0fb26ce369cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(\n",
    "        inputCol=\"OP_UNIQUE_CARRIER\", \n",
    "        outputCol= \"OP_UNIQUE_CARRIER_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "        inputCol=\"OP_UNIQUE_CARRIER_index\", \n",
    "        outputCol=\"OP_UNIQUE_CARRIER_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "df1_indexed_0_model = indexer.fit(df1)\n",
    "df1_indexed_0 = df1_indexed_0_model.transform(df1)\n",
    "df1_encoded_0_model = encoder.fit(df1_indexed_0)\n",
    "df1_encoded_0 = df1_encoded_0_model.transform(df1_indexed_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc3f93a-14fd-491c-8880-76058efc214b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1_encoded_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e52dd4-b7c3-41f8-91ba-54ad589873d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_indexed_0.printSchema()\n",
    "df1_encoded_0.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130fa323-41ca-4230-b66b-6ceb46c78e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_indexed_0.printSchema()\n",
    "df1_encoded_0.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d6cfaa-f3d2-4df6-8250-5d6e44a641f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.filter(f.col('OP_UNIQUE_CARRIER').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7cb5987-a613-4b22-af41-e05e29fc28f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1_encoded_0.select('OP_UNIQUE_CARRIER_index').limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618e34f6-b176-4fd5-ba83-4b310b56f565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1_encoded_0.select('OP_UNIQUE_CARRIER_vec').limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f91c22-3df0-4cfb-ae45-d5a7bc640efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1_encoded_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28150cbd-0d9e-426c-be81-797baa7bbb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol='{0}_index'.format(\n",
    "    column), outputCol='{0}_ohe'.format(\n",
    "    column, handleInvalid='keep')) for column in cat_cols]\n",
    "\n",
    "prepre_pipeline = Pipeline(stages = indexers + encoders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95dcf04f-cf2f-451d-9990-2edaa05bbc94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_prepre_model = prepre_pipeline.fit(df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73c79cb-c022-4bea-9990-f8f3d19c39b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_prepre_transformed = df1_prepre_model.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0659331-5a84-4ac5-b8a9-af45c62bcac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_prepre_transformed.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930cffcd-9e86-4faf-a14a-9e605dc7b895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.filter(f.col('priorflight_isdelayed_calc').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c356c55a-bfea-4be6-ab6b-f8756a2f89e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1_prepre_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366dec48-6d98-4f03-bf96-bcea4f1ba7ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_prepre_transformed = df1_prepre_model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c5dfa1-1cd2-45fa-8277-e28ab0a68b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_prepre_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c9c165-bc1a-4ee6-b71f-ab0a4d00a95f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df2_prepre_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f5bab5-703b-4998-8bfe-b7bb92e1bed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "vec_pipeline = Pipeline(stages=[featuresCreator])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47f1ea2-a4f1-4efb-a13c-d2a53811b160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_pre_transformed_model = vec_pipeline.fit(df1_prepre_transformed)\n",
    "df1_pre_transformed = df1_pre_transformed_model.transform(df1_prepre_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893c763d-284f-428a-91dd-aad6a8d178b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_pre_transformed.printSchema()\n",
    "display(df1_pre_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbfec6b-6b7a-4517-932b-347f1c881d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_pre_transformed = df1_pre_transformed_model.transform(df2_prepre_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0ff478-e013-4556-82b3-95c90e37b8d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df2_pre_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e65bf4-8ffa-4b26-ae2c-96ba4c397551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_pre_transformed.first()['features'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a6c5c2-140f-4f78-9862-4b56f153ac2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "layers = [df2_pre_transformed.first()['features'].size, 4, 2, 2]\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                            featuresCol='features_scaled',\n",
    "                                            maxIter=50,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=[scaler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b958bc-23d4-44d4-a86f-ad044dc53bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mod_model = pipeline_mlp.fit(df1_pre_transformed)\n",
    "df2preds = mod_model.transform(df2_pre_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4d3acd-f5f3-45ac-8f9b-5a7abef252eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df2preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7aa436e-f6bd-4b8a-a503-13efdd41ca5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9703113-da00-47eb-aa9e-896e5609e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0e3cf8-2dd1-440a-8541-92be98b471a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df1_vec_transformed_model = vec_pipeline_full.fit(df1)\n",
    "df2_vec_transformed = df1_vec_transformed_model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a3142d-2bb7-4220-9db9-9e37fc9a03c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_vec_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75978043-a4fd-4931-a4f5-edccf1853808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df2_vec_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde467b8-6781-4ab1-a646-691d3458fd75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_vec_transformed.first()['features'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d632b8-76e2-4a06-8b44-5a2f427173a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_vec_transformed = df1_vec_transformed_model.transform(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54ade39-599a-4cc3-b74a-746ee8cb6376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1_vec_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54afbcad-f950-464a-aa5c-74c146bed3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "layers = [df1_vec_transformed.first()['features'].size, 4, 2, 2]\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                            featuresCol='features_scaled',\n",
    "                                            maxIter=50,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=[scaler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67bc032-a5fd-4cfe-b42a-d3f6b720e0f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vec_mod_model = pipeline_mlp.fit(df1_vec_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70515e78-8be3-40a1-aeaa-6c6e38f4f26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653c182f-7729-4c2a-8ea8-183af459744f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_vec_preds = vec_mod_model.transform(df2_vec_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ad23bc-888a-4193-9b59-69e06b3fe01d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\", \n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "evaluator.evaluate(df2_vec_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f79acf7-075d-49b7-9575-da7cab057197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator.getPredictionCol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b849496-b0a9-4bbe-8003-bd80552aa255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator.getProbabilityCol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44be1a24-3624-4f29-86af-a5660f4a6316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2_vec_preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c86a48-9b2f-4f23-ae65-55e498a023f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df2_vec_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf0c8d6-8c60-43fc-b2cf-a0f4ad595462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5bde83d-79b5-41c3-9e19-0c2d8a21d29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MOre stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fda0a93-201c-4725-a5b3-ddda66a4e5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol='{0}_index'.format(\n",
    "    column), outputCol='{0}_ohe'.format(\n",
    "    column, handleInvalid='keep')) for column in cat_cols]\n",
    "[encoders[i].setHandleInvalid('keep') for i in range(len(encoders))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fda74be-b097-4e73-b22c-df2e84a4ea79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[encoders[i].getHandleInvalid() for i in range(len(encoders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb723b22-a713-42fb-99f7-08087b699ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "stages = indexers + encoders\n",
    "vec_pipeline_full = Pipeline(stages= stages + [featuresCreator])\n",
    "\n",
    "df_train_vec_transformed_model = vec_pipeline_full.fit(df_train)\n",
    "#df2_vec_transformed = df1_vec_transformed_model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7619806-3d2f-41bc-9a11-49fd0f102cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "specs= df_train_vec_transformed_model.transform(df_train.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645c0155-8e2f-4d7f-81da-d310b886530b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece37938-50bc-4fd3-b229-9134c73b9da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "layers = [specs.first()['features'].size, 4, 2, 2]\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92cbfc35-a2ca-4f40-9830-dcdb1701003c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "241a1c2e-7f13-4def-ae5b-a1eba9ed6ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_vec_transformed = df_train_vec_transformed_model.transform(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ca3a53-51a2-4ece-adf7-bb0e59c68a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_train_vec_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b2ce36-c891-4e15-a274-7438b5ac294e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ec214f-669b-4d6e-b89f-4eae38d66ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                            featuresCol='features_scaled',\n",
    "                                            maxIter=50,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=[scaler, classifier])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f742260-6b49-41bc-9a61-348c24b499c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_vec_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bfa27b4-cd6b-4d17-8782-35493d198f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_vec_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb15d99-cafa-4da6-a0e6-df90e0fcb52c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_vec_mod_model = pipeline_mlp.fit(df_train_vec_transformed.select('features','outcome'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e7a1846-f65c-4dc2-8b3b-4c046a1601ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## run the mlp fr maybe "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.16-sg-5y-modeling-MLP",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
