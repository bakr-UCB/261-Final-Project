{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e4de20-2fac-4c65-aa47-724b68203cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d62c3-ed6a-47d0-bb21-3c0faee08bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "spark.sparkContext.setCheckpointDir(f\"{team_BASE_DIR}/modeling_checkpoints\")\n",
    "display(dbutils.fs.ls(f\"{team_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125fcf5c-0f72-4cd6-8b92-290c0416578f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: replace with Shruti's imputed weather df once available\n",
    "\n",
    "period = \"1y\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather_{period}_v1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a6e95f-1abb-4191-ac98-885a204291b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c44b970-b0a4-4b2c-9a0b-f3d460c81cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bbe8b4-024f-4d67-b31e-e9cd14db4056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define train/test split and cross-validation folds\n",
    "min_test_dt = \"2019-10-01\" # reserve last 3 months of year for held out test set\n",
    "k = 3\n",
    "\n",
    "# define var names\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a503b7-701e-4d2b-bdeb-012578b1e7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour variable (needed for seasonality)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "becf0366-4416-49ff-88c7-f96b50688500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define train and test data\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "# df_train.checkpoint()\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt)\n",
    "df_train.cache()\n",
    "# df_train.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e079e4-266b-4e77-9ba8-154a1f0addfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits(df, k=3, blocking=False, dep_utc_varname=\"dep_datetime\", verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation\n",
    "    '''\n",
    "    n = df.count()\n",
    "    df = df.withColumn(\"row_id\", f.row_number()\n",
    "            .over(Window.partitionBy().orderBy(dep_utc_varname)))\n",
    "    chunk_size = np.floor(n/(k+1))\n",
    "\n",
    "    idx = np.arange(0,)\n",
    "    idx = np.arange(0,n,chunk_size)\n",
    "    idx[-1] = n-1\n",
    "    idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print('')\n",
    "        print(f'Number of validation datapoints for each fold is {chunk_size:,}')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    bin_edges = df.filter(f.col(\"row_id\").isin(idx)).select(\"row_id\",dep_utc_varname).toPandas()\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = bin_edges[dep_utc_varname][0]\n",
    "        else:\n",
    "            t_min_train = bin_edges[dep_utc_varname][i]\n",
    "        # define maximum training time\n",
    "        t_max_train = bin_edges[dep_utc_varname][i+1]\n",
    "        # define minimum test time\n",
    "        t_min_test = bin_edges[dep_utc_varname][i+1]\n",
    "        # define maximum test_time\n",
    "        t_max_test = bin_edges[dep_utc_varname][i+2]\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get cross-validation split times\n",
    "cv_cutoffs = get_cv_time_limits(df_train.select(dep_utc_varname), k=3, blocking=True, \n",
    "    dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaba55bd-601f-4def-a082-b259656574c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train seasonality models for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831adc2c-61e3-4984-a797-764958c5d2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# informed by: https://www.databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html\n",
    "\n",
    "def forecast_delay(history_pd: pd.DataFrame) -> pd.DataFrame: \n",
    "    \n",
    "    model = Prophet(\n",
    "        interval_width=0.9,\n",
    "        growth='linear',\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        # holidays=us_holidays,\n",
    "        # seasonality_mode='multiplicative'\n",
    "    )\n",
    "\n",
    "    # TODO: add dummy row to history_pd so that timestamps end up as round hours\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(history_pd)\n",
    "    \n",
    "    # configure predictions\n",
    "    future_pd = model.make_future_dataframe(\n",
    "        periods=24*7, \n",
    "        freq='h',\n",
    "        include_history=False\n",
    "    )\n",
    "    \n",
    "    # make predictions\n",
    "    results_pd = model.predict(future_pd)\n",
    "\n",
    "    # ref date and dow\n",
    "    ref_date = history_pd.ds.iloc[0].date()\n",
    "    ref_dow = history_pd.DAY_OF_WEEK[0]\n",
    "\n",
    "    def get_dow(x,ref_date,dow):\n",
    "        d_days = (x.date() - ref_date).days + dow\n",
    "        d_days = d_days%7\n",
    "        if d_days == 0:\n",
    "            d_days = 7\n",
    "        return d_days\n",
    "\n",
    "    # dateshift\n",
    "    results_pd['dow'] = results_pd.ds.apply(lambda x: get_dow(x,ref_date,ref_dow))\n",
    "\n",
    "    # hour\n",
    "    results_pd['hour'] = results_pd.ds.apply(lambda x: x.hour)\n",
    "\n",
    "    # apply origin\n",
    "    results_pd['ORIGIN'] = history_pd.ORIGIN.iloc[0]\n",
    "\n",
    "    # # get seasonality\n",
    "    # results_pd['seasonality'] = results_pd['weekly'] + results_pd['daily']\n",
    "        \n",
    "    # return predictions\n",
    "    return results_pd[['dow','hour','weekly','daily','ORIGIN']]\n",
    "\n",
    "schema = StructType([StructField('dow', LongType(), True),\n",
    "                     StructField('hour', LongType(), True),\n",
    "                     StructField('weekly', DoubleType(), True),\n",
    "                     StructField('daily', DoubleType(), True),\n",
    "                     StructField('ORIGIN', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3583341c-b65e-4213-bd4e-005b21d1ff44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality( df, t_min, t_max, \n",
    "    dep_utc_varname=dep_utc_varname, delay_varname=\"DEP_DELAY\", \n",
    "    forecast_fn=forecast_delay, schema=schema ):\n",
    "\n",
    "    return (\n",
    "        df.filter((df[dep_utc_varname] >= t_min) & \\\n",
    "            (df[dep_utc_varname] < t_max))\n",
    "        .withColumnRenamed(delay_varname, \"y\")\n",
    "        .withColumnRenamed(dep_utc_varname, \"ds\")\n",
    "        .groupBy('ORIGIN')\n",
    "            .applyInPandas(forecast_fn, schema=schema)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53913682-beee-47d2-9f66-20151ae8913b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !!! SEASONALITY ALREADY RUN !!!\n",
    "\n",
    "# # train seasonality model for each cross validation split\n",
    "# for i in range(k):\n",
    "#     # train seasonality model for this cross validation split\n",
    "#     model = get_seasonality(df_train, cv_cutoffs[\"train_min\"][i], cv_cutoffs[\"train_max\"][i])\n",
    "#     # write out\n",
    "#     fn_out = f\"seasonality_model_{period}_cv{i}of{k}.parquet\"\n",
    "#     model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")\n",
    "\n",
    "# # train seasonality model for full training data\n",
    "# model = get_seasonality(df_train, datetime(1970,1,1), min_test_dt)\n",
    "# # write out\n",
    "# fn_out = f\"seasonality_model_{period}_train.parquet\"\n",
    "# model.write.parquet(f\"{team_BASE_DIR}/interim/{fn_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "971c3177-ebd6-49e5-bc84-11db084e0130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{team_BASE_DIR}/interim\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5bdccd-7230-44de-9d2e-c2fba8819855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modeling\n",
    "\n",
    "TODO: Separate out feature engineering process and save out feat eng. cv split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907f6dba-f229-4434-aa95-91656c9d5a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{team_BASE_DIR}/interim/join_checkpoints\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90782aa-2b8a-4782-9f5d-cccf24eb2a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_weather_cleaned_test.parquet\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_weather_cleaned_combo.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3272d007-df03-4a93-a54b-c57d69818310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d774210-88d3-466d-b91f-0c7dd4e1d21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get data types of each column in the DataFrame\n",
    "column_data_types = df.dtypes\n",
    "\n",
    "# Display the data types\n",
    "for column, data_type in column_data_types:\n",
    "    print(f\"Column: {column}, Data Type: {data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c4ef7d-8964-4aca-a618-768a5ef36855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "weather_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56b6ec6-5e1a-4088-a20d-c111f0ce502e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seasonality_cols = [\"daily\",\"weekly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c9502a5-d60b-4070-b378-9a1d2fa7b807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour variable (needed for seasonality)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a06d299-5f78-4d13-bcbb-126c19f03787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select relevant vars\n",
    "tmp = df.select(*weather_cols,\"DAY_OF_WEEK\",\"ORIGIN\",\"dep_hour_utc\",\"outcome\",dep_utc_varname,\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\")\n",
    "\n",
    "for column in weather_cols:\n",
    "    tmp = tmp.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "# # REMOVE ME !!! JUST FOR TESTING\n",
    "# tmp = tmp.filter(f.col(\"ORIGIN\") == \"BOS\")\n",
    "\n",
    "# simplistic cleaning: TODO: use Shruti's output instead\n",
    "tmp = tmp.fillna({col:0 for col in weather_cols})\n",
    "\n",
    "tmp.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "147c6093-8aee-4b11-aa02-a6294c2e1f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56dc4779-a19c-4236-86aa-88cb83df66c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define train and test data\n",
    "df_train = tmp.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "# df_train.checkpoint()\n",
    "df_test = tmp.filter(f.col(dep_utc_varname) >= min_test_dt)\n",
    "df_train.cache()\n",
    "# df_train.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e31124-5e8a-492b-a8d1-27b7311efdf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3cd118-b967-4e04-a17e-e45022a0b13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of categorical columns to encode\n",
    "categorical_columns = [\"OP_UNIQUE_CARRIER\", \"ORIGIN_ICAO\", \"DEST_ICAO\"]\n",
    "\n",
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# Index and encode each categorical column\n",
    "for column in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\",handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_vec\",handleInvalid=\"keep\")\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "features = [col + \"_vec\" for col in categorical_columns]+[*weather_cols,*seasonality_cols]\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid='keep') # !!! TODO: fill all nulls\n",
    "scaler = StandardScaler(inputCol=\"features\", \\\n",
    "    outputCol=\"features_scaled\",withMean=True)\n",
    "lr = LogisticRegression(featuresCol='features_scaled', \\\n",
    "    labelCol='outcome',maxIter=50)\n",
    "pipeline = Pipeline(stages=stages+[assembler,scaler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d45e1a-e805-40fc-ad7d-d122f59f8366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seasonality_data(df, fold, k):\n",
    "    if fold == 'full':\n",
    "        fn_model = f\"seasonality_model_{period}_train.parquet\"\n",
    "    else:\n",
    "        fn_model = f\"seasonality_model_{period}_cv{fold}of{k}.parquet\"\n",
    "    model = spark.read.parquet(f\"{team_BASE_DIR}/interim/{fn_model}\")\n",
    "\n",
    "    joined_df = df.join(model, \n",
    "                     (df[\"ORIGIN\"] == model[\"ORIGIN\"]) & \n",
    "                     (df[\"DAY_OF_WEEK\"] == model[\"dow\"]) & \n",
    "                     (df[\"dep_hour_utc\"] == model[\"hour\"]),\n",
    "                     how=\"left\").drop(model[\"ORIGIN\"])\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "# display(get_seasonality_data(df_train.limit(10), 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2d4b17-f16a-44c5-919b-a49112ff8fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def upsample(train_df,verbose=False):\n",
    "  '''Upsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = non_delay_count / delay_count\n",
    "\n",
    "  train_delay = train_df.filter(f.col('outcome') == 0)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 1).sample(withReplacement=True, fraction=keep_percent,seed=42)\n",
    "  train_upsampled = train_delay.union(train_non_delay)\n",
    "  return train_upsampled\n",
    "\n",
    "\n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n",
    "\n",
    "def cv_eval(preds):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  return F2, pr\n",
    "\n",
    "def timeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timSeriesSplit k-fold cross validation \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "      train_df = train_df.cache()\n",
    "    # elif sampling=='weights':\n",
    "    #   train_df = add_class_weights(train_df).cache()\n",
    "      \n",
    "    #print info on train and dev set for this fold\n",
    "    if verbose:\n",
    "      print('    TRAIN set for fold {} goes from {} to {}, count is {:,} flights ({})'.format((i+1), \n",
    "                                                                                      train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      train_df.count(),\n",
    "                                                                                      sampling + '-sampled' if sampling else 'no sampling'))\n",
    "      print('    DEV set for fold {} goes from {} to {}, count is {:,} flights'.format((i+1), \n",
    "                                                                                      dev_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "                                                                                      dev_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "                                                                                      dev_df.count()))\n",
    "      \n",
    "    # TODO: remove once feat engineering applied outside\n",
    "    train_df = get_seasonality_data(train_df, i, k)\n",
    "    train_df = train_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "    dev_df = get_seasonality_data(dev_df, i, k)\n",
    "    dev_df = dev_df.fillna({col:0 for col in ['daily','weekly']})\n",
    "\n",
    "    # print(train_df.dtypes)\n",
    "    # print(dev_df.dtypes)\n",
    "        \n",
    "    # Fit params on the model\n",
    "    model = pipeline.fit(train_df)\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "  # # Train on full df\n",
    "  # print('Training on full train dataset, and validating on dev dataset with best parameters from CV:')\n",
    "  # print(best_parameters)\n",
    "    \n",
    "  # if verbose:\n",
    "  #   print('    TRAIN set for best parameter fitted model goes from {} to {}, count is {:,} flights ({})'.format(train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.count(),\n",
    "  #                                                                                                    sampling + '-sampled' if sampling else 'no sampling'))\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd38f66-1f12-49fe-a619-227a69cecd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname=dep_utc_varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7e5aaf-398f-43c4-9b80-a4c7858bf91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "df_train = downsample(df_train).cache()\n",
    "df_train = get_seasonality_data(df_train, 'full', k).cache()\n",
    "df_test = get_seasonality_data(df_test, 'full', k).cache()\n",
    "model = pipeline.fit(df_train)\n",
    "dev_pred = model.transform(df_test)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred)[0]\n",
    "print(score)ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5dc4e9-6ace-4bc3-b9a5-268c391649ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sandbox: Figures for Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0b06a6-39f2-46d0-96f2-47f32e122cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fn_model = f\"seasonality_model_{period}_train.parquet\"\n",
    "model = spark.read.parquet(f\"{team_BASE_DIR}/interim/{fn_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a41c5d-6950-49e0-9560-29cf43a0b6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc3dc4a-0292-4c21-a7ac-ff6ca6e97b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airport = 'BOS'\n",
    "seas = model.filter(col('ORIGIN') == airport).toPandas()\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(10,5))\n",
    "seas['x'] = seas['dow'] + seas['hour']/24\n",
    "seas.sort_values('x',inplace=True)\n",
    "seas.plot(x='x',y='daily',ax=ax[0],legend=False)\n",
    "seas.plot(x='x',y='weekly',ax=ax[1],legend=False)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_xlabel('')\n",
    "ax[0].set_title(f'{airport} Seasonality Components\\nTrained on Jan-Sep 2019 Data')\n",
    "ax[1].set_xlabel('Day of week')\n",
    "ax[0].set_ylabel('Daily component (minutes)')\n",
    "ax[1].set_ylabel('Weekly component (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba797b13-e393-44af-a545-61b61e78776f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights = spark.read.parquet(f\"{team_BASE_DIR}/raw/flightdelays/parquet_airlines_data_{period}\")\n",
    "weather = spark.read.parquet(f\"{team_BASE_DIR}/interim/weather_{period}_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb179c6-13b7-4ca2-9f93-a6714f8eda59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(flights.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f392b082-e665-4a87-bc3c-0bfe9b741acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ecff08-892f-47b5-bb3d-3a1c3b7b08fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(weather.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c131484f-f522-46c3-abb4-7ad100ab1ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c4ba12-7a85-42ac-9057-9adc391ba43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the count of each value in the outcome variable\n",
    "outcome_counts = df.groupBy(\"outcome\").count()\n",
    "\n",
    "display(outcome_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62da19ca-7c47-452f-8a86-ddb9178a1e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = get_seasonality_data(df, 'full', 3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec8ee241-e51f-42f0-8caf-f6a3ea61c1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Select the relevant columns and convert to Pandas DataFrame\n",
    "selected_columns = [\"daily\", \"weekly\", \"outcome\"]\n",
    "df_pd = df.select(selected_columns).toPandas()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_pd.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb06ea5-8ab9-4ecf-81f9-f4c9956e6e83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Spearman Correlation Heatmap\\nSeasonality Features, 2019 (1 Year) Data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.11-eil-joined-modeling-NOT-CLEANED",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
