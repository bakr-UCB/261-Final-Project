{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea57f82-909d-40c5-a5ef-ff6655abcf9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a6327e-1199-4273-86b3-bb53c01e3c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Spark settings for better performance\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.executor.memory\", \"16g\")\\\n",
    "    .config(\"spark.executor.cores\", 4)\\\n",
    "    .appName('Final Project Training')\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d62c3-ed6a-47d0-bb21-3c0faee08bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, when\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7e45df-c877-46e1-b5f9-fa843a68c574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de30d1e2-82a4-461f-a323-79e9ca866d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds and overlap\n",
    "k = 5\n",
    "overlap = 0.2\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup)\n",
    "compute_seasonality = False\n",
    "apply_seasonality = False\n",
    "\n",
    "# define train/test split date\n",
    "if period == \"3m\":\n",
    "    min_test_dt = \"2015-03-01\"\n",
    "elif period == \"1y\":\n",
    "    min_test_dt = \"2019-10-01\"\n",
    "elif period == \"\":\n",
    "    min_test_dt = \"2019-01-01\"\n",
    "print(f\"Min test set date for {period} dataset: {min_test_dt}\")\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7820b43-a23e-488f-a7f3-d9b07493cf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data and perform simple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "\n",
    "# read in joined, cleaned dataset\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather_{period}.parquet\") # !!!\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_weather_cleaned_combo.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather{period}_v1.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned.parquet\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned_pr_v2.parquet\")\n",
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour and date variables (needed for seasonality and CV splits, respectively)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname))) \\\n",
    "    .withColumn(\"dep_date_utc\", to_date(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5455f469-d289-4a5c-935a-ebe06e5f16d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Group by the year and count the number of records for each year\n",
    "# df_year_counts = df.groupBy(\"YEAR\").count()\n",
    "\n",
    "# # Display the result\n",
    "# display(df_year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "# df_train.cache()\n",
    "# print(f\"Train data: {df_train.count()} records\")\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt) \\\n",
    "    .filter(f.col(dep_utc_varname) < \"2020-01-01\")\n",
    "# df_test.cache()\n",
    "# print(f\"Test data: {df_test.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa30565b-2ab1-45f9-ac57-8832abc556d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get cross-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc820cf1-c179-4279-8fe0-505d1758f9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits_by_days_with_overlap(df, k=3, blocking=False, overlap=0, dep_utc_varname=dep_utc_varname, verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation, based on # days in dataset\n",
    "    '''\n",
    "    \n",
    "    min_date = df.select(f.min(\"dep_date_utc\")).collect()[0][0]\n",
    "    max_date = df.select(f.max(\"dep_date_utc\")).collect()[0][0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "    total_width = k+1 - overlap*(k-1)\n",
    "    chunk_size = np.ceil(n_days/total_width) # last chunk may be slightly smaller than the others\n",
    "\n",
    "    # idx = np.arange(0,)\n",
    "    # idx = np.arange(0,n_days,chunk_size)\n",
    "    # idx[-1] = n_days-1\n",
    "    # idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Splitting data into {k} folds with {overlap} overlap')\n",
    "        print(f'Min date: {min_date}, max date: {max_date}')\n",
    "        print(f'{chunk_size:,} days per fold')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define indices based on chunk size and overlap\n",
    "        if i == 0:\n",
    "            train_min_offset = 0\n",
    "            train_max_offset = chunk_size\n",
    "        else:\n",
    "            train_min_offset += np.ceil((1-overlap)*chunk_size)\n",
    "            train_max_offset += np.floor((1-overlap)*chunk_size)\n",
    "        test_min_offset = train_max_offset\n",
    "        test_max_offset = test_min_offset + chunk_size\n",
    "\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = min_date\n",
    "        else:\n",
    "            t_min_train = min_date + timedelta(days=train_min_offset)\n",
    "        # define maximum training time\n",
    "        t_max_train = min_date + timedelta(days=train_max_offset)\n",
    "        # define minimum test time\n",
    "        t_min_test = min_date + timedelta(days=test_min_offset)\n",
    "        # define maximum test_time\n",
    "        t_max_test = min_date + timedelta(days=test_max_offset)\n",
    "\n",
    "        if t_max_test > max_date + timedelta(1):\n",
    "            t_max_test = max_date + timedelta(1)\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        print(\"(Note that the max dates are non-inclusive)\")\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc02f46-6e68-4780-91f9-e82265e6a3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs = [\n",
    "    {\"train_min\": \"2014-12-31\", \"train_max\": \"2015-10-09\", \"test_min\": \"2015-10-09\", \"test_max\": \"2016-07-17\"},\n",
    "    {\"train_min\": \"2015-08-14\", \"train_max\": \"2016-05-21\",\"test_min\": \"2016-05-21\", \"test_max\": \"2017-02-27\"},\n",
    "    {\"train_min\": \"2016-03-27\", \"train_max\": \"2017-01-01\",\"test_min\": \"2017-01-01\", \"test_max\": \"2017-10-10\"},\n",
    "    {\"train_min\": \"2016-11-08\", \"train_max\": \"2017-08-14\",\"test_min\": \"2017-08-14\", \"test_max\": \"2018-05-23\"},\n",
    "    {\"train_min\": \"2017-06-22\", \"train_max\": \"2018-03-27\",\"test_min\": \"2018-03-27\", \"test_max\": \"2019-01-01\"}\n",
    "    ]\n",
    "cv_cutoffs = pd.DataFrame(cv_cutoffs)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # get cross-validation split times\n",
    "# cv_cutoffs = get_cv_time_limits_by_days_with_overlap(df_train.select(\"dep_date_utc\"), k=k, blocking=True, overlap=overlap,\n",
    "#     dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "# cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5bdccd-7230-44de-9d2e-c2fba8819855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modeling Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ca65bd-089d-4899-8384-4f7bc2025ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define columns to be used in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c4ef7d-8964-4aca-a618-768a5ef36855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\"]\n",
    "\n",
    "# date related columns\n",
    "date_cols = [\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\"]\n",
    "\n",
    "#  'priorflight_cancelled_true',\n",
    "#  'priorflight_origin',\n",
    "#  'priorflight_dest',\n",
    "#  'priorflight_sched_deptime',\n",
    "#  'priorflight_true_deptime',\n",
    "#  'priorflight_sched_elapsed',\n",
    "#  'priorflight_true_elapsed',\n",
    "#  'priorflight_true_depdelay',\n",
    "#  'priorflight_sched_arrtime',\n",
    "#  'priorflight_true_arrtime',\n",
    "#  'priorflight_depdelay_calc',\n",
    "#  'priorflight_isdeparted',\n",
    "#  'priorflight_deptime_calc',\n",
    "#  'priorflight_isdelayed_calc',\n",
    "#  'priorflight_isarrived_calc',\n",
    "#  'priorflight_arr_time_calc',\n",
    "#  'turnaround_time_calc'\n",
    "\n",
    "# prior & current flight cols\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME'\n",
    "                #    ,'priorflight_elapsed_time_calc_raw'\n",
    "                ]\n",
    "\n",
    "bool_flight_cols = ['priorflight_isdeparted', \n",
    "                    'priorflight_isarrived_calc',\n",
    "                    'priorflight_isdelayed_calc',\n",
    "                    'priorflight_cancelled_true']\n",
    "\n",
    "# graph columns\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",dep_utc_varname] + [col for col in df.columns if \"train\" in col or \"test\" in col or \"daily\" in col or \"weekly\" in col or \"yearly\" in col or \"holidays\" in col]\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "categorical_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d358c96a-90d8-4a16-8f87-3e521240b637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"YEAR\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "keep_me = [\"outcome\",dep_utc_varname] + [col for col in df.columns if \"train\" in col or \"test\" in col or \"daily\" in col or \"weekly\" in col or \"yearly\" in col or \"holidays\" in col]\n",
    "\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *num_weather_cols, *graph_cols]\n",
    "# numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef77554-7fce-44d3-a857-07385a623e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_cols = [*keep_me, *numeric_cols, *categorical_cols]\n",
    "filter_cols = [c for c in filter_cols if c in df.columns]\n",
    "\n",
    "df_train = df_train.select(filter_cols)\n",
    "df_train.cache()\n",
    "df_test = df_test.select(filter_cols)\n",
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b9676c-7da2-427a-b2e7-f88b13d5f28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdf3442-9bfc-48f7-a6b6-7456cfb27842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "?SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c80b1d1-aae6-4073-afa7-0ff4c49e8e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# 1. Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# 4. Update feature list to include imputed columns\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "features = numeric_cols + categorical_vec_columns\n",
    "\n",
    "# 5. Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# 6. Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0dddc6-eb0f-48e8-942b-45a9a28e2771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b582b393-c135-43e6-a3c4-21177449b2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2d4b17-f16a-44c5-919b-a49112ff8fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE BELOW DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def upsample(train_df,verbose=False):\n",
    "  '''Upsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = non_delay_count / delay_count\n",
    "\n",
    "  train_delay = train_df.filter(f.col('outcome') == 0)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 1).sample(withReplacement=True, fraction=keep_percent,seed=42)\n",
    "  train_upsampled = train_delay.union(train_non_delay)\n",
    "  return train_upsampled\n",
    "\n",
    "\n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n",
    "\n",
    "def cv_eval(preds):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  return F2, pr\n",
    "\n",
    "def timeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timSeriesSplit k-fold cross validation \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "    # print(f\"{train_df.count()} (unsampled) TRAIN records in fold {i+1}\")\n",
    "    # print(f\"{dev_df.count()} DEV records in fold {i+1}\")\n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    # elif sampling=='weights':\n",
    "    #   train_df = add_class_weights(train_df).cache()\n",
    "      \n",
    "    # #print info on train and dev set for this fold\n",
    "    # if verbose:\n",
    "    #   print('    TRAIN set for fold {} goes from {} to {}, count is {:,} flights ({})'.format((i+1), \n",
    "    #                                                                                   train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "    #                                                                                   train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "    #                                                                                   train_df.count(),\n",
    "    #                                                                                   sampling + '-sampled' if sampling else 'no sampling'))\n",
    "    #   print('    DEV set for fold {} goes from {} to {}, count is {:,} flights'.format((i+1), \n",
    "    #                                                                                   dev_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "    #                                                                                   dev_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "    #                                                                                   dev_df.count()))\n",
    "    \n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    \n",
    "    print(f\"training for fold {i}\")\n",
    "        \n",
    "    # Fit params on the model\n",
    "    model = pipeline.fit(train_df)\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "  # # Train on full df\n",
    "  # print('Training on full train dataset, and validating on dev dataset with best parameters from CV:')\n",
    "  # print(best_parameters)\n",
    "    \n",
    "  # if verbose:\n",
    "  #   print('    TRAIN set for best parameter fitted model goes from {} to {}, count is {:,} flights ({})'.format(train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.count(),\n",
    "  #                                                                                                    sampling + '-sampled' if sampling else 'no sampling'))\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7e5aaf-398f-43c4-9b80-a4c7858bf91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "def final_eval(df_train, df_test,pipeline):\n",
    "    df_train = downsample(df_train).cache()\n",
    "    df_train = df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"test\",\"pagerank\")\n",
    "    df_train = df_train.fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "    df_test = df_test \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"test\",\"pagerank\")\n",
    "    df_test = df_test.fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "\n",
    "    model = pipeline.fit(df_train)\n",
    "    dev_pred = model.transform(df_test)\n",
    "    # get f2 score\n",
    "    score = cv_eval(dev_pred)[0]\n",
    "    print(score)\n",
    "\n",
    "    return dev_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8021d923-49f5-4f3f-a12d-99e716329b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def XGtimeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timeSeriesSplit k-fold cross validation on XGBoost.\n",
    "\n",
    "  Since XGBoost is not an MLLib object it will not automatically save, this function saves the model manually for each fold.\n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "\n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    \n",
    "    print(f\"training for fold {i}\")\n",
    "        \n",
    "    # Fit params on the model & save\n",
    "    print(f'Fitting pipeline...')\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    model_path = f\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_{i}\"\n",
    "    print(f'Saving model at: {model_path}')\n",
    "\n",
    "    model.write().overwrite().save(model_path)\n",
    "\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1986a888-c26e-4b72-a61a-5fe7a51792f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2faf079-7cd3-4608-8682-49b4f5e09347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# xgboost model\n",
    "xgb = SparkXGBClassifier(\n",
    "    booster=\"gbtree\",\n",
    "    features_col=\"features_scaled\",\n",
    "    label_col=\"outcome\",\n",
    "    max_depth=10,\n",
    "    num_workers=7\n",
    ")\n",
    "\n",
    "pipeline_xgb = Pipeline(stages=stages+[assembler,scaler,xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31280117-04ce-4642-85da-a3da8a19b01d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "XGtimeSeriesSplitCV(df_train, \n",
    "                    pipeline_xgb, \n",
    "                    cv_cutoffs, \n",
    "                    sampling='down', \n",
    "                    metric='f2', \n",
    "                    verbose=True, \n",
    "                    dep_utc_varname='sched_depart_utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258a893f-658d-4118-b173-6869a26327e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "XGtimeSeriesSplitCV(df_train, \n",
    "                    pipeline_xgb, \n",
    "                    cv_cutoffs, \n",
    "                    sampling='down', \n",
    "                    metric='f2', \n",
    "                    verbose=True, \n",
    "                    dep_utc_varname='sched_depart_utc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94e8f199-5b81-4ddd-96b2-916eb06eb3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76f98c6f-7d72-4294-b812-2f03e28959ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89bad03-2521-4f7e-bf2b-c721058e4ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcac93a3-9930-43c6-ab5a-438f02831965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_mod.stages[-1].get_booster()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d7e0bf-65f3-4afd-9a86-5757b7def3b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "va = fold1_mod.stages[-3]\n",
    "tree = fold1_mod.stages[-1]\n",
    "\n",
    "\n",
    "mappings=list(zip(va.getInputCols(), tree.get_feature_importances().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621b32b5-e1c8-441d-9ed6-672fb23baa63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_df=fold1_mod.stages[-1].get_booster().trees_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2d6884-0a91-455d-b8e7-fb7d6c60cb8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mappings_df=pd.DataFrame(mappings, columns=['name','id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db2dd99-6c0e-49f8-8bdb-7b50bfde0ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_gain=pd.DataFrame(list(tree.get_feature_importances('gain').items()), columns=['id', 'avg_gain'])\n",
    "weight=pd.DataFrame(list(tree.get_feature_importances('weight').items()), columns=['id', 'weight'])\n",
    "avg_cover=pd.DataFrame(list(tree.get_feature_importances('cover').items()), columns=['id', 'avg_cover'])\n",
    "\n",
    "feature_importance_df = avg_gain.merge(weight, on='id').merge(avg_cover, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107ae745-fd71-404d-b735-c56e021250fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_importance_df=feature_importance_df.merge(mappings_df, on='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123a87c0-abfb-4840-985b-9bcc531fe04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c379f551-cd5a-46ac-ad08-b44dc9516fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_importance_df.sort_values(by='avg_gain', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e33baa5-bb89-410f-b1f8-ebb52363d1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 features by avg_gain\n",
    "top_gain = feature_importance_df.nlargest(10, 'avg_gain')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_gain['name'][::-1], top_gain['avg_gain'][::-1], color='skyblue')\n",
    "plt.title('Top 10 Features by Average Gain')\n",
    "plt.xlabel('Average Gain')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "# Top 10 features by weight\n",
    "top_weight = feature_importance_df.nlargest(10, 'weight')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_weight['name'][::-1], top_weight['weight'][::-1], color='lightgreen')\n",
    "plt.title('Top 10 Features by Weight')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "# Top 10 features by avg_cover\n",
    "top_cover = feature_importance_df.nlargest(10, 'avg_cover')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_cover['name'][::-1], top_cover['avg_cover'][::-1], color='salmon')\n",
    "plt.title('Top 10 Features by Average Cover')\n",
    "plt.xlabel('Average Cover')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5075d3e0-3543-472b-ab10-9f04e54c0102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_df=fold1_df.merge(mappings_df, left_on='Feature', right_on = 'id').drop(['id','Category'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90a6a261-78cf-4287-86d1-db9202d25e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_df[fold1_df['name']=='mean_dep_delay']['Split'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6945eba9-42f3-4f21-9490-115c963d3295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_df[fold1_df['name']=='turnaround_time_calc']['Split'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b9eb56-7e31-42da-b2e9-a0731066aa06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ec2833-32f1-40c2-9ec7-b2c468024c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_mod.stages[-1].get_booster().get_split_value_histogram(feature='f0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcd24e0-f7ed-4578-97db-96f56217b04f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_mod.stages[-1].get_booster().num_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b506a164-f823-4965-9823-33292325f8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38ef730b-2103-47a7-b006-0203ce977624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72cb21a9-2d74-47cf-b20b-26d6ab75e6a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold1_mod.stages[-1].get_feature_importances().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c505bcaa-457f-40f1-873a-413597004184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cc51ed2-6f8d-4eb6-b7f3-297732f22a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a2220d-9ed9-4380-a4ed-1ec08d8af637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'QUARTER',\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"YEAR\",\n",
    "    \"origin_type\",\n",
    "    \"priororigin_type\",\n",
    "    \"priorflight_carrier\",\n",
    "    \"origin_region\"\n",
    "    ]\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\", \"priororigin_mean_dep_delay\"]\n",
    "\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME',\n",
    "                   'priorflight_sched_elapsed'\n",
    "                ]\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "keep_me = [\"outcome\",dep_utc_varname]\n",
    "\n",
    "\n",
    "numeric_cols = [*seasonality_cols, *time_cols, *num_flight_cols, *num_weather_cols, *graph_cols]\n",
    "# numeric_cols_cv = [*seasonality_cols_cv, *time_cols, *num_flight_cols, *weather_cols, *graph_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4639587d-c104-4ead-a15f-b761b10dd94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_cols = [*keep_me, *numeric_cols, *categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6128f7e4-6d21-4676-9170-b5d537b19411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_0\")\n",
    "\n",
    "fold1_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_1\")\n",
    "fold2_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_2\")\n",
    "fold3_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_3\")\n",
    "fold4_mod=PipelineModel.load(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/xgboost_fold_4\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d23e0b0-e1d2-4280-863f-7867f2f6d577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_test=fold0_mod.transform(df_test.withColumnsRenamed({'daily_full':'daily','weekly_full':'weekly','yearly_full':'yearly','holidays_full':'holidays','train':'pagerank'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3476b29-663a-410a-9cd7-c032f76348f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87023483-c4fd-4016-9d92-d735b2f7a4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fold0_test.withColumn(\"fold1_probs\", vector_to_array(\"probability\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "954f3b38-6055-47de-aef7-e682588e6f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_cols = [*keep_me, *numeric_cols, *categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119122fa-29c6-4b53-aeed-38767b4743bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df1 = fold0_test.withColumnRenamed('prediction','fold0_pred').select(filter_cols+['fold0_pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e8992f-e263-40a8-9fa5-0674fe699797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold0_test=fold0_mod.transform(df_test.withColumnsRenamed({'daily_full':'daily','weekly_full':'weekly','yearly_full':'yearly','holidays_full':'holidays','train':'pagerank'})).withColumn(\"fold0_probs\", vector_to_array(\"probability\")[1]).select(filter_cols+['fold0_probs']) #fold 0 preds\n",
    "\n",
    "fold1_test=fold1_mod.transform(fold0_test).withColumn(\"fold1_probs\", vector_to_array(\"probability\")[1]).select(filter_cols+['fold0_probs','fold1_probs']) #fold 1 preds\n",
    "\n",
    "fold2_test=fold2_mod.transform(fold1_test).withColumn(\"fold2_probs\", vector_to_array(\"probability\")[1]).select(filter_cols+['fold0_probs','fold1_probs', 'fold2_probs']) #fold 2 preds\n",
    "\n",
    "fold3_test=fold3_mod.transform(fold2_test).withColumn(\"fold3_probs\", vector_to_array(\"probability\")[1]).select(filter_cols+['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs']) \n",
    "\n",
    "fold4_test=fold4_mod.transform(fold3_test).withColumn(\"fold4_probs\", vector_to_array(\"probability\")[1]).select(filter_cols+['fold0_probs','fold1_probs', 'fold2_probs', 'fold3_probs', 'fold4_probs'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24707d1d-be62-490a-b04c-d45a98cbd502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold4_test.checkpoint()\n",
    "display(fold4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15aa527-211b-4e06-82e5-14c9e5f91321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.5  # decay rate; adjust as needed\n",
    "num_folds = 5\n",
    "\n",
    "raw_weights = np.array([alpha ** (num_folds - 1 - i) for i in range(num_folds)])\n",
    "weights = raw_weights / raw_weights.sum()  # normalize to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd02353-e909-4011-ba0e-8a54c0f96060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ewa_expr = sum([weights[i] * col(f\"fold{i}_probs\") for i in range(num_folds)])\n",
    "\n",
    "final_df = fold4_test.withColumn(\"ewa_prob\", ewa_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e6e3ae-ae68-440c-91df-a46f2ec8ce48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df=final_df.withColumn('prediction', when(col('ewa_prob') >= 0.5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "069a0014-6927-4c7c-ae46-d7fc7caf1342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318af128-1c97-4889-8170-b6432e263605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\",\n",
    "    predictionCol='prediction', \n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "evaluator.evaluate(final_df.withColumn(\"prediction\", col(\"prediction\").cast(DoubleType())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7920d77e-40ae-46b5-b23b-755169a5e378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(\"dbfs:/student-groups/Group_4_1/interim/modeling_checkpoints/model_results_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8502f2fd-6a63-4fdd-abf3-228b58d124cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df=final_df.withColumn('prediction2', when(col('ewa_prob') > 0.5, 1).otherwise(0))\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome\",\n",
    "    predictionCol='prediction2', \n",
    "    metricName=\"fMeasureByLabel\",\n",
    "    beta=2.0,\n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "evaluator.evaluate(final_df.withColumn(\"prediction2\", col(\"prediction2\").cast(DoubleType())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c2eb28-d7a0-4be5-bc42-a12023e8ff50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "def final_eval(df_train, df_test,pipeline):\n",
    "    df_train = downsample(df_train).cache()\n",
    "    df_train = df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"test\",\"pagerank\")\n",
    "    df_train = df_train.fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "    df_test = df_test \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"test\",\"pagerank\")\n",
    "    df_test = df_test.fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "\n",
    "    model = pipeline.fit(df_train)\n",
    "    dev_pred = model.transform(df_test)\n",
    "    # get f2 score\n",
    "    score = cv_eval(dev_pred)[0]\n",
    "    print(score)\n",
    "\n",
    "    return dev_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64a730ec-64d0-4f5c-b2cd-b9abd606b210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Past work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b4e343-07a1-43c1-8aab-bd2870942847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model, dev_pred_xgb = final_eval(df_train, df_test,pipeline_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148e2cc4-8801-4d09-9246-f0cbbd3b0f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=stages+[assembler,scaler,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18ae1e7-63d3-4c38-a100-895e941a1804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline_rf, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname='sched_depart_utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa053ee-b409-4c39-9b6c-ddcbb7477c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred_rf = final_eval(df_train, df_test,pipeline_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65eed5c4-6ded-48ba-8c3b-95e970a5cc17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION 1 \n",
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'priorflight_cancelled_true'\n",
    "    ]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol='{0}_index'.format(\n",
    "    column), outputCol='{0}_ohe'.format(\n",
    "    column, handleInvalid='keep')) for column in cat_cols]\n",
    "\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "scaler = MinMaxScaler(inputCol='features',outputCol='features_scaled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194eb009-15a5-44a1-9fac-ef1c67f23da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION 2\n",
    "pipeline0 = Pipeline(stages=indexers + encoders + [featuresCreator])\n",
    "model0 = pipeline0.fit(df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"test\",\"pagerank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6614bed3-712b-406d-bda4-09b0ff35c672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp = model0.transform(df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"test\",\"pagerank\")).limit(1).select('features').toPandas()\n",
    "L = tmp.iloc[0,0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39f1ba5-3b25-4480-8dfd-efa046ceb46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "L # 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861fda78-2000-41ac-9273-6751e5bf5eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION 3\n",
    "layers = [L, 4, 2, 2]\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                            featuresCol='features_scaled',\n",
    "                                            maxIter=50,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=indexers + encoders + [featuresCreator, scaler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c5a1f0-ca54-4fc8-8c5d-8d35137d00f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ALTERNATE\n",
    "cat_cols = [\n",
    "    'OP_UNIQUE_CARRIER',\n",
    "    'priorflight_isdeparted',\n",
    "    'priorflight_isarrived_calc',\n",
    "    'priorflight_isdelayed_calc',\n",
    "    'priorflight_cancelled_true'\n",
    "    ]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
    "    column), handleInvalid='keep') for column in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol='{0}_index'.format(\n",
    "    column), outputCol='{0}_ohe'.format(\n",
    "    column, handleInvalid='keep')) for column in cat_cols]\n",
    "\n",
    "# featuresCreator = VectorAssembler(\n",
    "#     inputCols=[encoder.getOutputCol() for encoder in encoders] + numeric_cols,\n",
    "#     outputCol='features', handleInvalid='skip')\n",
    "featuresCreator = VectorAssembler(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCol='features', handleInvalid='skip')\n",
    "\n",
    "scaler = MinMaxScaler(inputCol='features',outputCol='features_scaled')\n",
    "\n",
    "\n",
    "layers = [len(numeric_cols), 4, 2, 2]\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='outcome',\n",
    "                                            featuresCol='features_scaled',\n",
    "                                            maxIter=50,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=1234)\n",
    "pipeline_mlp = Pipeline(stages=[featuresCreator, scaler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cf594c-0096-40ed-9738-710c86794d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill_me = [c for c in df_train.columns if c not in ['sched_depart_utc']]\n",
    "\n",
    "# Fill missing values with 0 for the specified columns\n",
    "df_filled = df_train.fillna({c: 0 for c in numeric_cols if c in df_train.columns})\n",
    "\n",
    "# # Ensure the dataframe is properly formatted and has no issues with dimensions\n",
    "# df_filled = df_filled.repartition(200)  # Adjust the number of partitions as needed\n",
    "\n",
    "timeSeriesSplitCV(\n",
    "    df_filled,\n",
    "    pipeline_mlp,\n",
    "    cv_cutoffs,\n",
    "    sampling='down',\n",
    "    metric='f2',\n",
    "    verbose=True,\n",
    "    dep_utc_varname='sched_depart_utc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72155a22-3e28-4b45-9341-eb9f13cede96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred_mlp = final_eval(df_train.fillna({c: 0 for c in numeric_cols if c in df_train.columns}), df_test.fillna({c: 0 for c in numeric_cols if c in df_train.columns}), pipeline_mlp)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.17-sg-5y-XGBOOST",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
