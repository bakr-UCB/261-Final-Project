{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7242c0b-e377-4108-94d5-687826addb3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Baseline Modeling on Joined Data\n",
    "\n",
    "Erica Landreth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea57f82-909d-40c5-a5ef-ff6655abcf9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a6327e-1199-4273-86b3-bb53c01e3c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Spark settings for better performance\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.executor.memory\", \"16g\")\\\n",
    "    .config(\"spark.executor.cores\", \"32g\")\\\n",
    "    .config(\"spark.executor.cores\", 4)\\\n",
    "    .appName('Final Project Training')\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26d62c3-ed6a-47d0-bb21-3c0faee08bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, time\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from prophet.plot import plot_forecast_component\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, StructType, DoubleType, LongType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, to_timestamp, lit, udf\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, when\n",
    "from prophet.make_holidays import make_holidays_df\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7e45df-c877-46e1-b5f9-fa843a68c574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de30d1e2-82a4-461f-a323-79e9ca866d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data time period\n",
    "period = \"\" # on of the following values (\"\", \"3m\", \"6m\", \"1y\")\n",
    "\n",
    "# number of cross-validation folds and overlap\n",
    "k = 5\n",
    "overlap = 0.2\n",
    "\n",
    "# compute seasonality?\n",
    "# (False if you've already saved out seasonality models for a given CV split setup)\n",
    "compute_seasonality = False\n",
    "apply_seasonality = False\n",
    "\n",
    "# define train/test split date\n",
    "if period == \"3m\":\n",
    "    min_test_dt = \"2015-03-01\"\n",
    "elif period == \"1y\":\n",
    "    min_test_dt = \"2019-10-01\"\n",
    "elif period == \"\":\n",
    "    min_test_dt = \"2019-01-01\"\n",
    "print(f\"Min test set date for {period} dataset: {min_test_dt}\")\n",
    "\n",
    "# define what departure time variable is called\n",
    "dep_utc_varname = \"sched_depart_utc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7820b43-a23e-488f-a7f3-d9b07493cf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data and perform simple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1b990-617b-41b3-9c52-31891d2bf6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_BASE_DIR = f\"dbfs:/student-groups/Group_4_1\"\n",
    "\n",
    "# read in joined, cleaned dataset\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather_{period}.parquet\") # !!!\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_weather_cleaned_combo.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_flights_weather{period}_v1.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat.parquet\")\n",
    "# df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned.parquet\")\n",
    "df = spark.read.parquet(f\"{team_BASE_DIR}/interim/join_checkpoints/joined_{period}_timefeat_seasfeat_cleaned_pr.parquet\")\n",
    "\n",
    "# convert time variable to datetime\n",
    "df = df.withColumn(dep_utc_varname, to_timestamp(col(dep_utc_varname)))\n",
    "\n",
    "# add hour and date variables (needed for seasonality and CV splits, respectively)\n",
    "df = df.withColumn(\"dep_hour_utc\", f.hour(col(dep_utc_varname))) \\\n",
    "    .withColumn(\"dep_date_utc\", to_date(col(dep_utc_varname)))\n",
    "\n",
    "# define outcome variable\n",
    "df = df.withColumn(\"outcome\", (when((col(\"DEP_DELAY\") >= 15) | (col(\"CANCELLED\") == 1), 1).otherwise(0)).cast(\"double\"))\n",
    "\n",
    "# cast weather columns to double\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "for column in num_weather_cols:\n",
    "    df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5455f469-d289-4a5c-935a-ebe06e5f16d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Group by the year and count the number of records for each year\n",
    "# df_year_counts = df.groupBy(\"YEAR\").count()\n",
    "\n",
    "# # Display the result\n",
    "# display(df_year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f88b07-0bd8-4238-8105-12001d9b5dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df.filter(f.col(dep_utc_varname) < min_test_dt)\n",
    "df_train.cache()\n",
    "# print(f\"Train data: {df_train.count()} records\")\n",
    "df_test = df.filter(f.col(dep_utc_varname) >= min_test_dt) \\\n",
    "    .filter(f.col(dep_utc_varname) < \"2020-01-01\")\n",
    "df_test.cache()\n",
    "# print(f\"Test data: {df_test.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa30565b-2ab1-45f9-ac57-8832abc556d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get cross-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc820cf1-c179-4279-8fe0-505d1758f9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE IN THIS CELL DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def get_cv_time_limits_by_days_with_overlap(df, k=3, blocking=False, overlap=0, dep_utc_varname=dep_utc_varname, verbose=True):\n",
    "    '''\n",
    "    Get time bins for time-series cross validation, based on # days in dataset\n",
    "    '''\n",
    "    \n",
    "    min_date = df.select(f.min(\"dep_date_utc\")).collect()[0][0]\n",
    "    max_date = df.select(f.max(\"dep_date_utc\")).collect()[0][0]\n",
    "    n_days = (max_date - min_date).days + 1\n",
    "    total_width = k+1 - overlap*(k-1)\n",
    "    chunk_size = np.ceil(n_days/total_width) # last chunk may be slightly smaller than the others\n",
    "\n",
    "    # idx = np.arange(0,)\n",
    "    # idx = np.arange(0,n_days,chunk_size)\n",
    "    # idx[-1] = n_days-1\n",
    "    # idx = [int(i)+1 for i in idx]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Splitting data into {k} folds with {overlap} overlap')\n",
    "        print(f'Min date: {min_date}, max date: {max_date}')\n",
    "        print(f'{chunk_size:,} days per fold')\n",
    "        print(\"************************************************************\")\n",
    "\n",
    "    out = []\n",
    "    for i in range(k):\n",
    "        # define indices based on chunk size and overlap\n",
    "        if i == 0:\n",
    "            train_min_offset = 0\n",
    "            train_max_offset = chunk_size\n",
    "        else:\n",
    "            train_min_offset += np.ceil((1-overlap)*chunk_size)\n",
    "            train_max_offset += np.floor((1-overlap)*chunk_size)\n",
    "        test_min_offset = train_max_offset\n",
    "        test_max_offset = test_min_offset + chunk_size\n",
    "\n",
    "        # define minimum training time based on cross-validation style\n",
    "        if not blocking:\n",
    "            t_min_train = min_date\n",
    "        else:\n",
    "            t_min_train = min_date + timedelta(days=train_min_offset)\n",
    "        # define maximum training time\n",
    "        t_max_train = min_date + timedelta(days=train_max_offset)\n",
    "        # define minimum test time\n",
    "        t_min_test = min_date + timedelta(days=test_min_offset)\n",
    "        # define maximum test_time\n",
    "        t_max_test = min_date + timedelta(days=test_max_offset)\n",
    "\n",
    "        if t_max_test > max_date + timedelta(1):\n",
    "            t_max_test = max_date + timedelta(1)\n",
    "\n",
    "        out.append({\"train_min\":t_min_train, \"train_max\":t_max_train,\n",
    "                    \"test_min\":t_min_test, \"test_max\":t_max_test})\n",
    "    out = pd.DataFrame(out)\n",
    "        \n",
    "    if verbose:\n",
    "        for i in range(k):\n",
    "            print(f'    TRAIN set for fold {i} goes from {out[\"train_min\"][i]} to {out[\"train_max\"][i]}')\n",
    "            print(f'    TEST set for fold {i} goes from {out[\"test_min\"][i]} to {out[\"test_max\"][i]}')\n",
    "        print(\"(Note that the max dates are non-inclusive)\")\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc02f46-6e68-4780-91f9-e82265e6a3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_cutoffs = [\n",
    "    {\"train_min\": \"2014-12-31\", \"train_max\": \"2015-10-09\", \"test_min\": \"2015-10-09\", \"test_max\": \"2016-07-17\"},\n",
    "    {\"train_min\": \"2015-08-14\", \"train_max\": \"2016-05-21\",\"test_min\": \"2016-05-21\", \"test_max\": \"2017-02-27\"},\n",
    "    {\"train_min\": \"2016-03-27\", \"train_max\": \"2017-01-01\",\"test_min\": \"2017-01-01\", \"test_max\": \"2017-10-10\"},\n",
    "    {\"train_min\": \"2016-11-08\", \"train_max\": \"2017-08-14\",\"test_min\": \"2017-08-14\", \"test_max\": \"2018-05-23\"},\n",
    "    {\"train_min\": \"2017-06-22\", \"train_max\": \"2018-03-27\",\"test_min\": \"2018-03-27\", \"test_max\": \"2019-01-01\"}\n",
    "    ]\n",
    "cv_cutoffs = pd.DataFrame(cv_cutoffs)\n",
    "cv_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94c677d-b079-44a6-ac63-95d6fd60b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # get cross-validation split times\n",
    "# cv_cutoffs = get_cv_time_limits_by_days_with_overlap(df_train.select(\"dep_date_utc\"), k=k, blocking=True, overlap=overlap,\n",
    "#     dep_utc_varname=dep_utc_varname, verbose=True)\n",
    "# cv_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5bdccd-7230-44de-9d2e-c2fba8819855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ca65bd-089d-4899-8384-4f7bc2025ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define columns to be used in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c4ef7d-8964-4aca-a618-768a5ef36855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weather columns\n",
    "weather_cols = [col for col in df.columns if \"origin_Hourly\" in col]\n",
    "remove_me = [\"origin_HourlyPresentWeatherType\",\"origin_HourlySkyConditions\",\"origin_HourlyWindDirection\"]\n",
    "num_weather_cols = [c for c in weather_cols if c not in remove_me]\n",
    "\n",
    "# seasonality columns\n",
    "seasonality_cols = [\"daily\",\"weekly\",\"yearly\",\"holidays\"]\n",
    "\n",
    "# time columns\n",
    "time_cols = [\"mean_dep_delay\",\"prop_delayed\"]\n",
    "\n",
    "# date related columns\n",
    "date_cols = [\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\"]\n",
    "\n",
    "# flight metadata\n",
    "flight_metadata_cols = [\"OP_UNIQUE_CARRIER\",\"ORIGIN_ICAO\",\"DEST_ICAO\"]\n",
    "\n",
    "#  'priorflight_cancelled_true',\n",
    "#  'priorflight_origin',\n",
    "#  'priorflight_dest',\n",
    "#  'priorflight_sched_deptime',\n",
    "#  'priorflight_true_deptime',\n",
    "#  'priorflight_sched_elapsed',\n",
    "#  'priorflight_true_elapsed',\n",
    "#  'priorflight_true_depdelay',\n",
    "#  'priorflight_sched_arrtime',\n",
    "#  'priorflight_true_arrtime',\n",
    "#  'priorflight_depdelay_calc',\n",
    "#  'priorflight_isdeparted',\n",
    "#  'priorflight_deptime_calc',\n",
    "#  'priorflight_isdelayed_calc',\n",
    "#  'priorflight_isarrived_calc',\n",
    "#  'priorflight_arr_time_calc',\n",
    "#  'turnaround_time_calc'\n",
    "\n",
    "# prior & current flight cols\n",
    "num_flight_cols = ['turnaround_time_calc', \n",
    "                   'priorflight_depdelay_calc',\n",
    "                   'DISTANCE',\n",
    "                   'CRS_ELAPSED_TIME'\n",
    "                #    ,'priorflight_elapsed_time_calc_raw'\n",
    "                ]\n",
    "\n",
    "bool_flight_cols = ['priorflight_isdeparted', \n",
    "                    'priorflight_isarrived_calc',\n",
    "                    'priorflight_isdelayed_calc',\n",
    "                    'priorflight_cancelled_true']\n",
    "\n",
    "# graph columns\n",
    "graph_cols = [\"pagerank\"]\n",
    "\n",
    "# fields that will not be features but need to be kept for processing\n",
    "keep_me = [\"outcome\",\"DAY_OF_WEEK\",\"ORIGIN\",\"dep_hour_utc\",dep_utc_varname] # TODO: REVISIT WHICH ACTUALLY NEEDED\n",
    "\n",
    "########## Define columns to be used as numeric and categorical features in the pipeline ##########\n",
    "numeric_cols = [*num_weather_cols, *seasonality_cols, *time_cols, *num_flight_cols, *graph_cols]\n",
    "categorical_cols = [*date_cols, *flight_metadata_cols, *bool_flight_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b9676c-7da2-427a-b2e7-f88b13d5f28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f517283b-5e97-4371-bd81-a61ba0f4942a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## DEFINE ALL ARCHITECTURE TYPES\n",
    "\n",
    "# logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features_scaled', \\\n",
    "    labelCol='outcome',maxIter=50)\n",
    "\n",
    "# xgboost model\n",
    "xgb = SparkXGBClassifier(\n",
    "  features_col=\"features_scaled\",\n",
    "  label_col=\"outcome\",\n",
    "  max_depth=10,\n",
    "  \n",
    ")\n",
    "\n",
    "# random forest model\n",
    "rf = RandomForestClassifier(numTrees=30, maxDepth=10, featuresCol=\"features_scaled\" , labelCol=\"outcome\", seed=42)\n",
    "\n",
    "# multilayer perceptron model\n",
    "mlp = None # FILL IN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c80b1d1-aae6-4073-afa7-0ff4c49e8e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# 1. Index and encode categorical columns\n",
    "for column in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=column, \n",
    "        outputCol=column + \"_index\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=column + \"_index\", \n",
    "        outputCol=column + \"_vec\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# 4. Update feature list to include imputed columns\n",
    "categorical_vec_columns = [col + \"_vec\" for col in categorical_cols]\n",
    "\n",
    "features = numeric_cols + categorical_vec_columns\n",
    "\n",
    "# 5. Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features, \n",
    "    outputCol=\"features\", \n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# 6. Scale features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"features_scaled\"\n",
    ")\n",
    "\n",
    "# 7. Logistic regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features_scaled\", \n",
    "    labelCol=\"outcome\", \n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "##############################################################################\n",
    "# FILL IN DESIRED MODEL TYPE BELOW\n",
    "\n",
    "# # construct pipeline object from all components\n",
    "# pipeline = Pipeline(stages=stages+[assembler,scaler,xgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b582b393-c135-43e6-a3c4-21177449b2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2d4b17-f16a-44c5-919b-a49112ff8fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CODE BELOW DERIVED FROM DEMO 11 NOTEBOOK\n",
    "\n",
    "def upsample(train_df,verbose=False):\n",
    "  '''Upsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = non_delay_count / delay_count\n",
    "\n",
    "  train_delay = train_df.filter(f.col('outcome') == 0)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 1).sample(withReplacement=True, fraction=keep_percent,seed=42)\n",
    "  train_upsampled = train_delay.union(train_non_delay)\n",
    "  return train_upsampled\n",
    "\n",
    "\n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(f.col(\"outcome\") == 1).count()\n",
    "  non_delay_count = train_df.filter(f.col(\"outcome\") == 0).count()\n",
    "\n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(f.col('outcome') == 1)\n",
    "  train_non_delay = train_df.filter(f.col('outcome') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled\n",
    "\n",
    "def cv_eval(preds):\n",
    "  \"\"\"\n",
    "  Input: transformed df with prediction and label\n",
    "  Output: desired score \n",
    "  \"\"\"\n",
    "  rdd_preds_m = preds.select(['prediction', 'outcome']).rdd\n",
    "  rdd_preds_b = preds.select('outcome','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['outcome'])))\n",
    "  metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "  metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "  F2 = np.round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "  pr = metrics_b.areaUnderPR\n",
    "  return F2, pr\n",
    "\n",
    "def timeSeriesSplitCV(df, pipeline, cv_info, sampling=None, metric='f2', verbose=True, dep_utc_varname=dep_utc_varname):\n",
    "  '''\n",
    "  Perform timSeriesSplit k-fold cross validation \n",
    "  '''\n",
    "\n",
    "  k = len(cv_info)\n",
    "  \n",
    "  # Track score\n",
    "  scores=[]\n",
    "  \n",
    "  # Start k-fold\n",
    "  for i in range(k):\n",
    "    \n",
    "    # Create train set\n",
    "    train_df = df.filter((df[dep_utc_varname] >= cv_info[\"train_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"train_max\"][i])).cache()\n",
    "      \n",
    "    # Create dev set\n",
    "    dev_df = df.filter((df[dep_utc_varname] >= cv_info[\"test_min\"][i]) & \\\n",
    "      (df[dep_utc_varname] < cv_info[\"test_max\"][i])).cache() \n",
    "    \n",
    "    # print(f\"{train_df.count()} (unsampled) TRAIN records in fold {i+1}\")\n",
    "    # print(f\"{dev_df.count()} DEV records in fold {i+1}\")\n",
    "\n",
    "    # Apply sampling on train if selected\n",
    "    if sampling=='down':\n",
    "      train_df = downsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    elif sampling=='up':\n",
    "      train_df = upsample(train_df)\n",
    "      # train_df = train_df.cache()\n",
    "    # elif sampling=='weights':\n",
    "    #   train_df = add_class_weights(train_df).cache()\n",
    "      \n",
    "    # #print info on train and dev set for this fold\n",
    "    # if verbose:\n",
    "    #   print('    TRAIN set for fold {} goes from {} to {}, count is {:,} flights ({})'.format((i+1), \n",
    "    #                                                                                   train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "    #                                                                                   train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "    #                                                                                   train_df.count(),\n",
    "    #                                                                                   sampling + '-sampled' if sampling else 'no sampling'))\n",
    "    #   print('    DEV set for fold {} goes from {} to {}, count is {:,} flights'.format((i+1), \n",
    "    #                                                                                   dev_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "    #                                                                                   dev_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "    #                                                                                   dev_df.count()))\n",
    "    \n",
    "    \n",
    "    # prep seasonality columns (rename, fill as needed)\n",
    "    train_df = train_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    train_df = train_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "    dev_df = dev_df \\\n",
    "      .withColumnRenamed(f\"daily_{i}\",\"daily\") \\\n",
    "      .withColumnRenamed(f\"weekly_{i}\",\"weekly\") \\\n",
    "      .withColumnRenamed(f\"yearly_{i}\",\"yearly\") \\\n",
    "      .withColumnRenamed(f\"holidays_{i}\",\"holidays\") \\\n",
    "      .withColumnRenamed(f\"train_{i}\",\"pagerank\")\n",
    "    dev_df = dev_df.fillna({col:0 for col in \\\n",
    "      ['daily','weekly','yearly','holidays','mean_dep_delay','prop_delayed']})\n",
    "        \n",
    "    # Fit params on the model\n",
    "    model = pipeline.fit(train_df)\n",
    "    dev_pred = model.transform(dev_df)\n",
    "    if metric=='f2':\n",
    "      score = cv_eval(dev_pred)[0]\n",
    "    elif metric=='pr':\n",
    "      score = cv_eval(dev_pred)[1]\n",
    "    scores.append(score)\n",
    "    print(f'    Number of training datapoints for fold number {i+1} is {train_df.count():,} with a {metric} score of {score:.2f}') \n",
    "    print('------------------------------------------------------------')\n",
    "  \n",
    "  # Take average of all scores\n",
    "  avg_score = np.average(scores)    \n",
    "  print(f'Average {metric} score across all folds is {avg_score:.2f}')\n",
    "  print(\"************************************************************\")\n",
    "\n",
    "  # # Train on full df\n",
    "  # print('Training on full train dataset, and validating on dev dataset with best parameters from CV:')\n",
    "  # print(best_parameters)\n",
    "    \n",
    "  # if verbose:\n",
    "  #   print('    TRAIN set for best parameter fitted model goes from {} to {}, count is {:,} flights ({})'.format(train_df.agg({dep_utc_varname:'min'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.agg({dep_utc_varname:'max'}).collect()[0][0],\n",
    "  #                                                                                                    train_df.count(),\n",
    "  #                                                                                                    sampling + '-sampled' if sampling else 'no sampling'))\n",
    "  return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b6715074-7dfa-43a3-87b8-bcd68cef97c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7e5aaf-398f-43c4-9b80-a4c7858bf91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "def final_eval(df_train, df_test,pipeline):\n",
    "    df_train = downsample(df_train).cache()\n",
    "    df_train = df_train \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"train\",\"pagerank\")\n",
    "    df_train = df_train.fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "    df_test = df_test \\\n",
    "        .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "        .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "        .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "        .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "        .withColumnRenamed(f\"train\",\"pagerank\") # I THINK WE HAVE TO USE THE VALUE TRAINED ON THE TRAINING DATA, NOT TEST\n",
    "    df_test = df_test.fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "\n",
    "    model = pipeline.fit(df_train)\n",
    "    dev_pred = model.transform(df_test)\n",
    "    # get f2 score\n",
    "    score = cv_eval(dev_pred)[0]\n",
    "    print(score)\n",
    "\n",
    "    return dev_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5d32cffe-3433-490c-9c22-93fbcb88bc20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_test.withColumnRenamed(\"test\",\"pagerank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4243676d-12cf-46f5-94d0-c235a3741a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train, cross validate, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9bd38f66-1f12-49fe-a619-227a69cecd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# timeSeriesSplitCV(df_train, pipeline, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname='sched_depart_utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6daed906-95ff-48d1-b092-26622f3f3459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final_eval(df.limit(10), df.limit(10),pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8380f83-ef55-4e67-90c4-e2e3cb3570f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988c1594-03ee-43ae-ac3f-f667e6818942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import expr\n",
    "\n",
    "# # Generate a list of train_ columns\n",
    "# train_columns = [col for col in df_train.columns if col.startswith('train_')]\n",
    "\n",
    "# # Create an expression to calculate the average of train_ columns\n",
    "# avg_expr = \" + \".join(train_columns) + f\" / {len(train_columns)}\"\n",
    "\n",
    "# # Add the new column 'train' with the average value\n",
    "# df_train = df_train.withColumn(\"train\", expr(avg_expr))\n",
    "# display(df_train)\n",
    "\n",
    "### THERE IS ALREADY A COLUMN CALLED TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38627817-d3d3-426e-bf70-82df592303c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages=stages+[assembler,scaler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c56202-0373-4f02-b4c9-c9eba488fc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline_lr, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname='sched_depart_utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "435c009b-2b70-4e1d-93e8-d2606083330b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e5c27a4c-fd00-4003-be47-5eb17bd1c216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb488c7b-dbd8-43d5-8bbf-53bcb3bab7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred_lr = final_eval(df_train, df_test,pipeline_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1986a888-c26e-4b72-a61a-5fe7a51792f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0dddc6-eb0f-48e8-942b-45a9a28e2771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_xgb = Pipeline(stages=stages+[assembler,scaler,xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31280117-04ce-4642-85da-a3da8a19b01d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline_xgb, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname='sched_depart_utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca467781-fe7d-45a5-ae16-7455f7fc4a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training and evaluation\n",
    "\n",
    "TRAIN = downsample(df_train) \\\n",
    "    .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "    .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "    .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "    .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "    .withColumnRenamed(f\"train\",\"pagerank\") \\\n",
    "    .fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "TEST = df_test \\\n",
    "    .withColumnRenamed(f\"daily_full\",\"daily\") \\\n",
    "    .withColumnRenamed(f\"weekly_full\",\"weekly\") \\\n",
    "    .withColumnRenamed(f\"yearly_full\",\"yearly\") \\\n",
    "    .withColumnRenamed(f\"holidays_full\",\"holidays\") \\\n",
    "    .withColumnRenamed(f\"train\",\"pagerank\") \\\n",
    "    .fillna({col:0 for col in \\\n",
    "        ['daily','weekly','yearly','holidays','mean_dep_delay', 'prop_delayed']})\n",
    "\n",
    "\n",
    "model = pipeline_xgb.fit(TRAIN)\n",
    "dev_pred_xgb = model.transform(TEST)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred_xgb)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f6d6f6-8f1e-4c61-85ac-e8ce6e7f54a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TRAIN = TRAIN.withColumnRenamed(\"test\",\"pagerank\")\n",
    "TEST = TEST.withColumnRenamed(\"test\",\"pagerank\")\n",
    "\n",
    "model = pipeline_xgb.fit(TRAIN)\n",
    "dev_pred_xgb = model.transform(TEST)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred_xgb)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "90b4e343-07a1-43c1-8aab-bd2870942847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred_xgb = final_eval(df_train, df_test,pipeline_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0dce8f-17ff-4b94-b427-df435f1159da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148e2cc4-8801-4d09-9246-f0cbbd3b0f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=stages+[assembler,scaler,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18ae1e7-63d3-4c38-a100-895e941a1804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesSplitCV(df_train, pipeline_rf, cv_cutoffs, sampling='down', metric='f2', verbose=True, dep_utc_varname='sched_depart_utc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3cefc7a-814f-453a-9a40-a7056992038b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = pipeline_rf.fit(TRAIN)\n",
    "dev_pred_rf = model.transform(TEST)\n",
    "# get f2 score\n",
    "score = cv_eval(dev_pred_rf)[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa053ee-b409-4c39-9b6c-ddcbb7477c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dev_pred_rf = final_eval(df_train, df_test,pipeline_rf)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.14-eil-5y-modeling-PARED-DOWN",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
